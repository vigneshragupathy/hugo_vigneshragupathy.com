[{"content":" We\u0026rsquo;ll be using yml/yaml format for all examples down below, it is recommend to use yaml over toml as it is easier to read. You can find any YML to TOML converters if needed. Getting Started üöÄ Follow Hugo Docs\u0026rsquo;s - Quick Start guide to install . (Make sure you install Hugo \u0026gt;= v0.112.4)\nCreate a new site\nhugo new site MyFreshWebsite --format yaml # replace MyFreshWebsite with name of your website Note:\nOlder versions of Hugo may not support --format yaml Read more here about Hugo Docs\u0026rsquo;s - hugo new site command After you have created a new site, follow the below steps to add PaperMod\nInstalling/Updating PaperMod Themes reside in MyFreshWebsite/themes directory. PaperMod will be installed in MyFreshWebsite/themes/PaperMod Expand Method 1 - Git Clone INSTALL : Inside the folder of your Hugo site MyFreshWebsite, run:\ngit clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod --depth=1 You may use --branch v7.0 to end of above command if you want to stick to specific release.\nUPDATE: Inside the folder of your Hugo site MyFreshWebsite, run:\ncd themes/PaperMod git pull Expand Method 2 - Git Submodule (recomended) INSTALL : Inside the folder of your Hugo site MyFreshWebsite, run:\ngit submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod git submodule update --init --recursive # needed when you reclone your repo (submodules may not get cloned automatically) You may use --branch v7.0 to end of above command if you want to stick to specific release. Read more about git submodules here.\nUPDATE: Inside the folder of your Hugo site MyFreshWebsite, run:\ngit submodule update --remote --merge Expand Method 3 - Download an unzip Download PaperMod source as Zip from Github Releases and extract in your themes directory at MyFreshWebsite/themes/PaperMod\nDirect Links:\nMaster Branch (Latest) v7.0 v6.0 v5.0 v4.0 v3.0 v2.0 v1.0 Expand Method 4 - Hugo module INSTALL :\nInstall Go programming language in your operating system.\nIntialize your own hugo mod\nhugo mod init YOUR_OWN_GIT_REPOSITORY Add PaperMod in your config.yml file 1 2 3 module: imports: - path: github.com/adityatelange/hugo-PaperMod UPDATE:\nhugo mod get -u Read more : Hugo Docs\u0026rsquo;s - HUGO MODULES\nFinally set theme as PaperMod in your site config In config.yml add:\n1 theme: [\u0026#34;PaperMod\u0026#34;] Next up - Customizing PaperMod to suit your preferences. Your site will be blank after you set up for the very first time. You may go through this website\u0026rsquo;s source code - PaperMod\u0026rsquo;s exampleSite\u0026rsquo;s source Scroll below this page where you will find more specific details about each section. Kindly go through all of the pages below to know how to configure PaperMod. Support ü´∂ Star üåü PaperMod\u0026rsquo;s Github repository. Help spread the word about PaperMod by sharing it on social media and recommending it to your friends. üó£Ô∏è You can also sponsor üèÖ on Github Sponsors / Ko-Fi. Videos featuring PaperMod You can go through few videos which are available on YouTube for getting to know the creator\u0026rsquo;s thoughts as well as the setup process.\n‚ñ∂Ô∏è https://youtube.com/playlist?list=PLeiDFxcsdhUrzkK5Jg9IZyiTsIMvXxKZP\nQuick Links Papermod - Features Papermod - FAQs Papermod - Variables Papermod - Icons ChangeLog Sample config.yml Example Site Structure is present here: exampleSite\nUse appropriately\nbaseURL: \u0026#34;https://examplesite.com/\u0026#34; title: ExampleSite paginate: 5 theme: PaperMod enableRobotsTXT: true buildDrafts: false buildFuture: false buildExpired: false googleAnalytics: UA-123-45 minify: disableXML: true minifyOutput: true params: env: production # to enable google analytics, opengraph, twitter-cards and schema. title: ExampleSite description: \u0026#34;ExampleSite description\u0026#34; keywords: [Blog, Portfolio, PaperMod] author: Me # author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] # multiple authors images: [\u0026#34;\u0026lt;link or path of image for opengraph, twitter-cards\u0026gt;\u0026#34;] DateFormat: \u0026#34;January 2, 2006\u0026#34; defaultTheme: auto # dark, light disableThemeToggle: false ShowReadingTime: true ShowShareButtons: true ShowPostNavLinks: true ShowBreadCrumbs: true ShowCodeCopyButtons: false ShowWordCount: true ShowRssButtonInSectionTermList: true UseHugoToc: true disableSpecial1stPost: false disableScrollToTop: false comments: false hidemeta: false hideSummary: false showtoc: false tocopen: false assets: # disableHLJS: true # to disable highlight.js # disableFingerprinting: true favicon: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; favicon16x16: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; favicon32x32: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; apple_touch_icon: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; safari_pinned_tab: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; label: text: \u0026#34;Home\u0026#34; icon: /apple-touch-icon.png iconHeight: 35 # profile-mode profileMode: enabled: false # needs to be explicitly set title: ExampleSite subtitle: \u0026#34;This is subtitle\u0026#34; imageUrl: \u0026#34;\u0026lt;img location\u0026gt;\u0026#34; imageWidth: 120 imageHeight: 120 imageTitle: my image buttons: - name: Posts url: posts - name: Tags url: tags # home-info mode homeInfoParams: Title: \u0026#34;Hi there \\U0001F44B\u0026#34; Content: Welcome to my blog socialIcons: - name: x url: \u0026#34;https://x.com/\u0026#34; - name: stackoverflow url: \u0026#34;https://stackoverflow.com\u0026#34; - name: github url: \u0026#34;https://github.com/\u0026#34; analytics: google: SiteVerificationTag: \u0026#34;XYZabc\u0026#34; bing: SiteVerificationTag: \u0026#34;XYZabc\u0026#34; yandex: SiteVerificationTag: \u0026#34;XYZabc\u0026#34; cover: hidden: true # hide everywhere but not in structured data hiddenInList: true # hide on list pages and home hiddenInSingle: true # hide on single page editPost: URL: \u0026#34;https://github.com/\u0026lt;path_to_repo\u0026gt;/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link # for search # https://fusejs.io/api/options.html fuseOpts: isCaseSensitive: false shouldSort: true location: 0 distance: 1000 threshold: 0.4 minMatchCharLength: 0 limit: 10 # refer: https://www.fusejs.io/api/methods.html#search keys: [\u0026#34;title\u0026#34;, \u0026#34;permalink\u0026#34;, \u0026#34;summary\u0026#34;, \u0026#34;content\u0026#34;] menu: main: - identifier: categories name: categories url: /categories/ weight: 10 - identifier: tags name: tags url: /tags/ weight: 20 - identifier: example name: example.org url: https://example.org weight: 30 # Read: https://github.com/adityatelange/hugo-PaperMod/wiki/FAQs#using-hugos-syntax-highlighter-chroma pygmentsUseClasses: true markup: highlight: noClasses: false # anchorLineNos: true # codeFences: true # guessSyntax: true # lineNos: true # style: monokai Sample Page.md --- title: \u0026#34;My 1st post\u0026#34; date: 2020-09-15T11:30:03+00:00 # weight: 1 # aliases: [\u0026#34;/first\u0026#34;] tags: [\u0026#34;first\u0026#34;] author: \u0026#34;Me\u0026#34; # author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] # multiple authors showToc: true TocOpen: false draft: false hidemeta: false comments: false description: \u0026#34;Desc Text.\u0026#34; canonicalURL: \u0026#34;https://canonical.url/to/page\u0026#34; disableHLJS: true # to disable highlightjs disableShare: false disableHLJS: false hideSummary: false searchHidden: true ShowReadingTime: true ShowBreadCrumbs: true ShowPostNavLinks: true ShowWordCount: true ShowRssButtonInSectionTermList: true UseHugoToc: true cover: image: \u0026#34;\u0026lt;image path/url\u0026gt;\u0026#34; # image path/url alt: \u0026#34;\u0026lt;alt text\u0026gt;\u0026#34; # alt text caption: \u0026#34;\u0026lt;text\u0026gt;\u0026#34; # display caption under cover relative: false # when using page bundles set this to true hidden: true # only hide on current single page editPost: URL: \u0026#34;https://github.com/\u0026lt;path_to_repo\u0026gt;/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link --- You can use it by creating archetypes/post.md\nhugo new --kind post \u0026lt;name\u0026gt; ","permalink":"http://localhost:1313/posts/archive/papermod/papermod-installation/","summary":"Read aboout Install and Update instructions and sampled configuration templates","title":"Install / Update PaperMod"},{"content":"Intro We\u0026rsquo;ll be using yml/yaml format for all examples down below, I recommend using yml over toml as it is easier to read.\nYou can find any YML to TOML converters if necessary.\nAssets (js/css) The following is enabled by default\nminification - makes the assets size smallest as possible. bundling - bundles all the styles in one single asset fingerprint/intergity check. Default Theme light/dark/auto 1 2 3 4 params: # defaultTheme: light # defaultTheme: dark defaultTheme: auto # to switch between dark or light according to browser theme Theme Switch Toggle (enabled by default) Shows icon besides title of page to change theme\nTo disable it :\n1 disableThemeToggle: true You can refer following table for better understanding\u0026hellip;\ndefaultTheme disableThemeToggle checks local storage? checks system theme? Info auto true No Yes only system theme false Yes (if not-\u0026gt;2) Yes (2) switch present dark true No No force dark only false Yes No switch present light true No No force light only false Yes No switch present Archives Layout Create a page with archive.md in content directory with following content\n. ‚îú‚îÄ‚îÄ config.yml ‚îú‚îÄ‚îÄ content/ ‚îÇ ‚îú‚îÄ‚îÄ archives.md \u0026lt;--- Create archive.md here ‚îÇ ‚îî‚îÄ‚îÄ posts/ ‚îú‚îÄ‚îÄ static/ ‚îî‚îÄ‚îÄ themes/ ‚îî‚îÄ‚îÄ PaperMod/ and add the following to it\n--- title: \u0026#34;Archive\u0026#34; layout: \u0026#34;archives\u0026#34; url: \u0026#34;/archives/\u0026#34; summary: archives --- Note: Archives Layout does not support Multilingual Month Translations.\nex: archives.md\nRegular Mode (default-mode) Home-Info Mode Use 1st entry as some Information\nadd following to config file\nparams: homeInfoParams: Title: Hi there wave Content: Can be Info, links, about... socialIcons: # optional - name: \u0026#34;\u0026lt;platform\u0026gt;\u0026#34; url: \u0026#34;\u0026lt;link\u0026gt;\u0026#34; - name: \u0026#34;\u0026lt;platform 2\u0026gt;\u0026#34; url: \u0026#34;\u0026lt;link2\u0026gt;\u0026#34; Profile Mode Shows Index/Home page as Full Page with Social Links and Image\nadd following to config file\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 params: profileMode: enabled: true title: \u0026#34;\u0026lt;Title\u0026gt;\u0026#34; # optional default will be site title subtitle: \u0026#34;This is subtitle\u0026#34; imageUrl: \u0026#34;\u0026lt;image link\u0026gt;\u0026#34; # optional imageTitle: \u0026#34;\u0026lt;title of image as alt\u0026gt;\u0026#34; # optional imageWidth: 120 # custom size imageHeight: 120 # custom size buttons: - name: Archive url: \u0026#34;/archive\u0026#34; - name: Github url: \u0026#34;https://github.com/\u0026#34; socialIcons: # optional - name: \u0026#34;\u0026lt;platform\u0026gt;\u0026#34; url: \u0026#34;\u0026lt;link\u0026gt;\u0026#34; - name: \u0026#34;\u0026lt;platform 2\u0026gt;\u0026#34; url: \u0026#34;\u0026lt;link2\u0026gt;\u0026#34; Search Page PaperMod uses Fuse.js Basic for search functionality\nAdd the following to site config, config.yml\n1 2 3 4 5 outputs: home: - HTML - RSS - JSON # necessary for search Create a page with search.md in content directory with following content\n1 2 3 4 5 6 7 8 --- title: \u0026#34;Search\u0026#34; # in any language you want layout: \u0026#34;search\u0026#34; # necessary for search # url: \u0026#34;/archive\u0026#34; # description: \u0026#34;Description for Search\u0026#34; summary: \u0026#34;search\u0026#34; placeholder: \u0026#34;placeholder text in search input box\u0026#34; --- To hide a particular page from being searched, add it in post\u0026rsquo;s frontmatter\n1 searchHidden: true ex: search.md\nSearch Page also has Key bindings:\nArrow keys to move up/down the list Enter key (return) or Right Arrow key to go to the highlighted page Escape key to clear searchbox and results For Multilingual use search.\u0026lt;lang\u0026gt;.md ex. search.es.md.\nNote: Search will work only on current language, user is currently on !\nCustomizing Fusejs Options\nRefer https://fusejs.io/api/options.html for Options, Add those as shown below.\n1 2 3 4 5 6 7 8 9 10 params: fuseOpts: isCaseSensitive: false shouldSort: true location: 0 distance: 1000 threshold: 0.4 minMatchCharLength: 0 # limit: 10 # refer: https://www.fusejs.io/api/methods.html#search keys: [\u0026#34;title\u0026#34;, \u0026#34;permalink\u0026#34;, \u0026#34;summary\u0026#34;, \u0026#34;content\u0026#34;] Draft Page indication adds [draft] mark to indicate draft pages.\nPost Cover Image In post\u0026rsquo;s page-variables add :\n1 2 3 4 5 6 7 cover: image: \u0026#34;\u0026lt;image path/url\u0026gt;\u0026#34; # can also paste direct link from external site # ex. https://i.ibb.co/K0HVPBd/paper-mod-profilemode.png alt: \u0026#34;\u0026lt;alt text\u0026gt;\u0026#34; caption: \u0026#34;\u0026lt;text\u0026gt;\u0026#34; relative: false # To use relative path for cover image, used in hugo Page-bundles When you include images in the Page Bundle, multiple sizes of the image will automatically be provided using the HTML5 srcset field.\nTo reduce generation time and size of the site, you can disable this feature using\n1 2 3 params: cover: responsiveImages: false To enable hyperlinks to the full image size on post pages, use\n1 2 3 params: cover: linkFullImages: true Share Buttons on post Displays Share Buttons at Bottom of each post\nto show share buttons add\nparams: ShowShareButtons: true Show post reading time Displays Reading Time (the estimated time, in minutes, it takes to read the content.)\nTo show reading time add\nParams: ShowReadingTime: true Show Table of Contents (Toc) on blog post Displays ToC on blog-pages\nTo show ToC add following to page-variables\nShowToc: true To keep Toc Open by default on a post add following to page-variables:\nTocOpen: true BreadCrumb Navigation Adds BreadCrumb Navigation above Post\u0026rsquo;s Title to show subsections and Navigation to Home\nparams: ShowBreadCrumbs: true Can be diabled for particular page\u0026rsquo;s front-matter\n--- ShowBreadCrumbs: false --- Edit Link for Posts Add a button to suggest changes by using the file path of the post to link to a edit destination.\nFor site config use:\nParams: editPost: URL: \u0026#34;https://github.com/\u0026lt;path_to_repo\u0026gt;/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link Can be modified for individual pages\n--- editPost: URL: \u0026#34;https://github.com/\u0026lt;path_to_repo\u0026gt;/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link --- The example above would yield the following link for the post file posts/post-name.md: https://github.com/\u0026lt;path_to_repo\u0026gt;/content/posts/post-name.md\nParameter Required Default Value editPost.URL true - editPost.appendFilePath false false editPost.Text false Edit Since the link generated is a regular HTML anchor tag \u0026lt;a href=...\u0026gt;, you can also use other URL schemas like mailto://, e.g. URL: \u0026quot;mailto://mail@example.com?subject=Suggesting changes for \u0026quot;\nOther Posts suggestion below a post Adds a Previous / Next post suggestion under a single post\nparams: ShowPostNavLinks: true Code Copy Button Adds a copy button in code block to copy the code it contains\nparams: ShowCodeCopyButtons: true Multiple Authors To Use multiple authors for a post, in post-variables:\n--- author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] --- To use Multiple Authors Site-wide, in config.yml:\nparams: author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] Comments to add comments, create a html file\nlayouts/partials/comments.html\nand paste code provided by your comments provider\nalso in config add this\nparams: comments: true read more about this hugo-comments\nAccessKeys c - ToC Open/Close g - Go To Top h - Home (according to current lang) t - Theme toggle / - Jumps to search page if in menu What\u0026rsquo;s AccessKeys ?\nEnhanced SEO Enabled only when env: production\nRich Results/Snippets Support Twitter Cards Support The Twitter Cards metadata, except twitter:image should not require additional configuration, since it is generated from metadata that you should already have (for instance the page title and description). The twitter:image uses the Post Cover Image, if present. In the absence of a cover images, the first image from the images frontmatter (a list) is used. images: - image_01.png - image_02.png Finally, if neither of those are provided, twitter:image comes from the first Page Bundle image with feature in the name, with a fallback to the first image with cover or thumbnail in the name. OpenGraph support The OpenGraph metadata, except og:image should not require additional configuration, since it is generated from metadata that you should already have (for instance the page title and description). The og:image uses the Post Cover Image, if present. In the absence of a cover images, the first image from the images frontmatter (a list) is used. images: - image_01.png - image_02.png Finally, if neither of those are provided, og:image comes from the first Page Bundle image with feature in the name, with a fallback to the first image with cover or thumbnail in the name. For pages, you can also add audio (using frontmatter audio: filename.ext) and/or videos. videos: - filename01.mov - filename02.avi Multilingual Support Misc Scroll-Bar themed (by default) Smooth Scroll between in-page links (by default) Scroll-to-Top Button (by default) Displays a Scroll-to-Top button in right-bottom corner Google Analytics integration Syntax highlighting RSS feeds ","permalink":"http://localhost:1313/posts/archive/papermod/papermod-features/","summary":"Learn About All Features in PaperMod","title":"Features / Mods"},{"content":"Summary Monitoring in Kubernetes is a complex task.\nThe traditional monitoring framework is not sufficient to handle such a massive workload.\nZabbix since version 6.0 provides a native way of integration for monitoring Kubernetes cluster.\nZabbix-Kubernetes integration provides various templates to monitor kubernetes components like kube-controller-manager, kube-apiserver, kube-scheduler, kubelet, etc.\nIt also supports automatic discovery of kubernetes nodes, pods and also collects metrics agentlessly.\nWhy I don\u0026rsquo;t like the Zabbix\u0026rsquo;s direct way of monitoring Kubernetes cluster? Although Zabbix-Kubernetes integration looks promising in the beginning , it is not easy to use.\nThe documentation is not clear, it is mostly based on the assumption that the Zabbix server is running inside the same Kubernetes cluster.\nThere are so much details missing in the documentation especially if you want to do the monitoring from external Zabbix server or even to do a multiple cross cluster monitoring.\nAnd finally, most of all, the Zabbix monitoring system is not designed to monitor directly such a volatile, dynamic, multi-dimensional metrics from infrastructure like Kubernetes.\nWhy Prometheus? Prometheus is an opensource monitoring system which can collect massive amount of data in near real-time using it\u0026rsquo;s pull-based mechanism.\nPrometheus can be easily configured for service discovery.\nIt is agentless and works by sending an HTTP request so-called scrape, which can find the endpoints and scrap metrics from any source.\nThe response from scrape request is parsed and stored in Prometheus database.\nPrometheus provided powerful querying using it\u0026rsquo;s PromQL and the metrics collected can be pulled from any external system like Zabbix or Grafana via its API.\nUsing Prometheus federation, we can easily collect metrics from multiple Kubernetes clusters and create a hierarchically scaled monitoring system.\nHow Prometheus exposes metrics? Prometheus collects the metrics from any source by scraping the HTTP endpoint which contains metrics.\nIt also provides a web interface to view the metrics.\nThe metrics are exposed via API.\nPrometheus metrics can be queried by any HTTP client using it\u0026rsquo;s API Endpoints.\nQuery Prometheus using curl\ncurl -X GET \u0026#39;https://PROMETHEUS_HOSTNAME/api/v1/query?query=up\u0026#39; | jq status. Query on kubernetes node metrics\ncurl -X GET \u0026#39;https://PROMETHEUS_HOSTNAME/api/v1/query?query=kube_node_info\u0026#39; | jq status. Advanced query operations like boolean\ncurl -X GET \u0026#39;http://PROMETHEUS_HOSTNAME/api/v1/query?query={__name__=~\u0026#34;kube_pod_container_resource_limits_cpu_cores|kube_pod_status_phase\u0026#34;}\u0026gt;0\u0026#39; | jq . Why Prometheus is just not good enough? Prometheus is good for collecting the metrics from Kubernetes cluster. But it is not enough to act as a full-stack monitoring system.\nHere are some of the limitations.\nVery basic visualization of the metrics. No AI/ML features like automatic anomaly detection No role based access control Long term storage is not available. Proposed approach The proposed approach is to use Prometheus as a backend for Zabbix.\nThis approach provides the flexibility of using Prometheus pull based metrics collection, scalability and PromQL queries. Also the advantage of Zabbix for alerting, anomaly detection and role based access control etc.\nArchitecture of the proposed approach\n","permalink":"http://localhost:1313/posts/2022-07-01-kubernetes-monitoring-in-zabbix-via-prometheus-backend/","summary":"\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003eMonitoring in Kubernetes is a complex task.\u003c/p\u003e\n\u003cp\u003eThe traditional monitoring framework is not sufficient to handle such a massive workload.\u003c/p\u003e\n\u003cp\u003eZabbix since version 6.0 provides a native way of integration for monitoring Kubernetes cluster.\u003c/p\u003e\n\u003cp\u003eZabbix-Kubernetes integration provides various templates to monitor kubernetes components like \u003ccode\u003ekube-controller-manager\u003c/code\u003e, \u003ccode\u003ekube-apiserver\u003c/code\u003e, \u003ccode\u003ekube-scheduler\u003c/code\u003e, \u003ccode\u003ekubelet\u003c/code\u003e, etc.\u003c/p\u003e\n\u003cp\u003eIt also supports automatic discovery of kubernetes nodes, pods and also collects metrics agentlessly.\u003c/p\u003e\n\u003ch2 id=\"why-i-dont-like-the-zabbixs-direct-way-of-monitoring-kubernetes-cluster\"\u003eWhy I don\u0026rsquo;t like the Zabbix\u0026rsquo;s direct way of monitoring Kubernetes cluster?\u003c/h2\u003e\n\u003cp\u003eAlthough Zabbix-Kubernetes integration looks promising in the beginning , it is not easy to use.\u003c/p\u003e","title":"Kubernetes monitoring in Zabbix via Prometheus backend"},{"content":"Summary I joined State Bank of India on July 2020 and worked for 1 year and 2 months as Manager - IT Infrastructure Architect in Enterprise and Technology Architecture department in GITC, Navi Mumbai.\nMy role in SBI is technical, and it involves consulting, design, Architecture and reviewing of application and infrastructure.\nThrough out my tenure in SBI i worked on various short term projects. Among them setting up DevOps infrastructure for SBI is one.\nIt‚Äôs an ongoing project and we‚Äôre constantly finding ways to improve and make life easier for developers and Operations team across SBI.\nBreakup of overall time that had spent in SBI.\nWhat have I learned ? Importance of architecture principles and policies Some software architectural design Compliance and regulation in banking DevOps tools and practises Managing vendor partners Tracking project timelines Certificate of appreciation for delivering a talk on Kubernetes My timeline in SBI Sep 2019 - Specialist exam for SBI Dec 2019 - Personal Interview in Mumbai, SBI Jan 2020 - Received job offer from SBI Feb 2020 - Pandemic and full work from home start in Ericsson Apr 2021 - Relieving extension in Ericsson Jun 2020 - Last day in Ericsson Jul 2020 - Relocation to Mumbai and joining SBI Aug 2020 - Joining Enterprise and Technology architecture dept in SBI Oct 2020 - Travel to native and bringing family to Mumbai April 2021 - Travel to native and leaving family May 2021 - COVID like symptoms and home quarantine June 2021 - Decision to leave SBI and relocate from Mumbai July 2021 - New job offer received Aug 2021 - Resignation initiation in SBI Sep 2021 - Last working day in SBI Why leaving job from State Bank of India The decision for leaving the job at state bank of India is both personal and family. Some of the key points that influence the decisions are no WFH, no relocation/transfer of job, micro management, unnecessary and complicated process, poor decision making and low learning curve/Legacy technologies.\nOne of the important factor that any organisation should focus is equally changing the organisation culture when they modernise the technology stack. There are significant effort put on hiring technical experts, digital transformation, cloud strategy but very little on organisation culture.\nTo SBI (on request from SBI colleagues) Invest as equally as technology transformation in cultural transformation. Let the internal team solve the hard problems and use the external teams /vendors only to enable the in-house teams IT operations need engineers, not just the people managers. Replace people managers with engineering managers whereever needed. Do not mix them both. Make an open/transparent and employee friendly organisation. So whats next ? So what am I doing next? It\u0026rsquo;s been a privilege and an adventure to work in SBI as Infrastructure Architect with so many amazing people. But I\u0026rsquo;m now excited about my new role as Sr Site Reliability Engineer at Sage Intacct, Bangalore.\n","permalink":"http://localhost:1313/posts/2021-09-16-lifeatsbi/","summary":"\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003eI joined State Bank of India on July 2020 and worked for 1 year and 2 months as Manager - IT Infrastructure Architect in Enterprise and Technology Architecture department in GITC, Navi Mumbai.\u003c/p\u003e\n\u003cp\u003eMy role in SBI is technical, and it involves consulting, design, Architecture and reviewing of application and infrastructure.\u003c/p\u003e\n\u003cp\u003eThrough out my tenure in SBI i worked on various short term projects. Among them setting up DevOps infrastructure for SBI is one.\u003c/p\u003e","title":"My life at SBI"},{"content":"Plotly4Nagios is a nagios plugin to display the performance data in Graph. It uses the RRD database provided by pnp4nagios and visualize it in interactive graph format using plotly javascript. The first pre-release is published today in github and here is the installation document. You can experiment it and report the issue/feedback for further enhancement.\nPlotly4Nagios is accepted and listed under official nagios addons\nGIT badges Features Easy integration with nagios notes_url. Single page view for all performance metrics. Easy template change using configuration variable. Docker container based deploy and run. Prerequisite pnp4nagios Installation Download plotly4nagios.tar.gz and extract it under /usr/local/plotly4nagios Modify the config.json variables according to the environment Copy the plotly4nagios/plotly4nagios.conf to /etc/http/conf.d/ folder and restart httpd Add the follwing with notes_url to templates.cfg. notes_url /plotly4nagios/plotly4nagios.html?host=\\$HOSTNAME\\$\u0026amp;srv=_HOST_ notes_url /plotly4nagios/plotly4nagios.html?host=\\$HOSTNAME$\u0026amp;srv=\\$SERVICEDESC$ Restart httpd and nagios. Installation with docker(Ubuntu image) Build the docker image using the below command git clone https://github.com/vigneshragupathy/plotly4nagios.git cd plotly4nagios docker build -t plotly4nagios . Run the docker container using the below command docker run -it --name plotly4nagios -p 80:80 plotly4nagios Alternatively direct pull and run from docker hub.\ndocker run -d -p 80:80 --name plotly4nagios vigneshragupathy/plotly4nagios Open from the browser and view the application at http://localhost/nagios\nLogin details Username : nagiosadmin Password : nagios Demo Screenshot Dark mode License Copyright 2020-2021 ¬© Vignesh Ragupathy. All rights reserved.\nLicensed under the MIT License\n","permalink":"http://localhost:1313/posts/2021-03-24-plotly4nagios-graph-plugin-for-nagios-monitoring/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/vigneshragupathy/plotly4nagios\"\u003ePlotly4Nagios\u003c/a\u003e is a nagios plugin to display the performance data in Graph. It uses the RRD database provided by pnp4nagios and visualize it in interactive graph format using plotly javascript. The first pre-release is published today in \u003ca href=\"https://github.com/vigneshragupathy/plotly4nagios\"\u003egithub\u003c/a\u003e and here is the installation document. You can experiment it and report the issue/feedback for further enhancement.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePlotly4Nagios is accepted and listed under official \u003ca href=\"https://exchange.nagios.org/directory/Addons/Graphing-and-Trending/Plotly4Nagios/details\"\u003enagios addons\u003c/a\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch2 id=\"git-badges\"\u003eGIT badges\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"GitHub\" loading=\"lazy\" src=\"https://img.shields.io/github/license/vigneshragupathy/plotly4nagios\"\u003e\n\u003ca href=\"https://travis-ci.com/vigneshragupathy/plotly4nagios\"\u003e\u003cimg alt=\"Build Status\" loading=\"lazy\" src=\"https://travis-ci.com/vigneshragupathy/plotly4nagios.svg?branch=main\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"features\"\u003eFeatures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eEasy integration with nagios \u003ccode\u003enotes_url\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eSingle page view for all performance metrics.\u003c/li\u003e\n\u003cli\u003eEasy template change using configuration variable.\u003c/li\u003e\n\u003cli\u003eDocker container based deploy and run.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"prerequisite\"\u003ePrerequisite\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://support.nagios.com/kb/article/nagios-core-performance-graphs-using-pnp4nagios-801.html\"\u003epnp4nagios\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"installation\"\u003eInstallation\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDownload plotly4nagios.tar.gz and extract it under /usr/local/plotly4nagios\u003c/li\u003e\n\u003cli\u003eModify the config.json variables according to the environment\u003c/li\u003e\n\u003cli\u003eCopy the plotly4nagios/plotly4nagios.conf to /etc/http/conf.d/ folder and restart httpd\u003c/li\u003e\n\u003cli\u003eAdd the follwing with  notes_url to templates.cfg.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    notes_url /plotly4nagios/plotly4nagios.html?host\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"se\"\u003e\\$\u003c/span\u003eHOSTNAME\u003cspan class=\"se\"\u003e\\$\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"nv\"\u003esrv\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e_HOST_\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    notes_url /plotly4nagios/plotly4nagios.html?host\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"se\"\u003e\\$\u003c/span\u003eHOSTNAME$\u003cspan class=\"p\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"nv\"\u003esrv\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"se\"\u003e\\$\u003c/span\u003eSERVICEDESC$\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eRestart httpd and nagios.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"installation-with-dockerubuntu-image\"\u003eInstallation with docker(Ubuntu image)\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eBuild the docker image using the below command\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit clone https://github.com/vigneshragupathy/plotly4nagios.git\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003ecd\u003c/span\u003e plotly4nagios\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker build -t plotly4nagios .\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eRun the docker container using the below command\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edocker run -it --name plotly4nagios -p 80:80 plotly4nagios\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAlternatively direct pull and run from docker hub.\u003c/p\u003e","title":"Plotly4Nagios - A Graph plugin for nagios monitoring"},{"content":" Photo by Vignesh Ragupathy{:target=\u0026quot;_blank\u0026quot;}.\nI have been utilizing AWS to host my personal blog for almost 3 years now. Originally my blog was hosted in WordPress and then I migrated to ghost{:target=\u0026quot;_blank\u0026quot;}. It\u0026rsquo;s been 2 years now in ghost and I thought of exploring a new hosting option which should be free, supports custom domain name and free SSL{:target=\u0026quot;_blank\u0026quot;}.\nJekyll{:target=\u0026quot;_blank\u0026quot;} is a ruby based static blog generator and it has an advantage of free hosting in GitHub. The letsencrypt SSL certificate is also provided by GitHub for my custom domain so i don‚Äôt have to worry about managing it.\nI also created a separate website{:target=\u0026quot;_blank\u0026quot;} to showcase my open-source tools and I can use the same AWS instance for hosting it. It is a Django application which uses more memory/CPU, so i can run it in a dedicated instance instead of running the ghost and Django together.\nOne of the challenges in a Django application is hosting your static content. Django recommends using a proxy server like Nginx to serve its static content.\nI use my nginx proxy to serve the static content. But due to performance reason , i started to explore the free CDN to serve my static content\nBelow is the nginx configuration snippet for mapping static content.\n{% highlight console %} location /static/ { root /tools.vikki.in/static; } {% endhighlight %}\nAfter doing some research I chose to utilize unpkg or jsdelivr for my site.\nunpkg and jsdelivr are global CDN and they can be used to deliver any packages hosted in NPM\nunpkg{:target=\u0026quot;_blank\u0026quot;} and jsdelivr{:target=\u0026quot;_blank\u0026quot;} both provides CDN for the content hosted in NPM. So first we should have the static content published in NPM{:target=\u0026quot;_blank\u0026quot;}.\nNPM Package creation 1. Create the directory for adding packages for NPM {% highlight console %} mkdir npm mkdir npm/dist cd npm {% endhighlight %}\n2. Create a package.json file for your package {% highlight console %} npm init {% endhighlight %}\n{% highlight console %} This utility will walk you through creating a package.json file. It only covers the most common items, and tries to guess sensible defaults.\nSee npm help json for definitive documentation on these fields and exactly what they do.\nUse npm install pkg afterwards to install a package and save it as a dependency in the package.json file.\nPress ^C at any time to quit. package name: (npm) vikki-tools version: (1.0.0) 1.0.7 description: Libraries for https://tools.vikki.in entry point: (index.js) dist/index.js test command: git repository: https://github.com/vignesh88/tools.git keywords: vikki, tools author: Vignesh Ragupathy license: (ISC) About to write to /home/vikki/npm/package.json:\n{% endhighlight %} {% highlight json linenos %} { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;vikki-tools\u0026rdquo;, \u0026ldquo;version\u0026rdquo;: \u0026ldquo;1.0.7\u0026rdquo;, \u0026ldquo;description\u0026rdquo;: \u0026ldquo;Libraries for https://tools.vikki.in\u0026rdquo;, \u0026ldquo;main\u0026rdquo;: \u0026ldquo;dist/index.js\u0026rdquo;, \u0026ldquo;scripts\u0026rdquo;: { \u0026ldquo;test\u0026rdquo;: \u0026ldquo;echo \u0026quot;Error: no test specified\u0026quot; \u0026amp;\u0026amp; exit 1\u0026rdquo; }, \u0026ldquo;repository\u0026rdquo;: { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;git\u0026rdquo;, \u0026ldquo;url\u0026rdquo;: \u0026ldquo;git+https://github.com/vignesh88/tools.git\u0026rdquo; }, \u0026ldquo;keywords\u0026rdquo;: [ \u0026ldquo;vikki\u0026rdquo;, \u0026ldquo;tools\u0026rdquo; ], \u0026ldquo;author\u0026rdquo;: \u0026ldquo;Vignesh Ragupathy\u0026rdquo;, \u0026ldquo;license\u0026rdquo;: \u0026ldquo;ISC\u0026rdquo;, \u0026ldquo;bugs\u0026rdquo;: { \u0026ldquo;url\u0026rdquo;: \u0026ldquo;https://github.com/vignesh88/tools/issues\" }, \u0026ldquo;homepage\u0026rdquo;: \u0026ldquo;https://github.com/vignesh88/tools#readme\" } {% endhighlight %} {% highlight console %} Is this OK? (yes) yes {% endhighlight %}\n3. Create a index.js I added a javascript function that will be used to copy text to clipboard.\n{% highlight console %} vim dist/index.js {% endhighlight %}\n{% highlight javascript linenos %} function copyToClipboard(x,y) { if( document.getElementById(x).value) { data_2_copy = document.getElementById(x).value; } else { data_2_copy = document.getElementById(x).innerText; }\nvar e = document.createElement(\u0026ldquo;textarea\u0026rdquo;); e.style.opacity = \u0026ldquo;0\u0026rdquo;, e.style.position = \u0026ldquo;fixed\u0026rdquo;, e.textContent = data_2_copy; var t = document.getElementsByTagName(\u0026ldquo;body\u0026rdquo;)[0]; t.appendChild(e), e.select(), document.execCommand(\u0026ldquo;copy\u0026rdquo;), t.removeChild(e), $(y).show(), setTimeout((function () { $(y).hide() }), 1e3) } {% endhighlight %}\n4. Copy all your static content to dist directory Now lets copy all our static content to the dist directory. I have various css,images,javascript that will be used in various app inside my django application.\nBelow are the files which i copied.\n{% highlight console %} tree . . ‚îú‚îÄ‚îÄ dist ‚îÇ¬†‚îú‚îÄ‚îÄ admin ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ css ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ autocomplete.css ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ base.css ‚îÇ¬†‚îú‚îÄ‚îÄ geoip ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ css ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ geoip_dark.css ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ geoip_light.css ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ js ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ geoip.js ‚îÇ¬†‚îú‚îÄ‚îÄ index.js ‚îÇ¬†‚îî‚îÄ‚îÄ password_generator ‚îÇ¬†‚îú‚îÄ‚îÄ css ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ password_generator_dark.css ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ password_generator_light.css ‚îÇ¬†‚îú‚îÄ‚îÄ img ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ copy-full.svg ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ regenerate.svg ‚îÇ¬†‚îî‚îÄ‚îÄ js ‚îÇ¬†‚îî‚îÄ‚îÄ password_generator.js ‚îú‚îÄ‚îÄ package.json ‚îî‚îÄ‚îÄ README.md {% endhighlight %}\n5. Publish you static content as package in NPM Now we are all set, let\u0026rsquo;s connect to NPM and publish our package.\nYou should already have an account in NPM to publish.\n{% highlight console %} npm login Username: r_vignesh Password: Email: (this IS public) me@vikki.in Logged in as r_vignesh on https://registry.npmjs.org/. {% endhighlight %}\n{% highlight console %} npm publish\nnpm notice npm notice package: vikki-tools@1.0.7 npm notice === Tarball Contents === npm notice 1.1kB dist/admin/img/LICENSE npm notice 8.4kB dist/admin/css/autocomplete.css npm notice 16.4kB dist/admin/css/base.css npm notice 698B dist/base64/css/base64_dark.css npm notice 159B dist/base64/css/base64_light.css npm notice 85.9kB dist/admin/fonts/Roboto-Regular-webfont.woff npm notice === Tarball Details === npm notice name: vikki-tools npm notice version: 1.0.7 npm notice package size: 1.1 MB npm notice unpacked size: 3.9 MB npm notice shasum: a9153c3a9bb68bc34d5040d2088a5b95a256e4cc npm notice integrity: sha512-zynWl1/pL0Wvk[\u0026hellip;]k3yhkCzBz7+0A== npm notice total files: 188 npm notice\nvikki-tools@1.0.7 {% endhighlight %} That\u0026rsquo;s it. Now we have the package published in NPM.\nUnpkg and Jsdelivr provide CDN for NPM packages without any configuration.\n6. Verify published package in NPM Lets try to access it using unpkg. Open your browser and enter the url in the below format.\nhttps://unpkg.com/pacakage/\nFor using specific version https://unpkg.com/package@version/:file\nMy package name is vikki-tools so the format will be https://unpkg.com/vikki-tools/{:target=\u0026quot;_blank\u0026rdquo;}.\nThe leading / at the end of the URL is important.\nScreenshot from browser Using Unpkg to serve static content in website We can now load the static content from NPM on our website.\n{% highlight html linenos %}\n{% endhighlight %} Using Jsdelivr to serve static content in website We can also use Jsdelivr instead of unpkg.\n{% highlight html linenos %}\n{% endhighlight %} Auto minified version from jsdelivr Jsdelivr also provides the auto minified version of the CSS and Javascript from NPM. If you want to use minified version css and js, just add .min extension to the filename\n{% highlight html linenos %}\n{% endhighlight %} Script to automatically update the static and CDN URL in Django For ease, I created a script to automatically update all static content in your template directory in the Django application.\nThe code is available in the Github URL{:target=\u0026quot;_blank\u0026rdquo;}\nDemo video ","permalink":"http://localhost:1313/posts/2020-06-12-publish-package-in-npm-and-serve-the-static-content-from-cdn/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/content/images/cover/npm.jpg\"\u003e\n\u003cem\u003ePhoto by \u003ca href=\"https://photography.vikki.in/vikki-photography-budapest-3\"\u003eVignesh Ragupathy\u003c/a\u003e{:target=\u0026quot;_blank\u0026quot;}.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eI have been utilizing AWS to host my personal blog for almost 3 years now. Originally my blog was hosted in WordPress and then I migrated to \u003ca href=\"https://ghost.org/\"\u003eghost\u003c/a\u003e{:target=\u0026quot;_blank\u0026quot;}. It\u0026rsquo;s been 2 years now in ghost and I thought of exploring a new hosting option which should be free, supports custom domain name and free \u003ca href=\"https://letsencrypt.org/\"\u003eSSL\u003c/a\u003e{:target=\u0026quot;_blank\u0026quot;}.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://jekyllrb.com/\"\u003eJekyll\u003c/a\u003e{:target=\u0026quot;_blank\u0026quot;} is a ruby based static blog generator and it has an advantage of free hosting in GitHub. The letsencrypt SSL certificate is also provided by GitHub for my custom domain so i don‚Äôt have to worry about managing it.\u003c/p\u003e","title":"Publish package in NPM and serve the static content from CDN"},{"content":"Kubernetes cluster state is saved in etcd datastore. In the post we are going to see how to take a backup for etcd database in kubernetes cluster.\nSetup I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\nStep 1: Create a directory and backup all certificates Kubernetes cluster have all the certificates saved in the defautl path /etc/kubernetes/pki. Take the backup of all the files and save it in the backup directory\n{% highlight console %}\nvikki@kubernetes1:$ mkdir backup vikki@kubernetes1:$ cd backup/ {% endhighlight %}\n{% highlight console %}\nvikki@kubernetes1:~/backup$ sudo cp -rvf /etc/kubernetes/pki . \u0026lsquo;/etc/kubernetes/pki\u0026rsquo; -\u0026gt; \u0026lsquo;./pki\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/ca.key\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/ca.key\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/ca.crt\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/ca.crt\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/apiserver.key\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/apiserver.key\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/apiserver.crt\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/apiserver.crt\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/apiserver-kubelet-client.key\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/apiserver-kubelet-client.key\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/apiserver-kubelet-client.crt\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/apiserver-kubelet-client.crt\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/front-proxy-ca.key\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/front-proxy-ca.key\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/front-proxy-ca.crt\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/front-proxy-ca.crt\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/front-proxy-client.key\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/front-proxy-client.key\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/front-proxy-client.crt\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/front-proxy-client.crt\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/etcd\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/etcd\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/etcd/ca.key\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/etcd/ca.key\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/etcd/ca.crt\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/etcd/ca.crt\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/etcd/server.key\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/etcd/server.key\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/etcd/server.crt\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/etcd/server.crt\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/etcd/peer.key\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/etcd/peer.key\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/etcd/peer.crt\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/etcd/peer.crt\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/etcd/healthcheck-client.key\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/etcd/healthcheck-client.key\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/etcd/healthcheck-client.crt\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/etcd/healthcheck-client.crt\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/apiserver-etcd-client.key\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/apiserver-etcd-client.key\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/apiserver-etcd-client.crt\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/apiserver-etcd-client.crt\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/sa.key\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/sa.key\u0026rsquo; \u0026lsquo;/etc/kubernetes/pki/sa.pub\u0026rsquo; -\u0026gt; \u0026lsquo;./pki/sa.pub\u0026rsquo;\n{% endhighlight %}\nStep 2: Download the etcdctl binary Download the etcdctl binary. I have created a shortlinks for the etcd-v3.2.28 which works in ubuntu16.04 and kubernetes v16.\n{% highlight console %}\nvikki@kubernetes1:~/backup$ wget shortlinks.vikki.in/etcd \u0026ndash;2019-11-26 22:05:52\u0026ndash; http://shortlinks.vikki.in/etcd Resolving shortlinks.vikki.in (shortlinks.vikki.in)\u0026hellip; 172.217.160.179, 2404:6800:4007:80d::2013 Connecting to shortlinks.vikki.in (shortlinks.vikki.in)|172.217.160.179|:80\u0026hellip; connected. HTTP request sent, awaiting response\u0026hellip; 302 Found Location: https://github.com/vignesh88/blog/raw/master/kubernetes/etcd/etcd-v3.2.28-linux-amd64.tar.gz [following] \u0026ndash;2019-11-26 22:05:53\u0026ndash; https://github.com/vignesh88/blog/raw/master/kubernetes/etcd/etcd-v3.2.28-linux-amd64.tar.gz Resolving github.com (github.com)\u0026hellip; 13.234.176.102 Connecting to github.com (github.com)|13.234.176.102|:443\u0026hellip; connected. HTTP request sent, awaiting response\u0026hellip; 302 Found Location: https://raw.githubusercontent.com/vignesh88/blog/master/kubernetes/etcd/etcd-v3.2.28-linux-amd64.tar.gz [following] \u0026ndash;2019-11-26 22:05:54\u0026ndash; https://raw.githubusercontent.com/vignesh88/blog/master/kubernetes/etcd/etcd-v3.2.28-linux-amd64.tar.gz Resolving raw.githubusercontent.com (raw.githubusercontent.com)\u0026hellip; 151.101.8.133 Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.8.133|:443\u0026hellip; connected. HTTP request sent, awaiting response\u0026hellip; 200 OK Length: 10529149 (10M) [application/octet-stream] Saving to: ‚Äòetcd‚Äô\netcd 100%[=============================================================================\u0026gt;] 10.04M 4.47MB/s in 2.2s\n2019-11-26 22:05:57 (4.47 MB/s) - ‚Äòetcd‚Äô saved [10529149/10529149]\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~/backup$ ls etcd pki\n{% endhighlight %}\nExtract the etcd package\n{% highlight console %}\nvikki@kubernetes1:~/backup$ tar -xvf etcd etcd-v3.2.28-linux-amd64/ etcd-v3.2.28-linux-amd64/etcdctl etcd-v3.2.28-linux-amd64/etcd etcd-v3.2.28-linux-amd64/README-etcdctl.md etcd-v3.2.28-linux-amd64/README.md etcd-v3.2.28-linux-amd64/Documentation/ etcd-v3.2.28-linux-amd64/Documentation/faq.md etcd-v3.2.28-linux-amd64/Documentation/tuning.md etcd-v3.2.28-linux-amd64/Documentation/dl_build.md etcd-v3.2.28-linux-amd64/Documentation/benchmarks/ etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-benchmarks.md etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-3-demo-benchmarks.md etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-storage-memory-benchmark.md etcd-v3.2.28-linux-amd64/Documentation/benchmarks/_index.md etcd-v3.2.28-linux-amd64/Documentation/benchmarks/README.md etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-2-1-0-alpha-benchmarks.md etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-memory-benchmarks.md etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-2-2-0-benchmarks.md etcd-v3.2.28-linux-amd64/Documentation/benchmarks/etcd-3-watch-memory-benchmark.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/ etcd-v3.2.28-linux-amd64/Documentation/op-guide/security.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/authentication.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/recovery.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/container.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/supported-platform.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/monitoring.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/clustering.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/etcd3_alert.rules etcd-v3.2.28-linux-amd64/Documentation/op-guide/grafana.json etcd-v3.2.28-linux-amd64/Documentation/op-guide/_index.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/versioning.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/maintenance.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/gateway.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/runtime-configuration.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/performance.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/v2-migration.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/configuration.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/etcd-sample-grafana.png etcd-v3.2.28-linux-amd64/Documentation/op-guide/hardware.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/grpc_proxy.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/runtime-reconf-design.md etcd-v3.2.28-linux-amd64/Documentation/op-guide/failures.md etcd-v3.2.28-linux-amd64/Documentation/dev-guide/ etcd-v3.2.28-linux-amd64/Documentation/dev-guide/api_concurrency_reference_v3.md etcd-v3.2.28-linux-amd64/Documentation/dev-guide/experimental_apis.md etcd-v3.2.28-linux-amd64/Documentation/dev-guide/api_grpc_gateway.md etcd-v3.2.28-linux-amd64/Documentation/dev-guide/_index.md etcd-v3.2.28-linux-amd64/Documentation/dev-guide/interacting_v3.md etcd-v3.2.28-linux-amd64/Documentation/dev-guide/grpc_naming.md etcd-v3.2.28-linux-amd64/Documentation/dev-guide/local_cluster.md etcd-v3.2.28-linux-amd64/Documentation/dev-guide/api_reference_v3.md etcd-v3.2.28-linux-amd64/Documentation/dev-guide/limit.md etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/ etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/swagger/ etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/swagger/v3lock.swagger.json etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/swagger/rpc.swagger.json etcd-v3.2.28-linux-amd64/Documentation/dev-guide/apispec/swagger/v3election.swagger.json etcd-v3.2.28-linux-amd64/Documentation/_index.md etcd-v3.2.28-linux-amd64/Documentation/README.md etcd-v3.2.28-linux-amd64/Documentation/upgrades/ etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_2.md etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_1.md etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrading-etcd.md etcd-v3.2.28-linux-amd64/Documentation/upgrades/_index.md etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_4.md etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_0.md etcd-v3.2.28-linux-amd64/Documentation/upgrades/upgrade_3_3.md etcd-v3.2.28-linux-amd64/Documentation/reporting_bugs.md etcd-v3.2.28-linux-amd64/Documentation/production-users.md etcd-v3.2.28-linux-amd64/Documentation/branch_management.md etcd-v3.2.28-linux-amd64/Documentation/platforms/ etcd-v3.2.28-linux-amd64/Documentation/platforms/aws.md etcd-v3.2.28-linux-amd64/Documentation/platforms/_index.md etcd-v3.2.28-linux-amd64/Documentation/platforms/freebsd.md etcd-v3.2.28-linux-amd64/Documentation/platforms/container-linux-systemd.md etcd-v3.2.28-linux-amd64/Documentation/demo.md etcd-v3.2.28-linux-amd64/Documentation/dev-internal/ etcd-v3.2.28-linux-amd64/Documentation/dev-internal/logging.md etcd-v3.2.28-linux-amd64/Documentation/dev-internal/discovery_protocol.md etcd-v3.2.28-linux-amd64/Documentation/dev-internal/_index.md etcd-v3.2.28-linux-amd64/Documentation/dev-internal/release.md etcd-v3.2.28-linux-amd64/Documentation/learning/ etcd-v3.2.28-linux-amd64/Documentation/learning/api_guarantees.md etcd-v3.2.28-linux-amd64/Documentation/learning/glossary.md etcd-v3.2.28-linux-amd64/Documentation/learning/why.md etcd-v3.2.28-linux-amd64/Documentation/learning/auth_design.md etcd-v3.2.28-linux-amd64/Documentation/learning/data_model.md etcd-v3.2.28-linux-amd64/Documentation/learning/api.md etcd-v3.2.28-linux-amd64/Documentation/docs.md etcd-v3.2.28-linux-amd64/Documentation/integrations.md etcd-v3.2.28-linux-amd64/Documentation/rfc/ etcd-v3.2.28-linux-amd64/Documentation/rfc/_index.md etcd-v3.2.28-linux-amd64/Documentation/rfc/v3api.md etcd-v3.2.28-linux-amd64/Documentation/metrics.md etcd-v3.2.28-linux-amd64/READMEv2-etcdctl.md\n{% endhighlight %}\nNavigate to the extraced directory\n{% highlight console %}\nvikki@kubernetes1:~/backup$ ls etcd etcd-v3.2.28-linux-amd64 pki\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~/backup$ cd etcd-v3.2.28-linux-amd64/\n{% endhighlight %}\nStep 3: Backup the etcd database Now you will see a etcdctl binary inside the directy.\nUse the key, certificate and CA certificate to take backup of the existing etcd database as shown below\n{% highlight console %}\nvikki@kubernetes1:~/backup/etcd-v3.2.28-linux-amd64$ sudo ETCDCTL_API=3 ./etcdctl \u0026ndash;endpoints https://127.0.0.1:2379 \u0026ndash;cert=/etc/kubernetes/pki/etcd/server.crt \u0026ndash;key=/etc/kubernetes/pki/etcd/server.key \u0026ndash;cacert=/etc/kubernetes/pki/etcd/ca.crt snapshot save ../etc_database_backup.db Snapshot saved at ../etc_database_backup.db\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:/backup/etcd-v3.2.28-linux-amd64$ cd .. vikki@kubernetes1:/backup$ ls etcd etc_database_backup.db etcd-v3.2.28-linux-amd64 pki\n{% endhighlight %}\nNow we can see the backup has been taken and saved as etc_database_backup.db\n","permalink":"http://localhost:1313/posts/2019-11-28-backup-of-etcd-database-in-kubernetes/","summary":"\u003cp\u003eKubernetes cluster state is saved in etcd datastore. In the post we are going to see how to take a backup for etcd database in kubernetes cluster.\u003c/p\u003e\n\u003ch3 id=\"setup\"\u003e\u003cstrong\u003eSetup\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eI am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\u003c/p\u003e\n\u003c!--kg-card-begin: image--\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"/content/images/2019/11/setup-7.jpg\" class=\"kg-image\"\u003e\u003c/figure\u003e\u003c!--kg-card-end: image--\u003e\n\u003ch5 id=\"step-1-create-a-directory-and-backup-all-certificates\"\u003eStep 1: Create a directory and backup all certificates\u003c/h5\u003e\n\u003cp\u003eKubernetes cluster have all the certificates saved in the defautl path /etc/kubernetes/pki. Take the backup of all the files and save it in the backup directory\u003c/p\u003e","title":"Backup of etcd database in kubernetes"},{"content":"There are 3 elements involved in RBAC. In this post we are going to see how to provide user level access to resources.\nSubjects - Users or Process that wants access to Kubernetes API Resources - Kubernetes API objects like pods, deployments etc Verbs - Set of operations like get, watch create etc Setup I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\nStep 1: Create a private key Create a new directly and navigate to the directory\n{% highlight console %}\nvikki@kubernetes1:$ mkdir ssl vikki@kubernetes1:$ cd ssl/\n{% endhighlight %}\nUse openssl command a generate a private key user1.key\n{% highlight console %}\nvikki@kubernetes1:/ssl$ openssl genrsa -out user1.key 2048 Generating RSA private key, 2048 bit long modulus \u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;.+++ \u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;+++ e is 65537 (0x10001) vikki@kubernetes1:/ssl$ ls user1.key\n{% endhighlight %}\nStep 2: Generate a CSR Use the private key generated in the privous step and generate the certificate signing request(csr)\n{% highlight console %}\nvikki@kubernetes1:~/ssl$ openssl req -new -key user1.key -out user1.csr -subj \u0026ldquo;/CN=user1/O=vikki.in\u0026rdquo;\nvikki@kubernetes1:~/ssl$ ls user1.csr user1.key\n{% endhighlight %}\nStep 3: Sign the CSR and generate certificate The kubernetes cluster have the CA(certificate authority) key and certificate available under /etc/kubernetes/pki location\n{% highlight console %}\nvikki@kubernetes1:~/ssl$ ls /etc/kubernetes/pki/ca. ca.crt ca.key\n{% endhighlight %}\nUse the CA certificate and key to sign the CSR\n{% highlight console %}\nvikki@kubernetes1:~/ssl$ sudo openssl x509 -req -in user1.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out user1.crt -days 365 [sudo] password for vikki: Signature ok subject=/CN=user1/O=vikki.in Getting CA Private Key\nvikki@kubernetes1:~/ssl$ ls user1.crt user1.csr user1.key\n{% endhighlight %}\nNow we have the private key user1.key and signed certificate user1.crt\nStep 4: Set credential for the user Now set credential for the user user1 with the private key and the signed certificate.\n{% highlight console %}\nvikki@kubernetes1:~/ssl$ kubectl config set-credentials user1 \u0026ndash;client-certificate=user1.crt \u0026ndash;client-key=user1.key User \u0026ldquo;user1\u0026rdquo; set.\n{% endhighlight %}\nStep 5: Set context for the user Now set the new context with the username , cluster etc.\nWe can also map the context to specific namespace using the \u0026ndash;namespacec option. By default it will take the default namespace\n{% highlight console %}\nvikki@kubernetes1:~/ssl$ kubectl config set-context user1-context \u0026ndash;cluster=kubernetes \u0026ndash;user=user1 Context \u0026ldquo;user1-context\u0026rdquo; created.\nvikki@kubernetes1:~/ssl$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE\nkubernetes-admin@kubernetes kubernetes kubernetes-admin\nuser1-context kubernetes user1 {% endhighlight %}\nNow we can see there are 2 context created.\nStep 6: Create a role Create a role and map the resources and verb required.\n{% highlight console %}\nvikki@kubernetes1:~/ssl$ kubectl create role myrole \u0026ndash;verb=get,create,list \u0026ndash;resource=pods role.rbac.authorization.k8s.io/myrole created\n{% endhighlight %}\nStep 7: Create a rolebinding Create a rolebinding and map the role and user.\n{% highlight console %}\nvikki@kubernetes1:~/ssl$ kubectl create rolebinding myrolebinding \u0026ndash;role=myrole \u0026ndash;user=user1 rolebinding.rbac.authorization.k8s.io/myrolebinding created\n{% endhighlight %}\nStep 8: Verify role and rolebinding List the role and rolebinding and verify both are created.\n{% highlight console %}\nvikki@kubernetes1:/ssl$ kubectl get role NAME AGE myrole 58s vikki@kubernetes1:/ssl$ kubectl get rolebindings.rbac.authorization.k8s.io NAME AGE myrolebinding 8s\n{% endhighlight %}\nStep 9: Change the context and verify role based access {% highlight console %}\nvikki@kubernetes1:~/ssl$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE\nkubernetes-admin@kubernetes kubernetes kubernetes-admin\nuser1-context kubernetes user1 {% endhighlight %}\nSwitch to newly created contesxt user1-context\n{% highlight console %}\nvikki@kubernetes1:~/ssl$ kubectl config use-context user1-context Switched to context \u0026ldquo;user1-context\u0026rdquo;.\nvikki@kubernetes1:~/ssl$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE kubernetes-admin@kubernetes kubernetes kubernetes-admin\nuser1-context kubernetes user1 {% endhighlight %}\nNow we can see the current context is switched to user1-context.\nTry to create a deployement\n{% highlight console %}\nvikki@kubernetes1:~/ssl$ kubectl run nginx-deployment \u0026ndash;image=nginx kubectl run \u0026ndash;generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run \u0026ndash;generator=run-pod/v1 or kubectl create instead. Error from server (Forbidden): deployments.apps is forbidden: User \u0026ldquo;user1\u0026rdquo; cannot create resource \u0026ldquo;deployments\u0026rdquo; in API group \u0026ldquo;apps\u0026rdquo; in the namespace \u0026ldquo;default\u0026rdquo;\n{% endhighlight %}\nThe deployment creation is failed because the new context has resource mapped only for pod.\nNow lets create a pod and verify the status\n{% highlight console %}\nvikki@kubernetes1:~/ssl$ vim pod.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~/ssl$ kubectl create -f pod.yaml pod/myapp-pod created\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~/ssl$ kubectl get pods NAME READY STATUS RESTARTS AGE httpd-7765f5994-vc2j5 1/1 Running 1 2d myapp-pod 0/1 ContainerCreating 0 5s nginx-7bffc778db-p4ff5 1/1 Running 1 2d\n{% endhighlight %}\nWe can see now the pod created successfully and running in the new context.\nWe can also test the permission of user using the below command\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl auth can-i create deployments \u0026ndash;as user1 no\nvikki@kubernetes1:~$ kubectl auth can-i create pods \u0026ndash;as user1 yes\n{% endhighlight %}\n","permalink":"http://localhost:1313/posts/2019-11-28-rbac-in-kubernetes/","summary":"\u003cp\u003eThere are 3 elements involved in RBAC. In this post we are going to see how to provide user level access to resources.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSubjects - Users or Process that wants access to Kubernetes API\u003c/li\u003e\n\u003cli\u003eResources - Kubernetes API objects like pods, deployments etc\u003c/li\u003e\n\u003cli\u003eVerbs - Set of operations like get, watch create etc\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"setup\"\u003e\u003cstrong\u003eSetup\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eI am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\u003c/p\u003e","title":"RBAC in kubernetes"},{"content":"This post we are going to discuss how to upgrade the kubernetes cluster, both master and worker nodes. We are going to upgrade a older version v1.15 to v.1.16.\nSetup I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\nMaster node Step 1: Verify the current version of kubelet and kubeadm running in all nodes {% highlight console %}\nvikki@kubernetes1:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes1 Ready master 41d v1.15.5 kubernetes2 Ready 41d v1.15.5\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubeadm version kubeadm version: \u0026amp;version.Info{Major:\u0026ldquo;1\u0026rdquo;, Minor:\u0026ldquo;15\u0026rdquo;, GitVersion:\u0026ldquo;v1.15.5\u0026rdquo;, GitCommit:\u0026ldquo;20c265fef0741dd71a66480e35bd69f18351daea\u0026rdquo;, GitTreeState:\u0026ldquo;clean\u0026rdquo;, BuildDate:\u0026ldquo;2019-10-15T19:14:19Z\u0026rdquo;, GoVersion:\u0026ldquo;go1.12.10\u0026rdquo;, Compiler:\u0026ldquo;gc\u0026rdquo;, Platform:\u0026ldquo;linux/amd64\u0026rdquo;}\n{% endhighlight %}\nWe can see kubelet and kubeadm is running in version v1.15.5\nStep 2: Verify the lastest stable availble version Run the update in kubernetes master node\n{% highlight console %}\nroot@kubernetes1:~# apt update\n{% endhighlight %}{% highlight console %}\nroot@kubernetes1:~# apt-cache policy kubeadm kubeadm: Installed: 1.15.5-00 Candidate: 1.16.3-00 Version table: \u0026hellip;..\n{% endhighlight %}\nWe can see from this output that current version is 1.15.5 and the latest available is 1.16.3\nStep 3: Upgrade the kubeadm to latest version Upgrade the kubeadm in master to the latest version 1.16.3\n{% highlight console %}\nroot@kubernetes1:~# apt-mark unhold kubeadm \u0026amp;\u0026amp; \\\napt-get update \u0026amp;\u0026amp; apt-get install -y kubeadm=1.16.3-00 \u0026amp;\u0026amp; apt-mark hold kubeadm kubeadm was already not hold. Hit:1 http://us.archive.ubuntu.com/ubuntu xenial InRelease Hit:2 http://security.ubuntu.com/ubuntu xenial-security InRelease Hit:3 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease Hit:4 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease Hit:5 https://apt.kubernetes.io kubernetes-xenial InRelease Reading package lists\u0026hellip; Done Reading package lists\u0026hellip; Done Building dependency tree Reading state information\u0026hellip; Done The following packages will be upgraded: kubeadm 1 upgraded, 0 newly installed, 0 to remove and 142 not upgraded. Need to get 8,762 kB of archives. After this operation, 4,062 kB of additional disk space will be used. Get:1 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubeadm amd64 1.16.3-00 [8,762 kB] Fetched 8,762 kB in 7s (1,117 kB/s) (Reading database \u0026hellip; 60808 files and directories currently installed.) Preparing to unpack \u0026hellip;/kubeadm_1.16.3-00_amd64.deb \u0026hellip; Unpacking kubeadm (1.16.3-00) over (1.15.5-00) \u0026hellip; Setting up kubeadm (1.16.3-00) \u0026hellip; kubeadm set on hold.\n{% endhighlight %}\nStep 4: Run the upgrade plan Now in the previous step we already updated the kubeadm to latest version. Now lets run the upgrade plan and see the available options.\n{% highlight console %}\nroot@kubernetes1:~# sudo kubeadm upgrade plan [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster\u0026hellip; [upgrade/config] FYI: You can look at this config file with \u0026lsquo;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026rsquo; [preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: v1.15.5 [upgrade/versions] kubeadm version: v1.16.3 [upgrade/versions] Latest stable version: v1.16.3 [upgrade/versions] Latest version in the v1.15 series: v1.15.6\nComponents that must be upgraded manually after you have upgraded the control plane with \u0026lsquo;kubeadm upgrade apply\u0026rsquo;: COMPONENT CURRENT AVAILABLE Kubelet 2 x v1.15.5 v1.15.6\nUpgrade to the latest version in the v1.15 series:\nCOMPONENT CURRENT AVAILABLE API Server v1.15.5 v1.15.6 Controller Manager v1.15.5 v1.15.6 Scheduler v1.15.5 v1.15.6 Kube Proxy v1.15.5 v1.15.6 CoreDNS 1.3.1 1.6.2 Etcd 3.3.10 3.3.10\nYou can now apply the upgrade by executing the following command:\nkubeadm upgrade apply v1.15.6 Components that must be upgraded manually after you have upgraded the control plane with \u0026lsquo;kubeadm upgrade apply\u0026rsquo;: COMPONENT CURRENT AVAILABLE Kubelet 2 x v1.15.5 v1.16.3\nUpgrade to the latest stable version:\nCOMPONENT CURRENT AVAILABLE API Server v1.15.5 v1.16.3 Controller Manager v1.15.5 v1.16.3 Scheduler v1.15.5 v1.16.3 Kube Proxy v1.15.5 v1.16.3 CoreDNS 1.3.1 1.6.2 Etcd 3.3.10 3.3.15-0\nYou can now apply the upgrade by executing the following command:\nkubeadm upgrade apply v1.16.3 {% endhighlight %}\nWe can see that we can upgrade the cluster to v1.16.3 from the above output\nStep 5: Drain the pods in master node Now drain all the pods execpt daemonsets in the master node\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl drain kubernetes1 \u0026ndash;ignore-daemonsets node/kubernetes1 cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-amd64-bfr9j, kube-system/kube-proxy-8267n evicting pod \u0026ldquo;coredns-5c98db65d4-v4cms\u0026rdquo; evicting pod \u0026ldquo;coredns-5c98db65d4-g4gv4\u0026rdquo; pod/coredns-5c98db65d4-v4cms evicted pod/coredns-5c98db65d4-g4gv4 evicted node/kubernetes1 evicted\n{% endhighlight %}\nStep 6: Run the upgrade Now run the upgrade to the lastest version 1.16.3\n{% highlight console %}\nvikki@kubernetes1:~$ sudo kubeadm upgrade apply v1.16.3 [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster\u0026hellip; [upgrade/config] FYI: You can look at this config file with \u0026lsquo;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026rsquo; [preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade/version] You have chosen to change the cluster version to \u0026ldquo;v1.16.3\u0026rdquo; [upgrade/versions] Cluster version: v1.15.5 [upgrade/versions] kubeadm version: v1.16.3 [upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y [upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd] [upgrade/prepull] Prepulling image for component etcd. [upgrade/prepull] Prepulling image for component kube-apiserver. [upgrade/prepull] Prepulling image for component kube-controller-manager. [upgrade/prepull] Prepulling image for component kube-scheduler. [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [upgrade/prepull] Prepulled image for component etcd. [upgrade/prepull] Prepulled image for component kube-apiserver. [upgrade/prepull] Prepulled image for component kube-controller-manager. [upgrade/prepull] Prepulled image for component kube-scheduler. [upgrade/prepull] Successfully prepulled the images for all the control plane components [upgrade/apply] Upgrading your Static Pod-hosted control plane to version \u0026ldquo;v1.16.3\u0026rdquo;\u0026hellip; Static pod: kube-apiserver-kubernetes1 hash: b9d225e73ea8b0b21921bdd78ea5415e Static pod: kube-controller-manager-kubernetes1 hash: f106f0d94b93b77e4db974b1c477d277 Static pod: kube-scheduler-kubernetes1 hash: 131c3f63daec7c0750818f64a2f75d20 [upgrade/etcd] Upgrading to TLS for etcd Static pod: etcd-kubernetes1 hash: 352a2620657e49493504dc8f27c83195 [upgrade/staticpods] Preparing for \u0026ldquo;etcd\u0026rdquo; upgrade [upgrade/staticpods] Renewing etcd-server certificate [upgrade/staticpods] Renewing etcd-peer certificate [upgrade/staticpods] Renewing etcd-healthcheck-client certificate [upgrade/staticpods] Moved new manifest to \u0026ldquo;/etc/kubernetes/manifests/etcd.yaml\u0026rdquo; and backed up old manifest to \u0026ldquo;/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-11-26-18-04-49/etcd.yaml\u0026rdquo; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s) Static pod: etcd-kubernetes1 hash: 352a2620657e49493504dc8f27c83195 Static pod: etcd-kubernetes1 hash: 3ce412f7cfe0c06939809c93f738e798 [apiclient] Found 1 Pods for label selector component=etcd [upgrade/staticpods] Component \u0026ldquo;etcd\u0026rdquo; upgraded successfully! [upgrade/etcd] Waiting for etcd to become available [upgrade/staticpods] Writing new Static Pod manifests to \u0026ldquo;/etc/kubernetes/tmp/kubeadm-upgraded-manifests001214141\u0026rdquo; [upgrade/staticpods] Preparing for \u0026ldquo;kube-apiserver\u0026rdquo; upgrade [upgrade/staticpods] Renewing apiserver certificate [upgrade/staticpods] Renewing apiserver-kubelet-client certificate [upgrade/staticpods] Renewing front-proxy-client certificate [upgrade/staticpods] Renewing apiserver-etcd-client certificate [upgrade/staticpods] Moved new manifest to \u0026ldquo;/etc/kubernetes/manifests/kube-apiserver.yaml\u0026rdquo; and backed up old manifest to \u0026ldquo;/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-11-26-18-04-49/kube-apiserver.yaml\u0026rdquo; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s) Static pod: kube-apiserver-kubernetes1 hash: b9d225e73ea8b0b21921bdd78ea5415e Static pod: kube-apiserver-kubernetes1 hash: b9d225e73ea8b0b21921bdd78ea5415e Static pod: kube-apiserver-kubernetes1 hash: b9d225e73ea8b0b21921bdd78ea5415e Static pod: kube-apiserver-kubernetes1 hash: d8708cb2f25dd11bbb41dd9729149325 [apiclient] Found 1 Pods for label selector component=kube-apiserver [upgrade/staticpods] Component \u0026ldquo;kube-apiserver\u0026rdquo; upgraded successfully! [upgrade/staticpods] Preparing for \u0026ldquo;kube-controller-manager\u0026rdquo; upgrade [upgrade/staticpods] Renewing controller-manager.conf certificate [upgrade/staticpods] Moved new manifest to \u0026ldquo;/etc/kubernetes/manifests/kube-controller-manager.yaml\u0026rdquo; and backed up old manifest to \u0026ldquo;/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-11-26-18-04-49/kube-controller-manager.yaml\u0026rdquo; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s) Static pod: kube-controller-manager-kubernetes1 hash: f106f0d94b93b77e4db974b1c477d277 Static pod: kube-controller-manager-kubernetes1 hash: e6e76bb8264f2e84070efada35e93e71 [apiclient] Found 1 Pods for label selector component=kube-controller-manager [upgrade/staticpods] Component \u0026ldquo;kube-controller-manager\u0026rdquo; upgraded successfully! [upgrade/staticpods] Preparing for \u0026ldquo;kube-scheduler\u0026rdquo; upgrade [upgrade/staticpods] Renewing scheduler.conf certificate [upgrade/staticpods] Moved new manifest to \u0026ldquo;/etc/kubernetes/manifests/kube-scheduler.yaml\u0026rdquo; and backed up old manifest to \u0026ldquo;/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-11-26-18-04-49/kube-scheduler.yaml\u0026rdquo; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s) Static pod: kube-scheduler-kubernetes1 hash: 131c3f63daec7c0750818f64a2f75d20 Static pod: kube-scheduler-kubernetes1 hash: 8c5e33e50bb56e8adacd1cc99c56b2cb [apiclient] Found 1 Pods for label selector component=kube-scheduler [upgrade/staticpods] Component \u0026ldquo;kube-scheduler\u0026rdquo; upgraded successfully! [upload-config] Storing the configuration used in ConfigMap \u0026ldquo;kubeadm-config\u0026rdquo; in the \u0026ldquo;kube-system\u0026rdquo; Namespace [kubelet] Creating a ConfigMap \u0026ldquo;kubelet-config-1.16\u0026rdquo; in namespace kube-system with the configuration for the kubelets in the cluster [kubelet-start] Downloading configuration for the kubelet from the \u0026ldquo;kubelet-config-1.16\u0026rdquo; ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \u0026ldquo;/var/lib/kubelet/config.yaml\u0026rdquo; [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy\n[upgrade/successful] SUCCESS! Your cluster was upgraded to \u0026ldquo;v1.16.3\u0026rdquo;. Enjoy!\n[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven\u0026rsquo;t already done so.\n{% endhighlight %}\nWe can see the cluster is upgraded to lastest version v1.16.3\nStep 7: Uncordon the master node Now ucordon the master node\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl uncordon kubernetes1 node/kubernetes1 uncordoned\n{% endhighlight %}\nStep 8: Upgrade the kubelet Now in the previous steps we have upgraded the kubeadm and cluster , next we need to upgrade the kubelet and kubectl\n{% highlight console %}\nroot@kubernetes1:~# apt-mark unhold kubelet kubectl \u0026amp;\u0026amp; \\\napt-get update \u0026amp;\u0026amp; apt-get install -y kubelet=1.16.3-00 kubectl=1.16.3-00 \u0026amp;\u0026amp; apt-mark hold kubelet kubectl kubelet was already not hold. kubectl was already not hold. Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease Hit:2 http://us.archive.ubuntu.com/ubuntu xenial InRelease Hit:3 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease Hit:4 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease Hit:5 https://apt.kubernetes.io kubernetes-xenial InRelease Reading package lists\u0026hellip; Done Reading package lists\u0026hellip; Done Building dependency tree Reading state information\u0026hellip; Done The following packages will be upgraded: kubectl kubelet 2 upgraded, 0 newly installed, 0 to remove and 140 not upgraded. Need to get 29.9 MB of archives. After this operation, 7,134 kB of additional disk space will be used. Get:1 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubectl amd64 1.16.3-00 [9,233 kB] Get:2 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubelet amd64 1.16.3-00 [20.7 MB] Fetched 29.9 MB in 8s (3,436 kB/s) (Reading database \u0026hellip; 60808 files and directories currently installed.) Preparing to unpack \u0026hellip;/kubectl_1.16.3-00_amd64.deb \u0026hellip; Unpacking kubectl (1.16.3-00) over (1.15.5-00) \u0026hellip; Preparing to unpack \u0026hellip;/kubelet_1.16.3-00_amd64.deb \u0026hellip; Unpacking kubelet (1.16.3-00) over (1.15.5-00) \u0026hellip; Setting up kubectl (1.16.3-00) \u0026hellip; Setting up kubelet (1.16.3-00) \u0026hellip; kubelet set on hold. kubectl set on hold.\n{% endhighlight %}\nRestart the kubelet service\n{% highlight console %}\nroot@kubernetes1:~# sudo systemctl restart kubelet\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes1 Ready master 41d v1.16.3 kubernetes2 Ready 41d v1.15.5\n{% endhighlight %}\nNow we can see the master node kubernetes1 is upgraded to v1.16.3\nWorker node Step 1: Upgrade the kubeadm to version 1.16.3 {% highlight console %}\nroot@kubernetes2:~# apt-mark unhold kubeadm \u0026amp;\u0026amp; \\\napt-get update \u0026amp;\u0026amp; apt-get install -y kubeadm=1.16.3-00 \u0026amp;\u0026amp; apt-mark hold kubeadm kubeadm was already not hold. Get:1 http://security.ubuntu.com/ubuntu xenial-security InRelease [109 kB] Get:2 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [785 kB] Hit:3 http://us.archive.ubuntu.com/ubuntu xenial InRelease Get:4 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB] Get:5 https://apt.kubernetes.io kubernetes-xenial InRelease [161 B] Get:6 https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages [31.3 kB] Get:7 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB] Get:8 http://us.archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [1,071 kB] Get:9 http://security.ubuntu.com/ubuntu xenial-security/main i386 Packages [617 kB] Get:10 http://security.ubuntu.com/ubuntu xenial-security/main Translation-en [302 kB] Get:11 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [466 kB] Get:12 http://us.archive.ubuntu.com/ubuntu xenial-updates/main i386 Packages [882 kB] Get:13 http://security.ubuntu.com/ubuntu xenial-security/universe i386 Packages [401 kB] Get:14 http://security.ubuntu.com/ubuntu xenial-security/universe Translation-en [191 kB] Get:15 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [5,724 B] Get:16 http://security.ubuntu.com/ubuntu xenial-security/multiverse i386 Packages [5,888 B] Get:17 http://us.archive.ubuntu.com/ubuntu xenial-updates/main Translation-en [413 kB] Get:18 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [771 kB] Get:19 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe i386 Packages [700 kB] Get:20 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe Translation-en [324 kB] Get:21 http://us.archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 Packages [16.8 kB] Get:22 http://us.archive.ubuntu.com/ubuntu xenial-updates/multiverse i386 Packages [15.9 kB] Fetched 7,333 kB in 23s (315 kB/s) Reading package lists\u0026hellip; Done Reading package lists\u0026hellip; Done Building dependency tree Reading state information\u0026hellip; Done The following packages will be upgraded: kubeadm 1 upgraded, 0 newly installed, 0 to remove and 139 not upgraded. Need to get 8,762 kB of archives. After this operation, 4,062 kB of additional disk space will be used. Get:1 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubeadm amd64 1.16.3-00 [8,762 kB] Fetched 8,762 kB in 7s (1,122 kB/s) (Reading database \u0026hellip; 60808 files and directories currently installed.) Preparing to unpack \u0026hellip;/kubeadm_1.16.3-00_amd64.deb \u0026hellip; Unpacking kubeadm (1.16.3-00) over (1.15.5-00) \u0026hellip; Setting up kubeadm (1.16.3-00) \u0026hellip; kubeadm set on hold.\n{% endhighlight %}\nStep 2: Drain the pods in worker node Now drain all the pods , except daemonsets from the worker node kubernetes2. This command should be run in master node in case the kubenetes config is not mapped in worker.\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl drain kubernetes2 \u0026ndash;ignore-daemonsets node/kubernetes2 cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-amd64-8v6lb, kube-system/kube-proxy-877b2 evicting pod \u0026ldquo;coredns-5644d7b6d9-t8qw5\u0026rdquo; evicting pod \u0026ldquo;httpd-7765f5994-gvlj6\u0026rdquo; evicting pod \u0026ldquo;nginx-7bffc778db-phdb5\u0026rdquo; evicting pod \u0026ldquo;coredns-5644d7b6d9-frz4v\u0026rdquo; pod/coredns-5644d7b6d9-frz4v evicted pod/coredns-5644d7b6d9-t8qw5 evicted pod/httpd-7765f5994-gvlj6 evicted pod/nginx-7bffc778db-phdb5 evicted node/kubernetes2 evicted\n{% endhighlight %}\nStep 3: Upgrade the worker node {% highlight console %}\nroot@kubernetes2:~# sudo kubeadm upgrade node [upgrade] Reading configuration from the cluster\u0026hellip; [upgrade] FYI: You can look at this config file with \u0026lsquo;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026rsquo; [upgrade] Skipping phase. Not a control plane node[kubelet-start] Downloading configuration for the kubelet from the \u0026ldquo;kubelet-config-1.16\u0026rdquo; ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \u0026ldquo;/var/lib/kubelet/config.yaml\u0026rdquo; [upgrade] The configuration for this node was successfully updated! [upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.\n{% endhighlight %}\nStep 4: Upgrade the kubelet and kubectl In previous steps we upgraded the kubeadm and cluster. Now we need to upgrade the kubelet and kubectl.\n{% highlight console %}\nroot@kubernetes2:~# apt-mark unhold kubelet kubectl \u0026amp;\u0026amp; \\\napt-get update \u0026amp;\u0026amp; apt-get install -y kubelet=1.16.3-00 kubectl=1.16.3-00 \u0026amp;\u0026amp; apt-mark hold kubelet kubectl kubelet was already not hold. kubectl was already not hold. Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease Hit:2 http://us.archive.ubuntu.com/ubuntu xenial InRelease Hit:3 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease Hit:4 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease Hit:5 https://apt.kubernetes.io kubernetes-xenial InRelease Reading package lists\u0026hellip; Done Reading package lists\u0026hellip; Done Building dependency tree Reading state information\u0026hellip; Done The following packages will be upgraded: kubectl kubelet 2 upgraded, 0 newly installed, 0 to remove and 137 not upgraded. Need to get 29.9 MB of archives. After this operation, 3,447 kB of additional disk space will be used. Get:1 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubectl amd64 1.16.3-00 [9,233 kB] Get:2 https://apt.kubernetes.io kubernetes-xenial/main amd64 kubelet amd64 1.16.3-00 [20.7 MB] Fetched 29.9 MB in 32s (934 kB/s) (Reading database \u0026hellip; 60808 files and directories currently installed.) Preparing to unpack \u0026hellip;/kubectl_1.16.3-00_amd64.deb \u0026hellip; Unpacking kubectl (1.16.3-00) over (1.16.2-00) \u0026hellip; Preparing to unpack \u0026hellip;/kubelet_1.16.3-00_amd64.deb \u0026hellip; Unpacking kubelet (1.16.3-00) over (1.15.5-00) \u0026hellip; Setting up kubectl (1.16.3-00) \u0026hellip; Setting up kubelet (1.16.3-00) \u0026hellip; kubelet set on hold. kubectl set on hold.\n{% endhighlight %}\nRestart the kubelet service in worker node\n{% highlight console %}\nroot@kubernetes2:~# sudo systemctl restart kubelet\n{% endhighlight %}\nStep 5: Uncordon the worker node Now uncordon all the pods in kubernetes2 worker onde\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl uncordon kubernetes2 node/kubernetes2 uncordoned\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes1 Ready master 41d v1.16.3 kubernetes2 Ready 41d v1.16.3\n{% endhighlight %}\nWe can see now both the master and worker nodes are upgraded from v1.15.5 to v1.16.3\n","permalink":"http://localhost:1313/posts/2019-11-26-upgrading-kubernetes-cluster-master-and-worker-nodes/","summary":"\u003cp\u003eThis post we are going to discuss how to upgrade the kubernetes cluster, both master and worker nodes. We are going to upgrade a older version v1.15 to v.1.16.\u003c/p\u003e\n\u003ch3 id=\"setup\"\u003e\u003cstrong\u003eSetup\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eI am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\u003c/p\u003e\n\u003c!--kg-card-begin: image--\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"/content/images/2019/11/setup-5.jpg\" class=\"kg-image\"\u003e\u003c/figure\u003e\u003c!--kg-card-end: image--\u003e\n\u003ch3 id=\"master-node\"\u003eMaster node\u003c/h3\u003e\n\u003ch5 id=\"step-1-verify-the-current-version-of-kubelet-and-kubeadm-running-in-all-nodes\"\u003eStep 1: Verify the current version of kubelet and kubeadm running in all nodes\u003c/h5\u003e\n\u003cp\u003e{% highlight console %}\u003c/p\u003e","title":"Upgrading kubernetes cluster master and worker nodes"},{"content":"Kubernetes cluster have a default scheduler kube-scheduler. If the default scheduler does not suits our requirement we can also create our own scheduler. In the post we will discus how to create multiple scheduler and schedule pods based on different scheduler.\nSetup I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\nStep 1: Copy the default scheduler yaml and modify Copy the default scheduler yaml from master node and modify the name.\nIn this below example i am naming the custom schedule are my-scheduler. Add a ServiceAccount and ClusterRoleBinding to the yaml file as given below.\nThe key changes made are\nline 27: name: my-scheduler line 30: serviceAccountName: my-scheduler line 38: - \u0026ndash;leader-elect=false line 39: - \u0026ndash;scheduler-name=my-scheduler {% highlight console %} vikki@kubernetes1:$ sudo cp /etc/kubernetes/manifests/kube-scheduler.yaml custom-scheduler.yaml vikki@kubernetes1:$ sudo chmod 777 custom-scheduler.yaml vikki@kubernetes1:~$ vim custom-scheduler.yaml\n{% endhighlight %}\nStep 2: Create the custom scheduler {% highlight console %}\nvikki@kubernetes1:~$ kubectl create -f custom-scheduler.yaml serviceaccount/my-scheduler created clusterrolebinding.rbac.authorization.k8s.io/my-scheduler-as-kube-scheduler created pod/my-scheduler created\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pods -n=kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-55754f75c-fgs8g 1/1 Running 7 22d calico-node-4h72l 1/1 Running 7 22d calico-node-ld84s 1/1 Running 7 22d calico-node-lrfz9 1/1 Running 2 31h calico-node-ws576 1/1 Running 1 27h coredns-5644d7b6d9-2g6rs 1/1 Running 7 22d coredns-5644d7b6d9-ccxsg 1/1 Running 7 22d etcd-kubernetes1 1/1 Running 8 22d kube-apiserver-kubernetes1 1/1 Running 8 22d kube-controller-manager-kubernetes1 1/1 Running 8 22d kube-proxy-6xd8l 1/1 Running 2 31h kube-proxy-96q5x 1/1 Running 7 22d kube-proxy-njl6r 1/1 Running 7 22d kube-proxy-whlhw 1/1 Running 1 27h kube-scheduler-kubernetes1 1/1 Running 8 22d my-scheduler 1/1 Running 0 6s\n{% endhighlight %}\nNow we can see a new custom scheduler my-scheduler is running along with defautl scheduler kube-scheduler-kubernetes1\nStep 3: Edit the custerrole for kube-scheduler If RBAC is enabled on your cluster, you must update the system:kube-scheduler cluster role. Edit the clusterrole for kube-scheduler and modify.\nBelow are the key changes made in orignal file\nline 33: - my-scheduler line 131: - storageclasses {% highlight console %} vikki@kubernetes1:~$ kubectl edit clusterrole system:kube-scheduler\n{% endhighlight %}\nStep 4: Create pods with different types of scheduler Create a pod without explicitly mentioning any scheuler name. This pod should be scheduled by default scheduler kube-scheduler\n{% highlight console %}\nvikki@kubernetes1:~$ vim pod_default_scheduler.yaml\n{% endhighlight %}\nCreate a pod by explicitly mentioning custom scheuler name my-scheduler.\n{% highlight console %}\nvikki@kubernetes1:~$ vim pod_custom_scheduler.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:$ kubectl create -f pod_default_scheduler.yaml pod/nginx-pod-default-scheduler created vikki@kubernetes1:$ kubectl create -f pod_custom_scheduler.yaml pod/nginx-pod-custom-scheduler created\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:$ kubectl get pods nginx-pod-custom-scheduler nginx-pod-default-scheduler NAME READY STATUS RESTARTS AGE nginx-pod-custom-scheduler 1/1 Running 0 9m9s nginx-pod-default-scheduler 1/1 Running 0 9m13s vikki@kubernetes1:$\n{% endhighlight %}\nNow we can see both the pods are created and running\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl get events -o wide |grep nginx-pod-custom-scheduler |head -1 Normal Scheduled pod/nginx-pod-custom-scheduler my-scheduler Successfully assigned default/nginx-pod-custom-scheduler to kubernetes3 0 nginx-pod-custom-scheduler.15da76383c2c2859\nvikki@kubernetes1:~$ kubectl get events -o wide |grep nginx-pod-default-scheduler |head -1 Normal Scheduled pod/nginx-pod-default-scheduler default-scheduler Successfully assigned default/nginx-pod-default-scheduler to kubernetes3 0 nginx-pod-default-scheduler.15da76372f4444f7\n{% endhighlight %}\nFrom the events, we can see pod nginx-pod-custom-scheduler is scheduled by my-scheduler and nginx-pod-default-scheduler by default-scheduler.\n","permalink":"http://localhost:1313/posts/2019-11-25-running-kubernetes-custom-scheduler/","summary":"\u003cp\u003eKubernetes cluster have a default scheduler kube-scheduler. If the default scheduler does not suits our requirement we can also create our own scheduler. In the post we will discus how to create multiple scheduler and schedule pods based on different scheduler.\u003c/p\u003e\n\u003ch3 id=\"setup\"\u003e\u003cstrong\u003eSetup\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eI am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\u003c/p\u003e","title":"Running kubernetes custom scheduler"},{"content":"Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them.\nStatic pods automatically restarts if it crashes. Static Pods are always bound to one Kubelet on a specific node. In the post we will try creating a static pod and watch the behaviour on delete.\nSetup I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\nStep 1: Find the static pod manifest location Go to any node where you want to run the static pod. I want to run in kubernetes3 node. Look for the kubelet process and config.yaml file associated with it\n{% highlight console %}\nroot@kubernetes3:~# ps -aux |grep kubelet root 905 3.6 9.4 544516 95676 ? Ssl 16:49 9:13 /usr/bin/kubelet \u0026ndash;bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \u0026ndash;kubeconfig=/etc/kubernetes/kubelet.conf \u0026ndash;config=/var/lib/kubelet/config.yaml \u0026ndash;cgroup-driver=cgroupfs \u0026ndash;network-plugin=cni \u0026ndash;pod-infra-container-image=k8s.gcr.io/pause:3.1 root 29041 0.0 0.0 14224 940 pts/0 S+ 21:03 0:00 grep \u0026ndash;color=auto kubelet\n{% endhighlight %}\nNow grep for the staticPodPath in config file\n{% highlight console %}\nroot@kubernetes3:~# cat /var/lib/kubelet/config.yaml |grep static staticPodPath: /etc/kubernetes/manifests\n{% endhighlight %}\nStep 2: Go to the directory and add a yaml for pod {% highlight console %}\nroot@kubernetes3:~# cd /etc/kubernetes/manifests/ root@kubernetes3:/etc/kubernetes/manifests# vim static-pod.yaml\n{% endhighlight %}\nNow try to grep for the pod name in the docker container list\n{% highlight console %}\nroot@kubernetes3:/etc/kubernetes/manifests# docker ps -a |grep static-web e5bf8054343a nginx \u0026ldquo;nginx -g \u0026lsquo;daemon of‚Ä¶\u0026rdquo; 17 seconds ago Up 16 seconds k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0 138ea3819894 k8s.gcr.io/pause:3.1 \u0026ldquo;/pause\u0026rdquo; 23 seconds ago Up 22 seconds k8s_POD_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0\n{% endhighlight %}\nWe can see a new static pod is created automatically\nStep 3: Delete the container and watch the behaviour Now lets try to delete the static pod {% highlight console %}\nroot@kubernetes3:/etc/kubernetes/manifests# docker stop k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0 k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0 root@kubernetes3:/etc/kubernetes/manifests# docker rm k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0 k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0\n{% endhighlight %}{% highlight console %}\nroot@kubernetes3:/etc/kubernetes/manifests# docker ps -a |grep static-web d3d9713b937e nginx \u0026ldquo;nginx -g \u0026lsquo;daemon of‚Ä¶\u0026rdquo; 1 second ago Up Less than a second k8s_web_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_1 138ea3819894 k8s.gcr.io/pause:3.1 \u0026ldquo;/pause\u0026rdquo; About a minute ago Up About a minute k8s_POD_static-web-kubernetes3_default_b42924f0dce4ce471e92742ee9bf65d7_0\n{% endhighlight %}\nWe can see the new static pod is automatically creating\n","permalink":"http://localhost:1313/posts/2019-11-24-creating-static-pod-in-kubernetes/","summary":"\u003cp\u003eStatic Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them.\u003cbr\u003e\nStatic pods automatically restarts if it crashes. Static Pods are always bound to one Kubelet on a specific node. In the post we will try creating a static pod and watch the behaviour on delete.\u003c/p\u003e\n\u003ch3 id=\"setup\"\u003e\u003cstrong\u003eSetup\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eI am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\u003c/p\u003e","title":"Creating static pod in kubernetes"},{"content":"We can assign the pod to node based on various methods. Lets discuss all the below methods in the post\nUsing nodeName Using labels in nodeSelector Node Affinity/Anti Affinity Pod Affinity/Anti Affinity Taints and tolerations Setup I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\nUsing nodeName Step 1: Create a pod and assign using nodeName {% highlight console %}\nvikki@kubernetes1:~$ vim pod_node_name.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl apply -f pod_node_name.yaml pod/nginx-pod-nodename created\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES busybox 1/1 Running 4 5h47m 192.168.249.141 kubernetes2 nginx-pod-nodename 1/1 Running 0 8s 192.168.80.199 kubernetes3 web-0 1/1 Running 2 5h21m 192.168.249.139 kubernetes2 web-1 1/1 Running 2 5h21m 192.168.249.140 kubernetes2 web-2 1/1 Running 2 5h22m 192.168.249.138 kubernetes2 {% endhighlight %}\nNow we can see the pod is created in the node \u0026ldquo;kubernetes3\u0026rdquo; conifgued in nodeName option\nUsing labels in nodeSelector Step 1: Add a new label to the node Check the current lables using the below command\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl get nodes \u0026ndash;show-labels NAME STATUS ROLES AGE VERSION LABELS kubernetes1 Ready master 20d v1.16.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes1,kubernetes.io/os=linux,node-role.kubernetes.io/master= kubernetes2 Ready 20d v1.16.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes2,kubernetes.io/os=linux kubernetes3 Ready 117m v1.16.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes3,kubernetes.io/os=linux,namee=node3\n{% endhighlight %}\nAdd a new lable \u0026ldquo;disktype=vhd\u0026rdquo; to the kubernetes3 node\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl label nodes kubernetes3 disktype=vhd node/kubernetes3 labeled\nvikki@kubernetes1:~$ kubectl get nodes \u0026ndash;show-labels NAME STATUS ROLES AGE VERSION LABELS kubernetes1 Ready master 20d v1.16.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes1,kubernetes.io/os=linux,node-role.kubernetes.io/master= kubernetes2 Ready 20d v1.16.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes2,kubernetes.io/os=linux kubernetes3 Ready 118m v1.16.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disktype=vhd,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes3,kubernetes.io/os=linux,namee=node3\n{% endhighlight %}\nStep 2: Create a pod and assign using nodeSelector {% highlight console %}\nvikki@kubernetes1:~$ vim pod_label.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl create -f pod_label.yaml pod/nginx-pod-label created\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES busybox 1/1 Running 3 4h50m 192.168.249.141 kubernetes2 nginx-pod-label 1/1 Running 0 4m14s 192.168.80.195 kubernetes3 web-0 1/1 Running 2 4h25m 192.168.249.139 kubernetes2 web-1 1/1 Running 2 4h25m 192.168.249.140 kubernetes2 web-2 1/1 Running 2 4h25m 192.168.249.138 kubernetes2 vikki@kubernetes1:$\n{% endhighlight %}\nNow we can see the new pod is assinged to kubernetes3 based on nodeSelector label option\nAdvance pod scheduling We can also assing the pod to a specific node using the Node/pod affinity and anti affinity rules.\nAffinity types:\nrequiredDuringSchedulingRequiredDuringExecution requiredDuringSchedulingIgnoredDuringExecution preferredDuringSchedulingIgnoredDuringExecution Affinity operators:\nIn NotIn Exists DoesNotExist Gt Lt Node Affinity/Anti Affinity Step 1: Create a pod with node affinity Create a pod with node affinity specs and match the lable using matchExpressions spec\n{% highlight console %}\nvikki@kubernetes1:~$ vim pod_node_affinity.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl create -f pod_node_affinity.yaml pod/nginx-pod-nodeaffinity created\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE busybox 1/1 Running 3 5h17m nginx-pod-label 1/1 Running 0 31m nginx-pod-nodeaffinity 0/1 Pending 0 4s web-0 1/1 Running 2 4h52m web-1 1/1 Running 2 4h52m web-2 1/1 Running 2 4h52m\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl describe pods nginx-pod-nodeaffinity |grep Events: -A 3 Events: Type Reason Age From Message \u0026mdash;- \u0026mdash;\u0026mdash; \u0026mdash;- \u0026mdash;- \u0026mdash;\u0026mdash;- Warning FailedScheduling default-scheduler 0/3 nodes are available: 3 node(s) didn\u0026rsquo;t match node selector.\n{% endhighlight %}\nNow the pod is failing because there is no node has the match lables\nLets add a label bandwidth to the kubernetes3 node\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl label nodes kubernetes3 bandwidth=100GB node/kubernetes3 labeled\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl describe pods nginx-pod-nodeaffinity |grep Events: -A 5 Events: Type Reason Age From Message \u0026mdash;- \u0026mdash;\u0026mdash; \u0026mdash;- \u0026mdash;- \u0026mdash;\u0026mdash;- Warning FailedScheduling default-scheduler 0/3 nodes are available: 3 node(s) didn\u0026rsquo;t match node selector. Warning FailedScheduling default-scheduler 0/3 nodes are available: 3 node(s) didn\u0026rsquo;t match node selector. Normal Scheduled default-scheduler Successfully assigned default/nginx-pod-nodeaffinity to kubernetes3\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES busybox 1/1 Running 3 5h23m 192.168.249.141 kubernetes2 nginx-pod-label 1/1 Running 0 37m 192.168.80.195 kubernetes3 nginx-pod-nodeaffinity 1/1 Running 0 6m16s 192.168.80.196 kubernetes3 web-0 1/1 Running 2 4h58m 192.168.249.139 kubernetes2 web-1 1/1 Running 2 4h58m 192.168.249.140 kubernetes2 web-2 1/1 Running 2 4h59m 192.168.249.138 kubernetes2 {% endhighlight %}\nNow we can see the node pod is successully assinged to the kubernetes3 node after adding the label\nPod Affinity/Anti Affinity We can also assing the pod to a specific node using the pod affinity and anti affinity rules.\nStep 1: Create a pod with pod affinity Create a pod with pod affinity specs and match the lable using matchExpressions spec\n{% highlight console %}\nvikki@kubernetes1:~$ vim pod_pod_affinity.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl create -f pod_pod_affinity.yaml pod/nginx-pod-podaffinity created\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE busybox 1/1 Running 4 5h36m nginx-pod-label 1/1 Running 0 49m nginx-pod-nodeaffinity 1/1 Running 0 18m nginx-pod-podaffinity 0/1 Pending 0 4s web-0 1/1 Running 2 5h10m web-1 1/1 Running 2 5h10m web-2 1/1 Running 2 5h11m\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl describe pods nginx-pod-podaffinity |grep Events: -A 5 Events: Type Reason Age From Message \u0026mdash;- \u0026mdash;\u0026mdash; \u0026mdash;- \u0026mdash;- \u0026mdash;\u0026mdash;- Warning FailedScheduling default-scheduler 0/3 nodes are available: 1 node(s) had taints that the pod didn\u0026rsquo;t tolerate, 2 node(s) didn\u0026rsquo;t match pod affinity rules, 2 node(s) didn\u0026rsquo;t match pod affinity/anti-affinity. Warning FailedScheduling default-scheduler 0/3 nodes are available: 1 node(s) had taints that the pod didn\u0026rsquo;t tolerate, 2 node(s) didn\u0026rsquo;t match pod affinity rules, 2 node(s) didn\u0026rsquo;t match pod affinity/anti-affinity.\n{% endhighlight %}\nNow the pod is failing because there is no pod running in any nodes that has the match lables\nLets create a new pod with the label configued previously\n{% highlight console %}\nvikki@kubernetes1:~$ vim pod_label_podaffinity.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl apply -f pod_label_podaffinity.yaml pod/nginx-pod-web-nginx-backend created\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl describe pods nginx-pod-podaffinity |grep Events: -A 5 Events: Type Reason Age From Message \u0026mdash;- \u0026mdash;\u0026mdash; \u0026mdash;- \u0026mdash;- \u0026mdash;\u0026mdash;- Warning FailedScheduling default-scheduler 0/3 nodes are available: 1 node(s) had taints that the pod didn\u0026rsquo;t tolerate, 2 node(s) didn\u0026rsquo;t match pod affinity rules, 2 node(s) didn\u0026rsquo;t match pod affinity/anti-affinity. Warning FailedScheduling default-scheduler 0/3 nodes are available: 1 node(s) had taints that the pod didn\u0026rsquo;t tolerate, 2 node(s) didn\u0026rsquo;t match pod affinity rules, 2 node(s) didn\u0026rsquo;t match pod affinity/anti-affinity. Normal Scheduled default-scheduler Successfully assigned default/nginx-pod-podaffinity to kubernetes3\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES busybox 1/1 Running 4 5h39m 192.168.249.141 kubernetes2 nginx-pod-label 1/1 Running 0 52m 192.168.80.195 kubernetes3 nginx-pod-nodeaffinity 1/1 Running 0 21m 192.168.80.196 kubernetes3 nginx-pod-podaffinity 1/1 Running 0 3m2s 192.168.80.197 kubernetes3 nginx-pod-web-nginx-backend 1/1 Running 0 14s 192.168.80.198 kubernetes3 web-0 1/1 Running 2 5h13m 192.168.249.139 kubernetes2 web-1 1/1 Running 2 5h13m 192.168.249.140 kubernetes2 web-2 1/1 Running 2 5h14m 192.168.249.138 kubernetes2 {% endhighlight %}\nNow we can see only the pod with lable app: web-nginx-backend is created, the previous pod is also created in the same node\nTaints and tolerations Node affinity is a property of pods that attracts them to a set of nodes. Taints are the opposite, they allow a node to repel a set of pods.\nStep 1: Create a label to node and assign pod using nodeSelector {% highlight console %}\nvikki@kubernetes1:$ kubectl label nodes kubernetes4 app=highperformance node/kubernetes4 labeled vikki@kubernetes1:$ kubectl get nodes kubernetes4 \u0026ndash;show-labels NAME STATUS ROLES AGE VERSION LABELS kubernetes4 Ready 2m55s v1.16.3 app=highperformance,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes4,kubernetes.io/os=linux\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ vim pod_label_1.yaml {% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl create -f pod_label_1.yaml pod/nginx-pod-taint created\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pod nginx-pod-taint -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-pod-taint 1/1 Running 0 10m 192.168.48.129 kubernetes4 {% endhighlight %}\nWe can see the new pod nginx-pod-taint is created and assigned to kubernetes4 node\nStep 2: Create a taint Now lets create a taint \u0026ldquo;NoSchedule\u0026rdquo; and add to the kubernetes4 node.\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl taint nodes kubernetes4 key1=value1:NoSchedule node/kubernetes4 tainted\nvikki@kubernetes1:~$ kubectl get nodes kubernetes4 -o yaml |grep -i taint -A 3 taints: - effect: NoSchedule key: key1 value: value1\n{% endhighlight %}\nStep 3: Create a new pod and try assign to kubernetes4 Now create a new pod and use nodeSelector to assign to kubernetes4\n{% highlight console %}\nvikki@kubernetes1:~$ vim pod_label_2.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl create -f pod_label_2.yaml pod/nginx-pod-taint-2 created\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pods nginx-pod-taint-2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-pod-taint-2 0/1 Pending 0 7s {% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl describe pod nginx-pod-taint-2 |grep -i events: -A 5 Events: Type Reason Age From Message \u0026mdash;- \u0026mdash;\u0026mdash; \u0026mdash;- \u0026mdash;- \u0026mdash;\u0026mdash;- Warning FailedScheduling default-scheduler 0/4 nodes are available: 1 node(s) had taints that the pod didn\u0026rsquo;t tolerate, 3 node(s) didn\u0026rsquo;t match node selector. Warning FailedScheduling default-scheduler 0/4 nodes are available: 1 node(s) had taints that the pod didn\u0026rsquo;t tolerate, 3 node(s) didn\u0026rsquo;t match node selector.\n{% endhighlight %}\nWe can see the pod creatation if failing due to the taints setting.\nStep 4: Update the pod with tolerations Now lets add toleration to the same pod for \u0026ldquo;NoSchedule\u0026rdquo; and apply the changes.\n{% highlight console %}\nvikki@kubernetes1:~$ vim pod_label_3.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl apply -f pod_label_3.yaml Warning: kubectl apply should be used on resource created by either kubectl create \u0026ndash;save-config or kubectl apply pod/nginx-pod-taint-2 configured\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl describe pod nginx-pod-taint-2 |grep -i events: -A 5 Events: Type Reason Age From Message \u0026mdash;- \u0026mdash;\u0026mdash; \u0026mdash;- \u0026mdash;- \u0026mdash;\u0026mdash;- Warning FailedScheduling default-scheduler 0/4 nodes are available: 1 node(s) had taints that the pod didn\u0026rsquo;t tolerate, 3 node(s) didn\u0026rsquo;t match node selector. Warning FailedScheduling default-scheduler 0/4 nodes are available: 1 node(s) had taints that the pod didn\u0026rsquo;t tolerate, 3 node(s) didn\u0026rsquo;t match node selector. Normal Scheduled default-scheduler Successfully assigned default/nginx-pod-taint-2 to kubernetes4\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pod nginx-pod-taint-2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-pod-taint-2 1/1 Running 0 11m 192.168.48.130 kubernetes4 {% endhighlight %}\nNow we can see the pod changed from failed to success state and assinged to kubernetes4 node according to the nodeSelector.\n","permalink":"http://localhost:1313/posts/2019-11-24-pod-scheduling-in-kubernetes-detailed-step-by-step/","summary":"\u003cp\u003eWe can assign the pod to node based on various methods. Lets discuss all the below methods in the post\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUsing nodeName\u003c/li\u003e\n\u003cli\u003eUsing labels in nodeSelector\u003c/li\u003e\n\u003cli\u003eNode Affinity/Anti Affinity\u003c/li\u003e\n\u003cli\u003ePod Affinity/Anti Affinity\u003c/li\u003e\n\u003cli\u003eTaints and tolerations\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"setup\"\u003e\u003cstrong\u003eSetup\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eI am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\u003c/p\u003e","title":"Pod scheduling in kubernetes - detailed step by step"},{"content":"StatefulSets are similar to deployment contains identical container spec but ensures an order of the deployment. StatefulSets deploy pods in a sequential orders. Each pod as its own identity and is named with a unique ID. In the below post we are going to create a statefulsets and watch the behaviour during deletion of pod, scaling of pod and during update of container image.\nThe StatefulSets consist of a headless service, pods and a volume. We are going to use a local-storage volume for statefulsets. It is also common to dynamically allocate storage using volumeClaimTemplates, but due to some limitation in virtualbox i am using a manual allocation using PersistVolumeClaim.\nSetup I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\nStep 1: Create a persistentVolume and persistemVolumeClaim {% highlight console %}\nvim pv.yaml\n{% endhighlight %}\n{% highlight console %} vim pv_claim.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl create -f pv.yaml persistentvolume/vikki-pv-volume created\nvikki@kubernetes1:~$ kubectl create -f pv_claim.yaml persistentvolumeclaim/vikki-pv-volume-claim created\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE vikki-pv-volume-claim Bound vikki-pv-volume 1Gi RWO local-storage 4s\nvikki@kubernetes1:~$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE vikki-pv-volume 1Gi RWO Retain Bound default/vikki-pv-volume-claim local-storage 9s\n{% endhighlight %}\nStep 2: Create a statefulSets Create a statefulSets and map the PVC created in previous steps. The statefull set consist of the following. -\nHeadless service StatefulSets with container details Persistent Volumes {% highlight console %} vim ss.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl create -f ss.yaml service/nginx created statefulset.apps/web created\n{% endhighlight %}\nStep 3: Verify statefulSets Now we can see the statefulsets are created and the respecive pod is created with a sequential unique id.\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl get statefulsets.apps NAME READY AGE web 2/2 6m40s\nvikki@kubernetes1:~$ kubectl get pod -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 15m web-1 1/1 Running 0 15m\n{% endhighlight %}\nStep 4: Verify the DNS for statefulSets Create a temporary busybox pod and test the dns resolution for each statefulSet pods based on the pod and stateful set name.\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl apply -f https://k8s.io/examples/admin/dns/busybox.yaml pod/busybox created\n{% endhighlight %}{% highlight console %}\nroot@kubernetes2:~# docker ps -a |grep busybox 8e3ba8c16f3e busybox \u0026ldquo;sleep 3600\u0026rdquo; 21 seconds ago Up 20 seconds k8s_busybox_busybox_default_454e3e1c-1b41-4f6e-a244-6d89cd8c8379_0 9c890ad85c23 k8s.gcr.io/pause:3.1 \u0026ldquo;/pause\u0026rdquo; 28 seconds ago Up 27 seconds k8s_POD_busybox_default_454e3e1c-1b41-4f6e-a244-6d89cd8c8379_0\nroot@kubernetes2:~# docker exec -it k8s_busybox_busybox_default_454e3e1c-1b41-4f6e-a244-6d89cd8c8379_0 /bin/sh / # / # ping web-0 ping: bad address \u0026lsquo;web-0\u0026rsquo; / # ping web-0.nginx PING web-0.nginx (192.168.249.183): 56 data bytes 64 bytes from 192.168.249.183: seq=0 ttl=63 time=0.551 ms 64 bytes from 192.168.249.183: seq=1 ttl=63 time=0.071 ms ^C \u0026mdash; web-0.nginx ping statistics \u0026mdash; 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.071/0.311/0.551 ms\n/ # ping web-1.nginx PING web-1.nginx (192.168.249.184): 56 data bytes 64 bytes from 192.168.249.184: seq=0 ttl=63 time=0.227 ms 64 bytes from 192.168.249.184: seq=1 ttl=63 time=0.256 ms ^C \u0026mdash; web-1.nginx ping statistics \u0026mdash; 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.227/0.241/0.256 ms / #\n{% endhighlight %}\nNow we can see both the statefulSets pods resolves for dns.\nStep 5: Verify the order of pod creation on deletion We can try deleting some pods and see how the new pods are creating\n{% highlight console %}\nvikki@kubernetes1:$ kubectl delete pod -l app=nginx pod \u0026ldquo;web-0\u0026rdquo; deleted pod \u0026ldquo;web-1\u0026rdquo; deleted vikki@kubernetes1:$\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:$ kubectl get pod -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 18m web-1 1/1 Running 0 18m web-0 1/1 Terminating 0 19m web-1 1/1 Terminating 0 19m web-0 0/1 Terminating 0 19m web-1 0/1 Terminating 0 19m web-1 0/1 Terminating 0 19m web-1 0/1 Terminating 0 19m web-0 0/1 Terminating 0 19m web-0 0/1 Terminating 0 19m web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 0/1 ContainerCreating 0 1s web-0 1/1 Running 0 2s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 0/1 ContainerCreating 0 1s web-1 1/1 Running 0 1s ^Cvikki@kubernetes1:$\n{% endhighlight %}\nWe can see the pod are creating in ascending order preserving the sequence status\nStep 6: Verify the order of pod creation on scalling Now lets scale up and scale down the statefulset and watch the order of pod creation and deletion\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl scale statefulset \u0026ndash;replicas=5 web statefulset.apps/web scaled\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pod -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 5m5s web-1 1/1 Running 0 5m3s web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 0/1 ContainerCreating 0 1s web-2 1/1 Running 0 1s web-3 0/1 Pending 0 0s web-3 0/1 Pending 0 0s web-3 0/1 ContainerCreating 0 0s web-3 0/1 ContainerCreating 0 1s web-3 1/1 Running 0 1s web-4 0/1 Pending 0 0s web-4 0/1 Pending 0 0s web-4 0/1 ContainerCreating 0 1s web-4 0/1 ContainerCreating 0 2s web-4 1/1 Running 0 3s\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl scale statefulset \u0026ndash;replicas=3 web statefulset.apps/web scaled\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pod -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 5m44s web-1 1/1 Running 0 5m42s web-2 1/1 Running 0 19s web-3 1/1 Running 0 18s web-4 1/1 Running 0 17s web-4 1/1 Terminating 0 40s web-4 0/1 Terminating 0 42s web-4 0/1 Terminating 0 47s web-4 0/1 Terminating 0 47s web-3 1/1 Terminating 0 48s web-3 0/1 Terminating 0 49s web-3 0/1 Terminating 0 58s web-3 0/1 Terminating 0 58s\n{% endhighlight %}\nWe can see from the above output the sequential order is preserved\nStep 6: Verify the order of pod creation on update Now lets update the image in the statefulset and watch for the pod creation and deletion\n{% highlight console %}\nvikki@kubernetes1:$ kubectl get statefulsets.apps web -o yaml |grep updateStrategy -A 3 updateStrategy: rollingUpdate: partition: 0 type: RollingUpdate vikki@kubernetes1:$\n{% endhighlight %}\nCurrent statefulsets has the updatestrategy of type \u0026ldquo;RollingUpdate\u0026rdquo;\nLets modify the container image to a different version\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl set image statefulset web nginx=k8s.gcr.io/nginx-slim:0.9 statefulset.apps/web image updated\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pod -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 9m57s web-1 1/1 Running 0 9m55s web-2 1/1 Running 0 4m32s web-2 1/1 Terminating 0 5m29s web-2 0/1 Terminating 0 5m30s web-2 0/1 Terminating 0 5m39s web-2 0/1 Terminating 0 5m39s web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 0/1 ContainerCreating 0 1s web-2 1/1 Running 0 32s web-1 1/1 Terminating 0 11m web-1 0/1 Terminating 0 11m web-1 0/1 Terminating 0 11m web-1 0/1 Terminating 0 11m web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 0/1 ContainerCreating 0 1s web-1 1/1 Running 0 2s web-0 1/1 Terminating 0 11m web-0 0/1 Terminating 0 11m web-0 0/1 Terminating 0 11m web-0 0/1 Terminating 0 11m web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 0/1 ContainerCreating 0 1s web-0 1/1 Running 0 2s\n{% endhighlight %}\nWe can see from the above output the sequential order is preserved\n","permalink":"http://localhost:1313/posts/2019-11-24-kubernetes-stateful-set-with-local-storage-persistent-volume/","summary":"\u003cp\u003eStatefulSets are similar to deployment contains identical container spec but ensures an order of the deployment. StatefulSets deploy pods in a sequential orders. Each pod as its own identity and is named with a unique ID. In the below post we are going to create a statefulsets and watch the behaviour during deletion of pod, scaling of pod and during update of container image.\u003c/p\u003e\n\u003cp\u003eThe StatefulSets consist of a headless service, pods and a volume. We are going to use a local-storage volume for statefulsets. It is also common to dynamically allocate storage using ¬†volumeClaimTemplates, but due to some limitation in virtualbox i am using a manual allocation using PersistVolumeClaim.\u003c/p\u003e","title":"Kubernetes stateful set with local-storage persistent volume"},{"content":"PersistentVolume and PersistentVolumeClaim in kubernetes provides a way to allocate storage for the pods. Kubernetes PV supports different types of storage. Now here in the below post we are going to use storage-class local-storage and watch the behaviour for different reclaimpolicy.\nSetup I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\nStep 1: Create a directory in one for the node and add a file {% highlight console %}\nroot@kubernetes2:# mkdir /pvdir root@kubernetes2:# echo \u0026ldquo;Hello world!\u0026rdquo; \u0026gt; /pvdir/index.html root@kubernetes2:~# cat /pvdir/index.html Hello world!\n{% endhighlight %}\nStep 2: Get the label of the node We need to get the label of the node where the directory is created.This we need to configure in the nodeAffinity specs in order to properly stick the pod to respective node where the local volume is present.\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl get nodes \u0026ndash;show-labels NAME STATUS ROLES AGE VERSION LABELS kubernetes1 Ready master 19d v1.16.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes1,kubernetes.io/os=linux,node-role.kubernetes.io/master= kubernetes2 Ready 19d v1.16.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=kubernetes2,kubernetes.io/os=linux\n{% endhighlight %}\nStep 3: Create a persistent volume(PV) Create a PV and map the local storage director and configure the label in the node selector\n{% highlight console %}\nvikki@kubernetes1:~$ vim pv.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl create -f pv.yaml persistentvolume/vikki-pv-volume created\n{% endhighlight %}\nStep 4: Verify the PV status Verify the PV status. It is currently configured with access type RWO and reclaim policy as Delete\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE vikki-pv-volume 1Gi RWO Delete Available local-storage 14s\n{% endhighlight %}\nStep 5: Create a persistent volume claim(PVC) Create a PVC and map the above created PV\n{% highlight console %}\nvikki@kubernetes1:~$ vim pv_claim.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl create -f pv_claim.yaml persistentvolumeclaim/vikki-pv-volume-claim created\n{% endhighlight %}\nStep 6: Verify the PVC status Verify the PVC and PV status. Now we can see the status changes from Available to Bound.\n{% highlight console %}\nvikki@kubernetes1:$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE vikki-pv-volume-claim Bound vikki-pv-volume 1Gi RWO local-storage 8s vikki@kubernetes1:$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE vikki-pv-volume 1Gi RWO Delete Bound default/vikki-pv-volume-claim local-storage 3m59s\n{% endhighlight %}\nStep 7: Create a pod and map the PVC Create a pod and map the volumes as below\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl run \u0026ndash;image=nginx nginx-pod \u0026ndash;generator=run-pod/v1 \u0026ndash;dry-run -o yaml \u0026gt; pod.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ vim pod.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl create -f pod.yaml pod/nginx-pod created\n{% endhighlight %}\nStep 8: Verify the pod status and volume inside Verify the status of the new pod and check the volume is mounted inside the container\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE fluentd-elasticsearch-bwhfx 0/1 ImagePullBackOff 0 4h3m fluentd-elasticsearch-pvlpv 0/1 ImagePullBackOff 0 4h3m nginx-85ff79dd56-zqllr 1/1 Running 1 5h51m nginx-pod 1/1 Running 0 10s\n{% endhighlight %}{% highlight console %}\nroot@kubernetes2:~# docker ps -a |grep nginx-pod e142048a048f nginx \u0026ldquo;nginx -g \u0026lsquo;daemon of‚Ä¶\u0026rdquo; 20 seconds ago Up 20 seconds k8s_nginx-pod_nginx-pod_default_b6b900bb-b214-4b96-bc90-18b2358ee225_0 df59f401a38a k8s.gcr.io/pause:3.1 \u0026ldquo;/pause\u0026rdquo; 26 seconds ago Up 25 seconds k8s_POD_nginx-pod_default_b6b900bb-b214-4b96-bc90-18b2358ee225_0\n{% endhighlight %}{% highlight console %}\nroot@kubernetes2:~# docker exec -it k8s_nginx-pod_nginx-pod_default_b6b900bb-b214-4b96-bc90-18b2358ee225_0 cat /usr/share/nginx/html/index.html Hello world!\n{% endhighlight %}\nNow we can see the directory from the PV is mounted in the pod and the files created are present\nVerify reclaim policy(Below steps are Optional) Step 9: Delete PVC and verify PV Now delete the pod and PVC and verify the status of PV\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl delete pod nginx-pod pod \u0026ldquo;nginx-pod\u0026rdquo; deleted\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl delete pvc vikki-pv-volume-claim persistentvolumeclaim \u0026ldquo;vikki-pv-volume-claim\u0026rdquo; deleted\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE vikki-pv-volume 1Gi RWO Delete Failed default/vikki-pv-volume-claim local-storage 5m30s\n{% endhighlight %}\nWe can see the staus of PV is automaticall changed to Failed. This is due the the reclaim policy Delete\nStep 10: (Optional ) Try recreate PVC We can optionally try recreating the PVC volume and verify the status\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl create -f pv_claim.yaml persistentvolumeclaim/vikki-pv-volume-claim created\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE vikki-pv-volume 1Gi RWO Delete Failed default/vikki-pv-volume-claim local-storage 5m57s vikki@kubernetes1:$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE vikki-pv-volume-claim Pending local-storage 2m10s\n{% endhighlight %}\nWe can see the status of PV is still Failed and the PVC always in Pending and never creates\nStep 11: Delete and create PV with reclaim policy as retain Now lets clean up all PV and PVC and recreate PV with reclaim policy set as \u0026ldquo;Retain\u0026rdquo;\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl delete pvc vikki-pv-volume-claim persistentvolumeclaim \u0026ldquo;vikki-pv-volume-claim\u0026rdquo; deleted\nvikki@kubernetes1:~$ kubectl delete pv vikki-pv-volume persistentvolume \u0026ldquo;vikki-pv-volume\u0026rdquo; deleted\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ vim pv.yaml persistentVolumeReclaimPolicy: Retain\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:$ kubectl create -f pv.yaml persistentvolume/vikki-pv-volume created vikki@kubernetes1:$ kubectl create -f pv_claim.yaml persistentvolumeclaim/vikki-pv-volume-claim created\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pvc,pv NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/vikki-pv-volume-claim Bound vikki-pv-volume 1Gi RWO local-storage 99s\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/vikki-pv-volume 1Gi RWO Retain Bound default/vikki-pv-volume-claim local-storage 118s\n{% endhighlight %}\nStep 12: Delete the PVC and verify the status of PV Now lets delete the PVC and verify the status of the PV\n{% highlight console %}\nvikki@kubernetes1:~$ kubectl delete pvc vikki-pv-volume-claim persistentvolumeclaim \u0026ldquo;vikki-pv-volume-claim\u0026rdquo; deleted\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE vikki-pv-volume 1Gi RWO Retain Released default/vikki-pv-volume-claim local-storage 2m11s\n{% endhighlight %}\nNow we can see the status of PV changes to \u0026ldquo;Released\u0026rdquo; instead of \u0026ldquo;Failed\u0026rdquo; due to the reclaim policy \u0026ldquo;Retain\u0026rdquo;\n","permalink":"http://localhost:1313/posts/2019-11-23-creating-persistent-local-storage-volume-in-kubernetes/","summary":"\u003cp\u003ePersistentVolume and PersistentVolumeClaim in kubernetes provides a way to allocate storage for the pods. Kubernetes PV supports different types of storage. Now here in the below post we are going to use storage-class \u003cstrong\u003elocal-storage\u003c/strong\u003e and watch the behaviour for different reclaimpolicy.\u003c/p\u003e\n\u003ch3 id=\"setup\"\u003e\u003cstrong\u003eSetup\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eI am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\u003c/p\u003e","title":"Creating persistent local-storage volume in Kubernetes"},{"content":"Daemonset ensures that all the nodes run a copy of a pod. It can be used for running storage/monitoring daemons like glusterd,Prometheus etc. Now in this post we are going to see how to create a daemonset and do an image update. We are also going to perform different update strategy and watch the behaviour of damonset updates.\nSetup I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\nStep 1: Create a daemon set {% highlight console %}\nvikki@kubernetes1:~$ kubectl create -f ds.yaml daemonset.apps/fluentd-elasticsearch created\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get ds NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE fluentd-elasticsearch 2 2 2 2 2 5s\n{% endhighlight %}\nStep 2: Verify the image version and updateStrategy of the daemonset {% highlight console %}\nvikki@kubernetes1:~$ kubectl describe ds fluentd-elasticsearch |grep Image Image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\nvikki@kubernetes1:~$ kubectl get ds fluentd-elasticsearch -o yaml |grep -A 3 -i updateStrategy updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate\nvikki@kubernetes1:$ kubectl get pod fluentd-elasticsearch- fluentd-elasticsearch-7ds49 fluentd-elasticsearch-qh8n8\nvikki@kubernetes1:$ kubectl describe pod fluentd-elasticsearch-7ds49 |grep Image: Image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\n{% endhighlight %}\nThe daemonset has a image version of \u0026quot; 2.5.2\u0026quot; and updateStrategy \u0026quot; RollingUpdate\u0026quot;\nStep 3: Update the daemon set container image to a different version say \u0026ldquo;2.5.2\u0026rdquo; {% highlight console %}\nvikki@kubernetes1:~$ kubectl set image ds fluentd-elasticsearch fluentd-elasticsearch=quay.io/fluentd_elasticsearch/fluentd:v2.5.1 daemonset.apps/fluentd-elasticsearch image updated\n{% endhighlight %}\nstep 4 : Verify the image version is updated in daemonset level also in pod level {% highlight console %}\nvikki@kubernetes1:~$ kubectl describe ds fluentd-elasticsearch |grep Image Image: quay.io/fluentd_elasticsearch/fluentd:v2.5.1\nvikki@kubernetes1:$ kubectl describe pod fluentd-elasticsearch- fluentd-elasticsearch-qh8n8 fluentd-elasticsearch-x2f57\nvikki@kubernetes1:$ kubectl describe pod fluentd-elasticsearch-qh8n8 |grep Image: Image: quay.io/fluentd_elasticsearch/fluentd:v2.5.1\n{% endhighlight %}\nNow we can see , as soon as the daemonset image is updated , the pod image also gets updated. This is because of the updateStrategy set as RollingUpdate\nStep 5: Now lets change the updateStrategy to OnDelete and watch the behaviour {% highlight console %}\nvikki@kubernetes1:$ kubectl get ds fluentd-elasticsearch -o yaml \u0026gt; ds_new.yaml vikki@kubernetes1:$ vim ds_new.yaml\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl apply -f ds_new.yaml Warning: kubectl apply should be used on resource created by either kubectl create \u0026ndash;save-config or kubectl apply daemonset.apps/fluentd-elasticsearch configured\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl describe ds fluentd-elasticsearch |grep Image Image: quay.io/fluentd_elasticsearch/fluentd:v2.5.1\nvikki@kubernetes1:~$ kubectl get ds fluentd-elasticsearch -o yaml |grep -A 3 -i updateStrategy updateStrategy: rollingUpdate: maxUnavailable: 1 type: OnDelete vikki@kubernetes1:~$ kubectl describe pod fluentd-elasticsearch-qh8n8 |grep Image: Image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\n{% endhighlight %}\nThe udpate strategy is changed to OnDelete and the version in 2.5.2\nStep 6: Lets change the image to different version {% highlight console %}\nvikki@kubernetes1:~$ kubectl set image ds fluentd-elasticsearch fluentd-elasticsearch=quay.io/fluentd_elasticsearch/fluentd:v2.5.0 daemonset.apps/fluentd-elasticsearch image updated\n{% endhighlight %}\nStep 7: Verify the image version in daemonset and pod {% highlight console %}\nvikki@kubernetes1:~$ kubectl describe ds fluentd-elasticsearch |grep Image Image: quay.io/fluentd_elasticsearch/fluentd:v2.5.0\nvikki@kubernetes1:~$ kubectl describe pod fluentd-elasticsearch-qh8n8 |grep Image: Image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\n{% endhighlight %}\nNow we can see the image version on Daemonset is updated , but the pod is still running in the older version\nStep 8: Lets delete the pod and wait for the new pods to be created and verify the image version in new pod {% highlight console %}\nvikki@kubernetes1:~$ kubectl delete pod fluentd-elasticsearch- fluentd-elasticsearch-qh8n8 fluentd-elasticsearch-x2f57\nvikki@kubernetes1:~$ kubectl delete pod fluentd-elasticsearch-qh8n8 fluentd-elasticsearch-x2f57 pod \u0026ldquo;fluentd-elasticsearch-qh8n8\u0026rdquo; deleted pod \u0026ldquo;fluentd-elasticsearch-x2f57\u0026rdquo; deleted\n{% endhighlight %}{% highlight console %}\nvikki@kubernetes1:~$ kubectl get pod fluentd-elasticsearch- fluentd-elasticsearch-89g44 fluentd-elasticsearch-mf5f9\nvikki@kubernetes1:~$ kubectl get pod fluentd-elasticsearch- fluentd-elasticsearch-89g44 fluentd-elasticsearch-mf5f9\nvikki@kubernetes1:~$ kubectl describe pod fluentd-elasticsearch-89g44 |grep Image: Image: quay.io/fluentd_elasticsearch/fluentd:v2.5.0\n{% endhighlight %}\nNow we can see the version is automatically updated on the new pod only after delete.\n","permalink":"http://localhost:1313/posts/2019-11-23-rolling-updates-and-rollbacks-in-kubernetes/","summary":"\u003cp\u003eDaemonset ensures that all the nodes run a copy of a pod. It can be used for running storage/monitoring daemons like glusterd,Prometheus etc. Now in this post we are going to see how to create a daemonset and do an image update. We are also going to perform different update strategy and watch the behaviour of damonset updates.\u003c/p\u003e\n\u003ch3 id=\"setup\"\u003e\u003cstrong\u003eSetup\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eI am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\u003c/p\u003e","title":"Rolling updates and update strategy in Kubernetes daemonsets"},{"content":"Kubernetes provides rollout options to do update on deployment and easily fallback to any revision. We are going to see how to update the deployment to a newer version of container image and rollback to previous version without affecting the services\nSetup I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\nStep 1: Create a deployment {% highlight console %}\nvikki@kubernetes1:~$ kubectl create deployment nginx \u0026ndash;image=nginx deployment.apps/nginx created\nvikki@kubernetes1:~$ kubectl get deployments -o wide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR nginx 1/1 1 1 25s nginx nginx app=nginx\n{% endhighlight %}\nStep 2: Export the deployment to yaml file and add the port option(for nginx image the port is 80) {% highlight console %}\nvikki@kubernetes1:~$ kubectl get deployments -o yaml \u0026gt; nginx.yaml\nvikki@kubernetes1:~$ vim nginx.yaml ports: - containerPort: 80 protocol: TCP\n{% endhighlight %}\nStep 3: Apply the changes to the deployment {% highlight console %}\nvikki@kubernetes1:~$ kubectl apply -f nginx.yaml Warning: kubectl apply should be used on resource created by either kubectl create \u0026ndash;save-config or kubectl applydeployment.apps/nginx configured\n{% endhighlight %}\nStep 4: Expose the deployment as ClusterIP {% highlight console %}\nvikki@kubernetes1:~$ kubectl expose deployment nginx \u0026ndash;type=ClusterIP service/nginx exposed\nvikki@kubernetes1:~$ kubectl get service nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP 10.102.68.171 80/TCP 8s\n{% endhighlight %}\nStep 5: Verify the service by accessing nginx deployment using the ClusterIP {% highlight console %}\nvikki@kubernetes1:~$ curl 10.102.68.171 Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org.Commercial support is available at nginx.com. Thank you for using nginx.\n{% endhighlight %}\nStep 6: Verify the current version of Nginx Go to the kubernetes node kubernetes2 and verify the nginx version\n{% highlight console %}\nroot@kubernetes2:~# docker exec -it k8s_nginx_nginx-85ff79dd56-mch7j_default_4608325f-2432-4ff3-86ab-e1f2b01dd8f2_0 nginx -v nginx version: nginx/1.17.6\n{% endhighlight %}\nNow we have made 2 changes to the nginx deployment\nOrignal Nginx deployment Nginx deployment with container port set as 80 Step 7: set Nginx with a different image version {% highlight console %}\nvikki@kubernetes1:~$ kubectl set image deployment nginx nginx=nginx:1.9.1 deployment.apps/nginx image updated\n{% endhighlight %}\nVerify the nginx version in node kubernetes2\n{% highlight console %}\nroot@kubernetes2:# docker ps -a |grep nginx e547e6fc1ad1 nginx \u0026ldquo;nginx -g \u0026lsquo;daemon of‚Ä¶\u0026rdquo; 37 seconds ago Up 36 seconds k8s_nginx_nginx-69fbc8b64f-2k6wm_default_4695011d-de3b-497d-81b3-2f5de48c3e92_0 1142c1ff00ee k8s.gcr.io/pause:3.1 \u0026ldquo;/pause\u0026rdquo; 41 seconds ago Up 40 seconds k8s_POD_nginx-69fbc8b64f-2k6wm_default_4695011d-de3b-497d-81b3-2f5de48c3e92_0 root@kubernetes2:# docker exec -it k8s_nginx_nginx-69fbc8b64f-2k6wm_default_4695011d-de3b-497d-81b3-2f5de48c3e92_0 nginx -v nginx version: nginx/1.9.1\n{% endhighlight %}\nNow the nginix version is changed from 1.17.6 to 1.9.1\nStep 8: Verify the service by accessing nginx deployment using the ClusterIP {% highlight console %}\nvikki@kubernetes1:~$ curl 10.102.68.171 Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org.Commercial support is available at nginx.com. Thank you for using nginx.\n{% endhighlight %}\nStep 9: Verify the rollout history {% highlight console %}\nvikki@kubernetes1:~$ kubectl rollout history deployment nginx deployment.apps/nginx REVISION CHANGE-CAUSE 1 2 3\n{% endhighlight %}\nNow we have made 3 changes to the nginx deployment\nStep 10: Now rollback to older revision and verify the nginx version changes {% highlight console %}\nvikki@kubernetes1:$ kubectl rollout undo deployment nginx \u0026ndash;to-revision=2 deployment.apps/nginx rolled back vikki@kubernetes1:$ kubectl rollout status deployment nginx deployment \u0026ldquo;nginx\u0026rdquo; successfully rolled out\n{% endhighlight %}{% highlight console %}\nroot@kubernetes2:~# docker ps -a |grep nginx 5825d85fca05 nginx \u0026ldquo;nginx -g \u0026lsquo;daemon of‚Ä¶\u0026rdquo; 11 seconds ago Up 11 seconds k8s_nginx_nginx-85ff79dd56-6vjx6_default_3ae5f3ba-af97-4320-9684-46c2ad8f69d6_0 8467c77a5e10 k8s.gcr.io/pause:3.1 \u0026ldquo;/pause\u0026rdquo; 17 seconds ago Up 16 seconds k8s_POD_nginx-85ff79dd56-6vjx6_default_3ae5f3ba-af97-4320-9684-46c2ad8f69d6_0 1142c1ff00ee k8s.gcr.io/pause:3.1 \u0026ldquo;/pause\u0026rdquo; 3 minutes ago Exited (0) 9 seconds ago k8s_POD_nginx-69fbc8b64f-2k6wm_default_4695011d-de3b-497d-81b3-2f5de48c3e92_0\nroot@kubernetes2:~# docker exec -it k8s_nginx_nginx-85ff79dd56-6vjx6_default_3ae5f3ba-af97-4320-9684-46c2ad8f69d6_0 nginx -v nginx version: nginx/1.17.6\n{% endhighlight %}\nNow the nginix version is changed from 1.9.1 to 1.17.6\nStep 11: Verify the service by accessing nginx deployment using the ClusterIP {% highlight console %}\nvikki@kubernetes1:~$ curl 10.102.68.171 Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org.Commercial support is available at nginx.com. Thank you for using nginx.\n{% endhighlight %}\nWe successfully done the rollout in nginx deployment\n","permalink":"http://localhost:1313/posts/2019-11-23-rolling-updates-and-rollbacks-in-kubernetes-deployment/","summary":"\u003cp\u003eKubernetes provides rollout options to do ¬†update on deployment and easily fallback to any revision. We are going to see how to update the deployment to a newer version of container image and rollback to previous version without affecting the services\u003c/p\u003e\n\u003ch3 id=\"setup\"\u003e\u003cstrong\u003eSetup\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eI am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\u003c/p\u003e","title":"Rolling updates and Rollbacks in Kubernetes deployment"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;‚Äî\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n‚Äî Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested Unordered list Fruit Apple Orange Banana Dairy Milk Cheese Nested Ordered list Fruit Apple Orange Banana Dairy Milk Cheese Third item Sub One Sub Two Other Elements ‚Äî abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/archive/markdown-syntax/","summary":"\u003cp\u003eThis article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\u003c/p\u003e","title":"Markdown Syntax Guide"},{"content":"Inline Code This is Inline Code\nOnly pre This is pre text Code block with backticks \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with backticks and language specified \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with backticks and language specified with line numbers 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with line numbers and highlighted lines PaperMod supports linenos=true or linenos=table 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; With linenos=inline line might not get highlighted properly. This issue is fixed with 045c084 1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3 \u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; 5 \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; 6 \u0026lt;meta 7 name=\u0026#34;description\u0026#34; 8 content=\u0026#34;Sample article showcasing basic Markdown syntax and formatting for HTML elements.\u0026#34; 9 /\u0026gt; 10 \u0026lt;/head\u0026gt; 11 \u0026lt;body\u0026gt; 12 \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; 13 \u0026lt;/body\u0026gt; 14\u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Github Gist ","permalink":"http://localhost:1313/posts/archive/code_syntax/","summary":"Sample article showcasing basic code syntax and formatting for HTML elements.","title":"Code Syntax Guide"},{"content":"Hugo ships with several Built-in Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\nFigure Shortcode (PaperMod enhanced) Photo by Aditya Telange on Unsplash\nYouTube X (Twitter) Shortcode PaperMod is now the most starred @GoHugoIO theme on #GitHub ! ‚ú®\nHere\u0026#39;s what it offers:\n- Simple, minimal \u0026amp; clean design\n- Light/Dark mode\n- Fuzzy search for content\n- Good page-speed insights\nand much more...\nHuge thanks to all supportersüôèhttps://t.co/YAEd2cfrrn\n\u0026mdash; Aditya Telange (@adityatelange) November 14, 2023 Vimeo Shortcode ","permalink":"http://localhost:1313/posts/archive/rich-content/","summary":"\u003cp\u003eHugo ships with several \u003ca href=\"https://gohugo.io/content-management/shortcodes/#use-hugos-built-in-shortcodes\"\u003eBuilt-in Shortcodes\u003c/a\u003e for rich content, along with a \u003ca href=\"https://gohugo.io/about/hugo-and-gdpr/\"\u003ePrivacy Config\u003c/a\u003e and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\u003c/p\u003e","title":"Rich Content and Shortcodes"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates (extend_head.html) like so: refer ISSUE #236 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTex on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: \\(\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887‚Ä¶\\) Block math:\n$$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","permalink":"http://localhost:1313/posts/archive/math-typesetting/","summary":"\u003cp\u003eMathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\u003c/p\u003e","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\nüôà :see_no_evil: üôâ :hear_no_evil: üôä :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n.emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","permalink":"http://localhost:1313/posts/archive/emoji-support/","summary":"\u003cp\u003eEmoji can be enabled in a Hugo project in a number of ways.\u003c/p\u003e","title":"Emoji Support"},{"content":"In my previous post, we seen how to configure kubernetes cluster ,how to deploy pods and grow the cluster. Now in this post i am going to show how to resource limiting cpu and memory in a kubernetes deployment. We can also limit resource at namespace level, which will be covered in the later post.\nI am going to use a special image vish/stress. This image has options for allocating cpu and memory, which can be parsed using an argument for doing the stress test.\nMy configuration for Master and Worker node is 4GB memory with 2 CPU cores each running in Virtualbox.\nFirst download the vish/stress image and create the deployment. The image vish/stress provides option for creating stress in cpu and memory\n{% highlight console %}\nvikki@drona-child-1:~$ kubectl run stress-test \u0026ndash;image=vish/stress deployment.apps \u0026ldquo;stress-test\u0026rdquo; created\n{% endhighlight %}\nWait till the pods status changes to running.\n{% highlight console %}\nvikki@drona-child-1:$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-768979984b-mmgbj 1/1 Running 6 65d stress-test-7795ffcbb-r9mft 0/1 ContainerCreating 0 6s vikki@drona-child-1:$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-768979984b-mmgbj 1/1 Running 6 65d stress-test-7795ffcbb-r9mft 1/1 Running 0 32s\n{% endhighlight %}\nNow verify the logs from respective container image. In my case the container name for pod stress-test-7795ffcbb-r9mft is e8e43da13b23.\nYou can also use \u0026quot;kubernetes logs stress-test-7795ffcbb-r9mft\u0026quot;(but its not working in my server)\nIt shows allocating 0 memory. By default the stress image will not allocation any memory/cpu.\n{% highlight console %}\n[root@drona-child-3 ~]# docker logs e8e43da13b23 I0819 13:01:00.200714 1 main.go:26] Allocating \u0026ldquo;0\u0026rdquo; memory, in \u0026ldquo;4Ki\u0026rdquo; chunks, with a 1ms sleep between allocations I0819 13:01:00.200804 1 main.go:29] Allocated \u0026ldquo;0\u0026rdquo; memory\n{% endhighlight %}\nLimiting Memory and CPU for the pod We are going to allocate more memory for this deployment and also going to set resource limit for CPU and Memory. Finally will monitor the deployment behaviour.\nExport the yaml for the current deployment \u0026ldquo;stress-test\u0026rdquo; and add the resource limit option and Memory/CPU allocation option.\n{% highlight console %}\nvikki@drona-child-1:$ kubectl get deployment stress-test -o yaml \u0026gt; stress.test.yaml vikki@drona-child-1:$ vim stress.test.yaml\n{% endhighlight %}\nNow in the above example i restricted cpu to 1 cores and memory to 4GB using limits options. Also i added the argument to allocate 2 cores and memory of 5050MB(~5GB).\nThe requests and limits option is analogous to the soft and hard limit in Linux.\nMy total memory available in the worker node is only 4 GB,I am over allocating memory just to check the behaviour. In real case you will be limiting the resources lesser than the available Memory/CPU, otherwise it makes no sense.\nNow apply the new yaml to the deployment and wait for the pods to go running status.\n{% highlight console %}\nvikki@drona-child-1:$ kubectl replace -f stress.test.yaml deployment.extensions \u0026ldquo;stress-test\u0026rdquo; replaced vikki@drona-child-1:$\n{% endhighlight %}{% highlight console %}\nvikki@drona-child-1:$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-768979984b-mmgbj 1/1 Running 6 65d stress-test-7795ffcbb-r9mft 0/1 ContainerCreating 0 8s stress-test-78944d5478-wmmtc 0/1 Terminating 0 20m vikki@drona-child-1:$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-768979984b-mmgbj 1/1 Running 6 65d stress-test-7795ffcbb-r9mft 1/1 Running 0 32s vikki@drona-child-1:~$\n{% endhighlight %}\nCheck the logs in the container. Its trying to allocate 5050MB memory and 2 CPU.\n{% highlight console %}\n[root@drona-child-3 ~]# docker logs a9ebee58d3ca I0819 13:36:36.618405 1 main.go:26] Allocating \u0026ldquo;5050Mi\u0026rdquo; memory, in \u0026ldquo;100Mi\u0026rdquo; chunks, with a 1s sleep between allocations I0819 13:36:36.618655 1 main.go:39] Spawning a thread to consume CPU I0819 13:36:36.618674 1 main.go:39] Spawning a thread to consume CPU\n{% endhighlight %}\nOpen a new terminal and monitor the Memory usage.\nThe memory will slowly raises to full usage and finally drops .After some attempts the pod will go \u0026ldquo;Crashloopbackoff\u0026rdquo; status.\nMemory value plotted in graph {% highlight console %} vikki@drona-child-1:$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-768979984b-mmgbj 1/1 Running 6 65d stress-test-57bb689598-8h4zm 0/1 CrashLoopBackOff 3 4m vikki@drona-child-1:$\n{% endhighlight %}\nTo verify the reason for container termination, we can run the describe pod. In our case it clearly say OOMKilled and was 26 times restarted.\n{% highlight console %}\nvikki@drona-child-1:$ kubectl describe pod stress-test-dbbcf4fd7-pqh99 |grep -A 5 State State: Waiting Reason: CrashLoopBackOff Last State: Terminated Reason: OOMKilled Exit Code: 137 Started: Sun, 19 Aug 2018 21:23:01 +0530 Finished: Sun, 19 Aug 2018 21:23:24 +0530 Ready: False vikki@drona-child-1:$ kubectl describe pod stress-test-dbbcf4fd7-pqh99 |grep -i restart Restart Count: 26 Warning BackOff 2m (x480 over 2h) kubelet, drona-child-3 Back-off restarting failed container vikki@drona-child-1:~$\n{% endhighlight %}\n","permalink":"http://localhost:1313/posts/2018-10-08-resource-limiting-cpu-and-memory-in-kubernetes/","summary":"\u003cp\u003eIn my previous post, we seen how to \u003ca href=\"/kubernetes-on-ubuntu-18-04-with-dashbaoard\"\u003econfigure kubernetes cluster\u003c/a\u003e ,\u003ca href=\"/kubernetes-growing-the-cluster-with-centos-7-node/\"\u003ehow to deploy pods and grow the cluster\u003c/a\u003e. Now in this post i am going to show how to resource limiting cpu and memory in a kubernetes deployment. We can also limit resource at namespace level, which will be covered in the later post.\u003c/p\u003e\n\u003cp\u003eI am going to use a special image \u003ca href=\"https://hub.docker.com/r/vish/stress/\"\u003evish/stress\u003c/a\u003e. This image has options for allocating cpu and memory, which can be parsed using an argument for doing the stress test.\u003c/p\u003e","title":"Resource limiting CPU and Memory in Kubernetes"},{"content":"In my previous post we seen how to install and configure kubernetes master node and dashboard on Ubuntu 18.04. Now this post is about growing the Kubernetes master by joining more nodes. For this setup i am going to use a Centos 7 VM running in virtualbox.\nInstallation Fist update the centos with all latest packages\n{% highlight console %}\n[root@drona-child-3 ~]# yum update -y\n{% endhighlight %}\nInstall docker and enable in startup\n{% highlight console %}\n[root@drona-child-3 ~]# yum install docker [root@drona-child-3 ~]# systemctl enable docker \u0026amp;\u0026amp; systemctl start docker Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.\n{% endhighlight %}\nNow add the kubernetes repository to yum configuration\n{% highlight console %}\n[root@drona-child-3 ~]# cat \u0026laquo;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo\n[kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF\n{% endhighlight %}\nDisable selinux. For permanant disable edit the file \u0026ldquo;/etc/sysconfig/selinux\u0026rdquo; otherwise the kube-flannel-xxx will goes to crashloop in next reboot.\nAfter that install kubernetes packages and enable in startup.\n{% highlight console %}\n[root@drona-child-3 ~]# setenforce 0 [root@drona-child-3 ~]# yum install -y kubelet kubeadm [root@drona-child-3 ~]# systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /etc/systemd/system/kubelet.service. [root@drona-child-3 ~]#\n{% endhighlight %}\nAdd the host entry for name resolution\n{% highlight console %}\n[root@drona-child-3 ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.0.5 drona-child-1 192.168.0.4 drona-child-3 [root@drona-child-3 ~]#\n{% endhighlight %}\nDisable swap\n{% highlight console %}\n[root@drona-child-3 ~]# swapoff -a\n{% endhighlight %}\nTill this step everything is same as we did in kubernetes master, except the difference of centos 7 operation system.\nAdding nodes to the cluster Now join the node to the kubernetes master using the join command. We already seen in the previous post how to get the token and hash in case you didn\u0026rsquo;t note it during master installation.\n{% highlight console %}\n[root@drona-child-3 ~]# kubeadm join 192.168.1.5:6443 \u0026ndash;token o9an7t.o4bs1up74xjwnol3 \u0026ndash;discovery-token-ca-cert-hash sha256:548c922cf4f845f3dc6d7da407516652879c8a5085c87e0322770e1475105591 [preflight] Running pre-flight checks. [WARNING FileExisting-crictl]: crictl not found in system path Suggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl [discovery] Trying to connect to API Server \u0026ldquo;192.168.1.5:6443\u0026rdquo; [discovery] Created cluster-info discovery client, requesting info from \u0026ldquo;https://192.168.1.5:6443\u0026rdquo; [discovery] Requesting info from \u0026ldquo;https://192.168.1.5:6443\u0026rdquo; again to validate TLS against the pinned public key [discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \u0026ldquo;192.168.1.5:6443\u0026rdquo; [discovery] Successfully established connection with API Server \u0026ldquo;192.168.1.5:6443\u0026rdquo;\nThis node has joined the cluster:\nCertificate signing request was sent to master and a response was received. The Kubelet was informed of the new secure connection details. Run \u0026lsquo;kubectl get nodes\u0026rsquo; on the master to see this node join the cluster.\n{% endhighlight %}\nCheck if the flannel interface is created and should have the pod network ip 40.168.x.x\n{% highlight console %}\n[root@drona-child-3 ~]# ip a show flannel.1 5: flannel.1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN group default link/ether ee:0d:1d:5c:48:6a brd ff:ff:ff:ff:ff:ff inet 40.168.1.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::ec0d:1dff:fe5c:486a/64 scope link valid_lft forever preferred_lft forever [root@drona-child-3 ~]#\n{% endhighlight %}\nSince cluster and authentication keys(~/.kube/config) not configured in secondary nodes(drona-child-3) we cannot run kubectl get nodes in secondary.\nWe have to do all the orchestration activity from the master node. I am connecting to the master node and checking the nodes status\n{% highlight console %}\nvikki@drona-child-1:$ kubectl get nodes NAME STATUS ROLES AGE VERSION drona-child-1 Ready master 22h v1.10.4 drona-child-3 Ready 1m v1.10.4 vikki@drona-child-1:$\n{% endhighlight %}\nCreating deployment in kubernetes cluster Let try deploying a pod. I am using nginx server. The below command will automatically pull the nginx image from the docker hub and deploy it as pod.\n{% highlight console %}\nvikki@drona-child-1:$ kubectl run nginx \u0026ndash;image nginx deployment.apps \u0026ldquo;nginx\u0026rdquo; created vikki@drona-child-1:$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-65899c769f-nllp5 1/1 Running 0 5m\n{% endhighlight %}\nNow we can see the deployment \u0026ldquo;nginx\u0026rdquo; is created.\n{% highlight console %}\nvikki@drona-child-1:~$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 1 1 1 0 2s\n{% endhighlight %}\nTo see more details about the deployment , use the describe command\n{% highlight console %}\nvikki@drona-child-1:$ kubectl describe deployment nginx Name: nginx Namespace: default CreationTimestamp: Fri, 15 Jun 2018 15:13:02 +0530 Labels: run=nginx Annotations: deployment.kubernetes.io/revision=1 Selector: run=nginx Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 1 max unavailable, 1 max surge Pod Template: Labels: run=nginx Containers: nginx: Image: nginx Port: Host Port: Environment: Mounts: Volumes: Conditions: Type Status Reason \u0026mdash;- \u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash; Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: NewReplicaSet: nginx-65899c769f (1/1 replicas created) Events: Type Reason Age From Message \u0026mdash;- \u0026mdash;\u0026mdash; \u0026mdash;- \u0026mdash;- \u0026mdash;\u0026mdash;- Normal ScalingReplicaSet 9m deployment-controller Scaled up replica set nginx-65899c769f to 1 vikki@drona-child-1:$\n{% endhighlight %}\nScaling the pods Now let scale the pod to 3 replica\n{% highlight console %}\nvikki@drona-child-1:$ kubectl scale deployment nginx \u0026ndash;replicas=3 deployment.extensions \u0026ldquo;nginx\u0026rdquo; scaled vikki@drona-child-1:$ kubectl get deployment nginx NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 3 3 3 3 46m\n{% endhighlight %}\nList the pods and verify it.\n{% highlight console %}\nvikki@drona-child-1:$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-768979984b-6d27s 1/1 Running 0 1m 40.168.1.5 drona-child-3 nginx-768979984b-mmgbj 1/1 Running 0 1m 40.168.1.4 drona-child-3 nginx-768979984b-vm74x 1/1 Running 0 13m 40.168.1.3 drona-child-3 vikki@drona-child-1:$\n{% endhighlight %}\nNow delete one of the pod and see if it is automatically scaling up to 3\n{% highlight console %}\nvikki@drona-child-1:$ kubectl delete pod nginx-768979984b-vm74x pod \u0026ldquo;nginx-768979984b-vm74x\u0026rdquo; deleted vikki@drona-child-1:$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-768979984b-6d27s 1/1 Running 0 6m 40.168.1.5 drona-child-3 nginx-768979984b-9lddt 0/1 ContainerCreating 0 2s drona-child-3 nginx-768979984b-mmgbj 1/1 Running 0 6m 40.168.1.4 drona-child-3 nginx-768979984b-vm74x 0/1 Terminating 0 18m drona-child-3 vikki@drona-child-1:~$\n{% endhighlight %}{% highlight console %}\nvikki@drona-child-1:$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-768979984b-6d27s 1/1 Running 0 6m 40.168.1.5 drona-child-3 nginx-768979984b-9lddt 1/1 Running 0 20s 40.168.1.6 drona-child-3 nginx-768979984b-mmgbj 1/1 Running 0 6m 40.168.1.4 drona-child-3 vikki@drona-child-1:$\n{% endhighlight %}\n","permalink":"http://localhost:1313/posts/2018-06-15-kubernetes-growing-the-cluster-with-centos-7-node/","summary":"\u003cp\u003eIn my \u003ca href=\"/kubernetes-on-ubuntu-18-04-with-dashbaoard/#kubernetes-token-generation\"\u003eprevious post\u003c/a\u003e we seen how to install and configure kubernetes master node and dashboard on Ubuntu 18.04. Now this post is about growing the Kubernetes master by joining more nodes. For this setup i am going to use a Centos 7 VM running in virtualbox.\u003c/p\u003e\n\u003c!--kg-card-begin: image--\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"/content/images/2018/06/vikki_centos7_vbox.jpg\" class=\"kg-image\" alt=\"vikki_centos7_vbox\"\u003e\u003c/figure\u003e\u003c!--kg-card-end: image--\u003e\n\u003ch3 id=\"installation\"\u003eInstallation\u003c/h3\u003e\n\u003cp\u003eFist update the centos with all latest packages\u003c/p\u003e\n\u003cp\u003e{% highlight console %}\u003c/p\u003e\n\u003cp\u003e[root@drona-child-3 ~]# yum update -y\u003c/p\u003e\n\u003cp\u003e{% endhighlight %}\u003c/p\u003e","title":"Kubernetes - Growing the cluster with Centos 7 node"},{"content":"This post i am going to show how to install Kubernetes, configure Master node and enable Kubernetes dashboard in Ubuntu 18.04 LTS. I also tried to show the video demo explaining the entire configuration in the end of this post, This is my first video demo!!!\nThis post has been updated for kubernetes version 1.18\nSetup I am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\nConfiguration First install docker, it is provided by the default ubuntu 18.04 repository.\n{% highlight console %}\nsudo apt-get install docker.io\n{% endhighlight %}\nEnable docker to auto start during reboot\n{% highlight console %}\nsudo systemctl enable docker Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install. Executing: /lib/systemd/systemd-sysv-install enable docker\n{% endhighlight %}{% highlight console %}\nsudo apt-get install curl\n{% endhighlight %}\nDownload the gpg key for kubernetes installation and add to ubuntu\n{% highlight console %}\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add OK\n{% endhighlight %}\nNow add the Google\u0026rsquo;s kubernetes repository\n{% highlight console %}\nsudo apt-add-repository \u0026ldquo;deb http://apt.kubernetes.io/ kubernetes-xenial main\u0026rdquo;\n{% endhighlight %}\nInstall kubeadm\n{% highlight console %}\nsudo apt-get install kubeadm\n{% endhighlight %}\nIf you want to disable the swap permanantly, edit /etc/fstab and comment the swap filesystem line. Kubernetes will not start if swap is enabled.\n{% highlight console %}\nsudo swapoff -a\n{% endhighlight %}\nMy IP address of the VM is 10.0.0.1 which i am going to advertise as apiserver and 40.168.0.0/16 is the newtwork for pod communication\n{% highlight console %}\nsudo kubeadm init \u0026ndash;pod-network-cidr=40.168.0.0/16 \u0026ndash;apiserver-advertise-address=10.0.0.1\nW0624 14:54:54.575835 4007 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] [init] Using Kubernetes version: v1.18.4 [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected \u0026ldquo;cgroupfs\u0026rdquo; as the Docker cgroup driver. The recommended driver is \u0026ldquo;systemd\u0026rdquo;. Please follow the guide at https://kubernetes.io/docs/setup/cri/ [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026lsquo;kubeadm config images pull\u0026rsquo; [kubelet-start] Writing kubelet environment file with flags to file \u0026ldquo;/var/lib/kubelet/kubeadm-flags.env\u0026rdquo; [kubelet-start] Writing kubelet configuration to file \u0026ldquo;/var/lib/kubelet/config.yaml\u0026rdquo; [kubelet-start] Starting the kubelet [certs] Using certificateDir folder \u0026ldquo;/etc/kubernetes/pki\u0026rdquo; [certs] Generating \u0026ldquo;ca\u0026rdquo; certificate and key [certs] Generating \u0026ldquo;apiserver\u0026rdquo; certificate and key [certs] apiserver serving cert is signed for DNS names [kubernetes1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.0.0.1] [certs] Generating \u0026ldquo;apiserver-kubelet-client\u0026rdquo; certificate and key [certs] Generating \u0026ldquo;front-proxy-ca\u0026rdquo; certificate and key [certs] Generating \u0026ldquo;front-proxy-client\u0026rdquo; certificate and key [certs] Generating \u0026ldquo;etcd/ca\u0026rdquo; certificate and key [certs] Generating \u0026ldquo;etcd/server\u0026rdquo; certificate and key [certs] etcd/server serving cert is signed for DNS names [kubernetes1 localhost] and IPs [10.0.0.1 127.0.0.1 ::1] [certs] Generating \u0026ldquo;etcd/peer\u0026rdquo; certificate and key [certs] etcd/peer serving cert is signed for DNS names [kubernetes1 localhost] and IPs [10.0.0.1 127.0.0.1 ::1] [certs] Generating \u0026ldquo;etcd/healthcheck-client\u0026rdquo; certificate and key [certs] Generating \u0026ldquo;apiserver-etcd-client\u0026rdquo; certificate and key [certs] Generating \u0026ldquo;sa\u0026rdquo; key and public key [kubeconfig] Using kubeconfig folder \u0026ldquo;/etc/kubernetes\u0026rdquo; [kubeconfig] Writing \u0026ldquo;admin.conf\u0026rdquo; kubeconfig file [kubeconfig] Writing \u0026ldquo;kubelet.conf\u0026rdquo; kubeconfig file [kubeconfig] Writing \u0026ldquo;controller-manager.conf\u0026rdquo; kubeconfig file [kubeconfig] Writing \u0026ldquo;scheduler.conf\u0026rdquo; kubeconfig file [control-plane] Using manifest folder \u0026ldquo;/etc/kubernetes/manifests\u0026rdquo; [control-plane] Creating static Pod manifest for \u0026ldquo;kube-apiserver\u0026rdquo; [control-plane] Creating static Pod manifest for \u0026ldquo;kube-controller-manager\u0026rdquo; W0624 14:57:38.087288 4007 manifests.go:225] the default kube-apiserver authorization-mode is \u0026ldquo;Node,RBAC\u0026rdquo;; using \u0026ldquo;Node,RBAC\u0026rdquo; [control-plane] Creating static Pod manifest for \u0026ldquo;kube-scheduler\u0026rdquo; W0624 14:57:38.089706 4007 manifests.go:225] the default kube-apiserver authorization-mode is \u0026ldquo;Node,RBAC\u0026rdquo;; using \u0026ldquo;Node,RBAC\u0026rdquo; [etcd] Creating static Pod manifest for local etcd in \u0026ldquo;/etc/kubernetes/manifests\u0026rdquo; [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026ldquo;/etc/kubernetes/manifests\u0026rdquo;. This can take up to 4m0s [apiclient] All control plane components are healthy after 21.003160 seconds [upload-config] Storing the configuration used in ConfigMap \u0026ldquo;kubeadm-config\u0026rdquo; in the \u0026ldquo;kube-system\u0026rdquo; Namespace [kubelet] Creating a ConfigMap \u0026ldquo;kubelet-config-1.18\u0026rdquo; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see \u0026ndash;upload-certs [mark-control-plane] Marking the node kubernetes1 as control-plane by adding the label \u0026ldquo;node-role.kubernetes.io/master=\u0026rsquo;\u0026rsquo;\u0026rdquo; [mark-control-plane] Marking the node kubernetes1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: u9ibqd.0x3nkr0lze6o5d7u [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026ldquo;cluster-info\u0026rdquo; ConfigMap in the \u0026ldquo;kube-public\u0026rdquo; namespace [kubelet-finalize] Updating \u0026ldquo;/etc/kubernetes/kubelet.conf\u0026rdquo; to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy\nYour Kubernetes control-plane has initialized successfully!\nTo start using your cluster, you need to run the following as a regular user:\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config\nYou should now deploy a pod network to the cluster. Run \u0026ldquo;kubectl apply -f [podnetwork].yaml\u0026rdquo; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/\nThen you can join any number of worker nodes by running the following on each as root:\nkubeadm join 10.0.0.1:6443 \u0026ndash;token u9ibqd.0x3nkr0lze6o5d7u \u0026ndash;discovery-token-ca-cert-hash sha256:9669e0ef3814e40ab4a3c2c85a450635ad6c75c25be931fdd064fda9fb9e7b89\n{% endhighlight %}\nTake a note of the join command, this will be used to join the other nodes to this kubernetes cluster.\nThe token generated is valid only for 24hours. In case the 24 hours exceed we need to generate the new token using the command \u0026ldquo;sudo kubeadm token create\u0026rdquo; .The current token can be view from master using the below command.\n{% highlight console %}\nsudo kubeadm token list\nTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS o9an7t.o4bs1up74xjwnol3 1h 2018-06-15T14:39:02+05:30 authentication,signing The default bootstrap token generated by \u0026lsquo;kubeadm init\u0026rsquo;. system:bootstrappers:kubeadm:default-node-token\n{% endhighlight %}\nThe CA cert hash is used for the node to join in secure manner. This also can be view from the below command. Combaining this token and hash we can join the nodes.\n{% highlight console %}\nopenssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | openssl dgst -sha256 -hex | sed \u0026rsquo;s/^.* //\u0026rsquo;\n548c922cf4f845f3dc6d7da407516652879c8a5085c87e0322770e1475105591\n{% endhighlight %}\nCreate the necessary directories\n{% highlight console %}\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config\n{% endhighlight %}\nNow deploy the network for pod communications , i am using the calico networking\n{% highlight console %}\nsudo kubectl apply -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml\nconfigmap/calico-config created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-node created daemonset.apps/calico-node created serviceaccount/calico-node created deployment.apps/calico-kube-controllers created serviceaccount/calico-kube-controllers created\n{% endhighlight %}\nNow wait for all pods to change to \u0026ldquo;Running\u0026rdquo; status. At this point all the pods should be in running status.\n{% highlight console %}\nsudo kubectl get pods \u0026ndash;all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-76d4774d89-mfjdf 0/1 Running 0 7m29s kube-system calico-node-lfz6b 1/1 Running 0 7m29s kube-system coredns-66bff467f8-kg47r 1/1 Running 0 8m15s kube-system coredns-66bff467f8-mbl88 1/1 Running 0 8m16s kube-system etcd-kubernetes1 1/1 Running 1 8m24s kube-system kube-apiserver-kubernetes1 1/1 Running 1 8m24s kube-system kube-controller-manager-kubernetes1 1/1 Running 1 8m24s kube-system kube-proxy-t2rmj 1/1 Running 1 8m16s kube-system kube-scheduler-kubernetes1 1/1 Running 1 8m24s\n{% endhighlight %}\nNow deploy the kubernetes dashoard. Kubernetes dashboard is used to manage the kubernetes cluster using the web GUI interface.\n{% highlight console %}\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml namespace/kubernetes-dashboard created serviceaccount/kubernetes-dashboard created service/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created secret/kubernetes-dashboard-csrf created secret/kubernetes-dashboard-key-holder created configmap/kubernetes-dashboard-settings created role.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created deployment.apps/kubernetes-dashboard created service/dashboard-metrics-scraper created deployment.apps/dashboard-metrics-scraper created\n{% endhighlight %}\nWait for kubernetes-dashboard-xxxx pods to goes \u0026ldquo;Running\u0026rdquo; status\n{% highlight console %}\nsudo kubectl get pods \u0026ndash;all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-76d4774d89-mfjdf 1/1 Running 0 10m kube-system calico-node-lfz6b 1/1 Running 0 10m kube-system coredns-66bff467f8-kg47r 1/1 Running 0 11m kube-system coredns-66bff467f8-mbl88 1/1 Running 0 11m kube-system etcd-kubernetes1 1/1 Running 1 11m kube-system kube-apiserver-kubernetes1 1/1 Running 1 11m kube-system kube-controller-manager-kubernetes1 1/1 Running 1 11m kube-system kube-proxy-t2rmj 1/1 Running 1 11m kube-system kube-scheduler-kubernetes1 1/1 Running 1 11m kubernetes-dashboard dashboard-metrics-scraper-6b4884c9d5-j757b 1/1 Running 0 51s kubernetes-dashboard kubernetes-dashboard-7b544877d5-pklbm 1/1 Running 0 52s\n{% endhighlight %}\nOnce the kubernetes-dashboard-xxxxxxx container goes to \u0026ldquo;Running\u0026rdquo; state we can start the kubectly proxy with the below command. If you want to start only in localhost then you can change the address option to localhost\n{% highlight console %}\nsudo kubectl proxy \u0026ndash;address=0.0.0.0\nStarting to serve on [::]:8001 {% endhighlight %}\nNow open the below kubernetes dashboard URL in browser\nhttp://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\nTo properly login to the kubernetes dashboard we need to creat a service account and assign the proper role.\n{% highlight console %} kubectl create serviceaccount dashboard -n default\nserviceaccount \u0026ldquo;dashboard\u0026rdquo; created {% endhighlight %}\n{% highlight console %} kubectl create clusterrolebinding dashboard-admin -n default \u0026ndash;clusterrole=cluster-admin \u0026ndash;serviceaccount=default:dashboard\nclusterrolebinding.rbac.authorization.k8s.io \u0026ldquo;dashboard-admin\u0026rdquo; created {% endhighlight %}\nNow the password to login kubernetes dashboard can be viewed by running the below command. Copy the decoded password and login to dashboard.\n{% highlight console %}\nkubectl get secret $(kubectl get serviceaccount dashboard -o jsonpath=\u0026quot;{.secrets[0].name}\u0026quot;) -o jsonpath=\u0026quot;{.data.token}\u0026quot; | base64 \u0026ndash;decode\neyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRhc2hib2FyZC10b2tlbi1nOGxnZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkYXNoYm9hcmQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI1MDU1ZmE1ZC02ZmI0LTExZTgtOTgzZi0wODAwMjcxNjVmMDYiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDpkYXNoYm9hcmQifQ.LAdmbfTmx8HACuZy3hRJzS4rP7LH2PaSi85oeNnuP4tWbtSVqonnkXNrENsrODA0zE8Dh77rCrKWM3j3HElJF8Wy3TzYVa2aopBxbIrqZgwESfeuUhhjc5twEE_zrxyVpbch1-lU_ye29li2iYn1649yDYosxBhKsic9A3dOtGWJuiTmFTYrt1cwzKRC201yAIdxxP6BiIm_h8nK5pAw_TPpF1tOxo_P4Zy9msn8tDR1NM_GX5Q10FJO8Ef7zNgFr9jgh_b_BnCj61niiIKzGJYzXs0b3Q3nvohoGfeGavTpG1eMc26-rJ1MwCT1beOvdkJv4DyjJFENPcZaalY0yw {% endhighlight %}\nVideo hands-on demo My next post on kubernetes Kubernetes scaling the cluster\n","permalink":"http://localhost:1313/posts/2018-06-14-kubernetes-on-ubuntu-18-04-with-dashbaoard/","summary":"\u003cp\u003eThis post i am going to show how to install Kubernetes, configure Master node and enable Kubernetes dashboard in Ubuntu 18.04 LTS. I also tried to show the ¬†video demo explaining the entire configuration in the end of this post, This is my first video demo!!!\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis post has been updated for kubernetes \u003cmark\u003eversion 1.18\u003c/mark\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"setup\"\u003eSetup\u003c/h3\u003e\n\u003cp\u003eI am using the Virtualbox(running in Ubuntu 18.04 physical machine) for this entire setup . The physical machine is Dell inspiron laptop with 12GB RAM , Intel¬Æ Core‚Ñ¢ i7-6500U CPU @ 2.50GHz √ó 4 and 512GB SSD hardisk.\u003c/p\u003e","title":"Kubernetes on Ubuntu 18.04 - Master and Dashboard setup"},{"content":"‡Æö‡Æø‡Æ§‡Øç‡Æ§‡Æø‡Æ∞‡Øà ‡Æé‡Æ©‡Øç‡Æ±‡Ææ‡Æ≤‡Øá ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æµ‡Æø‡Æ¥‡Ææ ‡ÆÆ‡Ææ‡Æ§‡ÆÆ‡Øç\u0026hellip; ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø, ‡Æ™‡Æ∞‡Æø‡Æü‡Øç‡Æö‡Øà ‡Æé‡Æ©‡Øç‡Æ±‡ØÅ ‡Æ®‡Øä‡Æ®‡Øç‡Æ§‡ØÅ‡Æ™‡Øã‡Æ© ‡Æö‡Æø‡Æ±‡Ææ‡Æ∞‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡ÆÖ‡Æ§‡ØÅ ‡Æâ‡ÆØ‡Æø‡Æ∞‡Øç ‡Æ™‡ØÅ‡Æ§‡ØÅ‡Æ™‡Øç‡Æ™‡Æø‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç ‡ÆÜ‡Æï‡Øç‡Æö‡Æø‡Æú‡Æ©‡Øç ‡Æé‡Æ©‡Øç‡Æ±‡Ææ‡Æ≤‡Øç ‡ÆÆ‡Æø‡Æï‡Æö‡Øç ‡Æö‡Æ∞‡Æø‡ÆØ‡Ææ‡Æï ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç.\n‡ÆÖ‡Æï‡Øç‡Æï‡Ææ‡Æ≤‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡ÆÆ‡Æ¥‡Øà ‡Æ™‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡Æµ‡Øá‡Æ≥‡Ææ‡Æ£‡Øç‡ÆÆ‡Øà, ‡ÆÖ‡Æ±‡ØÅ‡Æµ‡Æü‡Øà ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡ÆÆ‡Æ±‡Øç‡Æ± ‡ÆÆ‡Ææ‡Æ§‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æì‡Æü‡ØÅ‡ÆÆ‡Øç. ‡Æö‡Æø‡Æ§‡Øç‡Æ§‡Æø‡Æ∞‡Øà ‡Æµ‡ØÜ‡ÆØ‡Æø‡Æ≤‡Æø‡Æ≤‡Øç ‡Æµ‡Øá‡Æ≤‡Øà ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ ‡ÆÆ‡ØÅ‡Æü‡Æø‡ÆØ‡Ææ ‡Æï‡Ææ‡Æ∞‡Æ£‡Æ§‡Øç‡Æ§‡Ææ‡Æ≤‡Øç ‡ÆÖ‡Æ§‡ØÅ ‡ÆÆ‡ØÅ‡Æ¥‡ØÅ‡Æ§‡ØÅ‡ÆÆ‡Øç ‡Æï‡Øä‡Æ£‡Øç‡Æü‡Ææ‡Æü‡Øç‡Æü‡Æô‡Øç‡Æï‡Æ≥‡Øç, ‡Æ™‡Æ£‡Øç‡Æü‡Æø‡Æï‡Øà‡Æï‡Æ≥‡Øç ‡Æé‡Æ©‡Øç‡Æ±‡ØÅ ‡Æï‡Æ¥‡Æø(‡Æ≥‡Æø)‡Æ§‡Øç‡Æ§‡Æ©‡Æ∞‡Øç ‡Æ®‡ÆÆ‡Øç ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Øã‡Æ∞‡Øç.\n‡ÆÖ‡Æ§‡ØÅ 15 ‡Æ®‡Ææ‡Æ≥‡Øç ‡Æï‡Øä‡Æ£‡Øç‡Æü‡Ææ‡Æü‡Øç‡Æü‡ÆÆ‡Øç\u0026hellip; ‡Æ™‡ØÇ‡Æö‡Øç‡Æö‡Øä‡Æ∞‡Æø‡Æ§‡Æ≤‡Øç ‡Æ§‡Øä‡Æü‡Æô‡Øç‡Æï‡Æø ‡ÆÆ‡ØÅ‡Æ≥‡Øà‡Æ™‡Øç‡Æ™‡Ææ‡Æ∞‡Æø ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æ®‡Ææ‡Æ≥‡Øç ‡Æµ‡Æ∞‡Øà ‡Æ™‡Ææ‡Æü‡ØÅ ‡ÆÖ‡ÆÆ‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Æ≥‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡ÆÆ‡Øç. ‡Æö‡ØÜ‡Æ©‡Øç‡Æ©‡Øà ‡Æ™‡Øã‡Æ©‡Øç‡Æ± ‡Æ™‡ØÜ‡Æ∞‡ØÅ‡Æ®‡Æï‡Æ∞‡Æô‡Øç‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æµ‡Æö‡Æø‡Æ™‡Øç‡Æ™‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡ÆÜ‡Æ£‡Øç‡Æü‡ØÅ‡Æ§‡Øã‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æä‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡Æö‡Øç ‡Æö‡ØÜ‡Æ≤‡Øç‡Æµ‡Æ§‡Øá ‡Æá‡Æ§‡ØÅ‡Æ™‡Øã‡Æ©‡Øç‡Æ± ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æµ‡Æø‡Æ¥‡Ææ‡Æï‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡Æ§‡Øç‡Æ§‡Ææ‡Æ©‡Øç.\n‡Æö‡Æø‡Æ§‡Øç‡Æ§‡Æø‡Æ∞‡Øà ‡ÆÆ‡Ææ‡Æ§ ‡Æû‡Ææ‡ÆØ‡Æø‡Æ±‡ØÅ‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æ™‡ØÇ‡Æö‡Øç‡Æö‡Øä‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡ØÅ, ‡ÆÖ‡Æü‡ØÅ‡Æ§‡Øç‡Æ§ ‡Æé‡Æü‡Øç‡Æü‡Ææ‡ÆÆ‡Øç ‡Æ®‡Ææ‡Æ≥‡Øç ‡Æï‡Ææ‡Æ™‡Øç‡Æ™‡ØÅ ‡Æï‡Æü‡Øç‡Æü‡Æø, ‡Æ®‡Æü‡ØÅ‡Æµ‡Æø‡Æ≤‡Øç ‡Æâ‡Æ≥‡Øç‡Æ≥ ‡Æí‡Æ∞‡ØÅ ‡Æµ‡Ææ‡Æ∞‡ÆÆ‡Øç ‡Æí‡Æµ‡Øç‡Æµ‡Øä‡Æ∞‡ØÅ ‡Æ§‡ØÜ‡Æ∞‡ØÅ ‡ÆÆ‡Æ£‡Øç‡Æü‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æ®‡Æü‡Æ§‡Øç‡Æ§‡Æø ‡Æï‡ØÇ‡Æ¥‡Øç ‡Æä‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆÆ‡ØÅ‡Æ±‡Øà ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ, ‡ÆÖ‡Æü‡ØÅ‡Æ§‡Øç‡Æ§ ‡Æû‡Ææ‡ÆØ‡Æø‡Æ±‡ØÅ ‡ÆÖ‡Æ©‡Øç‡Æ±‡ØÅ ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æµ‡Æø‡Æ¥‡Ææ ‡Æ®‡Æü‡Øà‡Æ™‡ØÜ‡Æ±‡ØÅ‡ÆÆ‡Øç.\n‡Æ§‡Æø‡Æ∞‡ØÅ‡Æµ‡Æø‡Æ¥‡Ææ ‡Æ®‡Ææ‡Æ≥‡Æ©‡Øç‡Æ±‡ØÅ ‡Æï‡Ææ‡Æ≤‡Øà ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Øá ‡Æ™‡Ææ‡Æ≤‡Øç ‡Æï‡ØÅ‡Æü‡ÆÆ‡Øç, ‡Æ™‡Æ≤ ‡Æµ‡Æø‡Æ§‡ÆÆ‡Ææ‡Æ© ‡Æï‡Ææ‡Æµ‡Æü‡Æø ‡ÆÜ‡Æü‡Øç‡Æü‡Æô‡Øç‡Æï‡Æ≥‡Øç, ‡Æá‡Æ∞‡Æµ‡Æø‡Æ≤‡Øç ‡Æ™‡Æ≤‡Øç‡Æ≤‡Æï‡Øç‡Æï‡ØÅ‡Æü‡Æ©‡Øç ‡Æè‡Æ§‡Øá‡Æ©‡ØÅ‡ÆÆ‡Øç ‡Æï‡Æø‡Æ∞‡Ææ‡ÆÆ‡Æø‡ÆØ ‡Æ®‡Æü‡Æ©‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Æø‡ÆØ‡Æµ‡Øà ‡Æµ‡Æø‡ÆÆ‡Æ∞‡Æø‡Æö‡Øà‡ÆØ‡Ææ‡Æï ‡Æ§‡ØÇ‡Æ≥‡Øç ‡Æ™‡Æ±‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç.\n‡ÆÆ‡Æ±‡ØÅ‡Æ®‡Ææ‡Æ≥‡Øç ‡Æ§‡Æø‡Æô‡Øç‡Æï‡Æ≥‡Æ©‡Øç‡Æ±‡ØÅ ‡Æ§‡Øá‡Æ∞‡Øã‡Æü‡Øç‡Æü‡Æ§‡Øç‡Æ§‡ØÅ‡Æü‡Æ©‡Øç ‡Æï‡Æü‡Øç‡Æü‡Æø‡ÆØ ‡Æï‡Ææ‡Æ™‡Øç‡Æ™‡Æø‡Æ©‡Øà ‡ÆÖ‡Æ±‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ, ‡ÆÖ‡Æü‡ØÅ‡Æ§‡Øç‡Æ§ ‡Æ®‡Ææ‡Æ≥‡Øç ‡ÆÆ‡ØÅ‡Æ≥‡Øà‡Æ™‡Øç‡Æ™‡Ææ‡Æ∞‡Æø ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æ®‡Ææ‡Æ≥‡Øç ‡Æ®‡Æü‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç. ‡Æá‡Æ§‡Øç‡Æ§‡ØÅ‡Æü‡Æ©‡Øç ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æµ‡Æø‡Æ¥‡Ææ ‡ÆÆ‡ØÅ‡Æü‡Æø‡Æµ‡Æü‡Øà‡ÆØ‡ØÅ‡ÆÆ‡Øç.\n‡Æé‡Æô‡Øç‡Æï‡Æ≥‡ØÇ‡Æ∞‡Øç ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æµ‡Æø‡Æ¥‡Ææ‡Æµ‡Æø‡Æ©‡Øç ‡Æö‡Æø‡Æ±‡ØÅ ‡Æö‡Æø‡Æ±‡ØÅ ‡Æ®‡Æø‡Æ©‡Øà‡Æµ‡ØÅ‡Æï‡Æ≥‡Øç, ‡Æá‡Æ§‡Øã ‡Æ™‡ØÅ‡Æï‡Øà‡Æ™‡Øç‡Æ™‡Æü‡Æô‡Øç‡Æï‡Æ≥‡Ææ‡ÆØ‡Øç\u0026hellip;.\n‡Æ§‡Ææ‡Æ∞, ‡Æ§‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Øà ‡Æï‡Æø‡Æ¥‡Æø‡ÆØ \u0026hellip;\n‡Æ™‡Ææ‡Æ∞‡Æ≥‡Æ®‡Øç‡Æ§ ‡Æ§‡Øá‡Æ∞‡ØÅ\u0026hellip;\n‡Æö‡Æ∞‡Æø ‡Æö‡ØÜ‡Æ≤‡Øç‡ÆÉ‡Æ™‡Æø ‡Æá‡Æ≤‡Øç‡Æ≤‡Ææ‡ÆÆ ‡Æé‡Æ™‡Øç‡Æü‡Æø\u0026hellip;\n‡Æâ‡Æ©‡Øç‡Æ© ‡ÆØ‡Ææ‡Æ∞‡Øç ‡Æ§‡Æ≤‡Øà‡Æµ‡Ææ ‡Æ§‡Æ™‡Øç‡Æ™‡Ææ‡Æü‡Øç‡Æü‡ÆÆ‡Øç‡Æ©‡ØÅ ‡Æö‡Øä‡Æ©‡Øç‡Æ©‡Æ§‡ØÅ\u0026hellip; ‡Æ®‡ØÄ‡Æ§‡Ææ‡Æ©‡Øç ‡Æö‡Æ∞‡Æø‡ÆØ‡Ææ‡Æ© ‡ÆÜ‡Æü‡Øç‡Æü‡ÆÆ‡Øç\u0026hellip;\n‡Æ™‡Øã‡Æü‡ØÅ ‡ÆÜ‡Æü‡Øç‡Æü‡ÆÆ‡Øç ‡Æ™‡Øã‡Æü‡ØÅ\u0026hellip;\n‡Æ™‡Ææ‡Æ≤‡Øç ‡Æï‡ØÅ‡Æü‡ÆÆ‡Øç\u0026hellip;\n‡Æï‡Ææ‡Æµ‡Æü‡Æø‡ÆØ‡Ææ‡ÆÆ‡Øç ‡Æï‡Ææ‡Æµ‡Æü‡Æø\u0026hellip; ‡ÆÆ‡ÆØ‡Æø‡Æ≤‡Øç ‡Æï‡Ææ‡Æµ‡Æü‡Æø\u0026hellip;\n‡Æ®‡Ææ‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡ÆÆ‡Øç ‡Æ§‡Øá‡Æ∞‡ØÅ ‡Æá‡Æ¥‡ØÅ‡Æ™‡Øç‡Æ™‡Øã‡ÆÆ‡Øç‡Æ≤\u0026hellip;\n‡Æï‡ØÅ‡Æ§‡Øç‡Æ§‡Ææ‡Æ≤ ‡ÆÖ‡Æ∞‡ØÅ‡Æµ‡Æø‡ÆØ‡Æø‡Æ≤‡Øá ‡Æï‡ØÅ‡Æ≥‡Æø‡Æö‡Øç‡Æö‡Æ§‡ØÅ ‡Æ™‡Øã‡Æ≤‡Øç ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡Æ§‡Ææ\u0026hellip;\n‡Æá‡Æ§‡ØÅ ‡Æé‡Æô‡Øç‡Æï ‡Æè‡Æ∞‡Æø‡ÆØ‡Ææ\u0026hellip;\n‡Æá‡Æµ‡Æ©‡Øç ‡ÆØ‡Ææ‡Æ∞‡ØÜ‡Æ©‡Øç‡Æ±‡ØÅ ‡Æ§‡ØÜ‡Æ∞‡Æø‡Æï‡Æø‡Æ±‡Æ§‡Ææ\u0026hellip; ‡Æ§‡ØÄ‡ÆØ‡ØÜ‡Æ©‡Øç‡Æ±‡ØÅ ‡Æ™‡ØÅ‡Æ∞‡Æø‡Æï‡Æø‡Æ±‡Æ§‡Ææ...\n!\n‡Æ®‡Æ≤‡Øç‡Æ≤ ‡ÆÉ‡Æ™‡Øã‡Æü‡Øç‡Æü‡Øã‡Æµ‡Ææ ‡Æé‡Æü‡ØÅ‡Æô‡Øç‡Æï\u0026hellip;DP ‡ÆÆ‡Ææ‡Æ§‡Øç‡Æ§‡Æ©‡ØÅ‡ÆÆ‡Øç\u0026hellip;\u0026hellip;. ‡Æá‡Æ®‡Øç‡Æ§ ‡Æ™‡Øã‡Æ∏‡Øç ‡Æí‡Æï‡Øá‡Æµ‡Ææ\u0026hellip;\n‡Æé‡Æô‡Øç‡Æï ‡Æä‡Æ∞‡ØÅ ‡Æï‡Ææ‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡Ææ‡Æ∞‡Æø‡ÆØ‡ÆÆ‡Øç‡ÆÆ‡Ææ\u0026hellip;\n‡Æ®‡Ææ‡Æü‡ØÅ ‡Æö‡ØÜ‡Æ¥‡Æø‡Æï‡Øç‡Æï ‡Æµ‡Øá‡Æ£‡ØÅ‡ÆÆ‡Øç\u0026hellip; ‡Æ®‡Æ≤‡Øç‡Æ≤ ‡ÆÆ‡Æ¥‡Øà ‡Æ™‡ØÜ‡ÆØ‡Øç‡ÆØ ‡Æµ‡Øá‡Æ£‡ØÅ‡ÆÆ‡Øç\u0026hellip;\n‡Æ§‡Øá‡Æ∞‡Øç ‡Æ§‡ØÅ‡Æ≥‡Æø‡Æï‡Æ≥‡Øç\u0026hellip; ‡ÆÆ‡Øä‡Æü‡Øç‡Æü‡Øà ‡Æµ‡ØÜ‡ÆØ‡Æø‡Æ≤‡Æø‡Æ≤‡Øç ‡Æ™‡Æü‡Øç‡Æü‡Øà‡ÆØ‡Øà ‡Æï‡Æø‡Æ≥‡Æ™‡Øç‡Æ™‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Æ±‡Øà ‡ÆÜ‡Æü‡Øç‡Æü‡ÆÆ‡Øç\u0026hellip;‡Æï‡Ææ‡Æ£‡Øä‡Æ≥‡Æø\u0026hellip;\n‡Æâ‡ÆØ‡Æ∞‡Øç‡Æ§‡Æ∞ ‡Æ™‡Æü‡Æ§‡Øç‡Æ§‡Øà ‡Æá‡Æô‡Øç‡Æï‡Øá ‡Æ™‡Æ§‡Æø‡Æµ‡Æø‡Æ±‡Æï‡Øç‡Æï‡ÆÆ‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡Æï‡Øä‡Æ≥‡Øç‡Æ≥‡Æ≤‡Ææ‡ÆÆ‡Øç Download Album\ncontent courtesy - Pratheba Chandramohan\n","permalink":"http://localhost:1313/posts/2018-05-22-thiruvizha-madukkur-2018/","summary":"\u003cp\u003e‡Æö‡Æø‡Æ§‡Øç‡Æ§‡Æø‡Æ∞‡Øà ‡Æé‡Æ©‡Øç‡Æ±‡Ææ‡Æ≤‡Øá ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æµ‡Æø‡Æ¥‡Ææ ‡ÆÆ‡Ææ‡Æ§‡ÆÆ‡Øç\u0026hellip; ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø, ‡Æ™‡Æ∞‡Æø‡Æü‡Øç‡Æö‡Øà ‡Æé‡Æ©‡Øç‡Æ±‡ØÅ ‡Æ®‡Øä‡Æ®‡Øç‡Æ§‡ØÅ‡Æ™‡Øã‡Æ© ‡Æö‡Æø‡Æ±‡Ææ‡Æ∞‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡ÆÖ‡Æ§‡ØÅ ‡Æâ‡ÆØ‡Æø‡Æ∞‡Øç ‡Æ™‡ØÅ‡Æ§‡ØÅ‡Æ™‡Øç‡Æ™‡Æø‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç ‡ÆÜ‡Æï‡Øç‡Æö‡Æø‡Æú‡Æ©‡Øç ‡Æé‡Æ©‡Øç‡Æ±‡Ææ‡Æ≤‡Øç ‡ÆÆ‡Æø‡Æï‡Æö‡Øç ‡Æö‡Æ∞‡Æø‡ÆØ‡Ææ‡Æï ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç.\u003c/p\u003e\n\u003cp\u003e‡ÆÖ‡Æï‡Øç‡Æï‡Ææ‡Æ≤‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡ÆÆ‡Æ¥‡Øà ‡Æ™‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡Æµ‡Øá‡Æ≥‡Ææ‡Æ£‡Øç‡ÆÆ‡Øà, ‡ÆÖ‡Æ±‡ØÅ‡Æµ‡Æü‡Øà ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡ÆÆ‡Æ±‡Øç‡Æ± ‡ÆÆ‡Ææ‡Æ§‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æì‡Æü‡ØÅ‡ÆÆ‡Øç. ‡Æö‡Æø‡Æ§‡Øç‡Æ§‡Æø‡Æ∞‡Øà ‡Æµ‡ØÜ‡ÆØ‡Æø‡Æ≤‡Æø‡Æ≤‡Øç ‡Æµ‡Øá‡Æ≤‡Øà ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ ‡ÆÆ‡ØÅ‡Æü‡Æø‡ÆØ‡Ææ ‡Æï‡Ææ‡Æ∞‡Æ£‡Æ§‡Øç‡Æ§‡Ææ‡Æ≤‡Øç ‡ÆÖ‡Æ§‡ØÅ ‡ÆÆ‡ØÅ‡Æ¥‡ØÅ‡Æ§‡ØÅ‡ÆÆ‡Øç ‡Æï‡Øä‡Æ£‡Øç‡Æü‡Ææ‡Æü‡Øç‡Æü‡Æô‡Øç‡Æï‡Æ≥‡Øç, ‡Æ™‡Æ£‡Øç‡Æü‡Æø‡Æï‡Øà‡Æï‡Æ≥‡Øç ‡Æé‡Æ©‡Øç‡Æ±‡ØÅ ‡Æï‡Æ¥‡Æø(‡Æ≥‡Æø)‡Æ§‡Øç‡Æ§‡Æ©‡Æ∞‡Øç ‡Æ®‡ÆÆ‡Øç ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Øã‡Æ∞‡Øç.\u003c/p\u003e\n\u003cp\u003e‡ÆÖ‡Æ§‡ØÅ 15 ‡Æ®‡Ææ‡Æ≥‡Øç ‡Æï‡Øä‡Æ£‡Øç‡Æü‡Ææ‡Æü‡Øç‡Æü‡ÆÆ‡Øç\u0026hellip; ‡Æ™‡ØÇ‡Æö‡Øç‡Æö‡Øä‡Æ∞‡Æø‡Æ§‡Æ≤‡Øç ‡Æ§‡Øä‡Æü‡Æô‡Øç‡Æï‡Æø ‡ÆÆ‡ØÅ‡Æ≥‡Øà‡Æ™‡Øç‡Æ™‡Ææ‡Æ∞‡Æø ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æ®‡Ææ‡Æ≥‡Øç ‡Æµ‡Æ∞‡Øà ‡Æ™‡Ææ‡Æü‡ØÅ ‡ÆÖ‡ÆÆ‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Æ≥‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡ÆÆ‡Øç. ‡Æö‡ØÜ‡Æ©‡Øç‡Æ©‡Øà ‡Æ™‡Øã‡Æ©‡Øç‡Æ± ‡Æ™‡ØÜ‡Æ∞‡ØÅ‡Æ®‡Æï‡Æ∞‡Æô‡Øç‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æµ‡Æö‡Æø‡Æ™‡Øç‡Æ™‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡ÆÜ‡Æ£‡Øç‡Æü‡ØÅ‡Æ§‡Øã‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æä‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡Æö‡Øç ‡Æö‡ØÜ‡Æ≤‡Øç‡Æµ‡Æ§‡Øá ‡Æá‡Æ§‡ØÅ‡Æ™‡Øã‡Æ©‡Øç‡Æ± ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æµ‡Æø‡Æ¥‡Ææ‡Æï‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡Æ§‡Øç‡Æ§‡Ææ‡Æ©‡Øç.\u003c/p\u003e\n\u003cp\u003e‡Æö‡Æø‡Æ§‡Øç‡Æ§‡Æø‡Æ∞‡Øà ‡ÆÆ‡Ææ‡Æ§ ‡Æû‡Ææ‡ÆØ‡Æø‡Æ±‡ØÅ‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æ™‡ØÇ‡Æö‡Øç‡Æö‡Øä‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡ØÅ, ‡ÆÖ‡Æü‡ØÅ‡Æ§‡Øç‡Æ§ ‡Æé‡Æü‡Øç‡Æü‡Ææ‡ÆÆ‡Øç ‡Æ®‡Ææ‡Æ≥‡Øç ‡Æï‡Ææ‡Æ™‡Øç‡Æ™‡ØÅ ‡Æï‡Æü‡Øç‡Æü‡Æø, ‡Æ®‡Æü‡ØÅ‡Æµ‡Æø‡Æ≤‡Øç ‡Æâ‡Æ≥‡Øç‡Æ≥ ‡Æí‡Æ∞‡ØÅ ‡Æµ‡Ææ‡Æ∞‡ÆÆ‡Øç ‡Æí‡Æµ‡Øç‡Æµ‡Øä‡Æ∞‡ØÅ ‡Æ§‡ØÜ‡Æ∞‡ØÅ ‡ÆÆ‡Æ£‡Øç‡Æü‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æ®‡Æü‡Æ§‡Øç‡Æ§‡Æø ‡Æï‡ØÇ‡Æ¥‡Øç ‡Æä‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆÆ‡ØÅ‡Æ±‡Øà ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ, ‡ÆÖ‡Æü‡ØÅ‡Æ§‡Øç‡Æ§ ‡Æû‡Ææ‡ÆØ‡Æø‡Æ±‡ØÅ ‡ÆÖ‡Æ©‡Øç‡Æ±‡ØÅ ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æµ‡Æø‡Æ¥‡Ææ ‡Æ®‡Æü‡Øà‡Æ™‡ØÜ‡Æ±‡ØÅ‡ÆÆ‡Øç.\u003c/p\u003e","title":"‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æä‡Æ∞‡Øç‡Æ§‡Øç ‡Æ§‡Æø‡Æ∞‡ØÅ‡Æµ‡Æø‡Æ¥‡Ææ"},{"content":"Not I, not any one else can travel that road for you,You must travel it for yourself.It is not far, it is within reach,Perhaps you have been on it since you were born and did not know,Perhaps it is everywhere on water and on land - Walt Whitman.\nIt was a Friday evening on the end of summer, I called my wife from office and puzzled her by saying we have to prepare some eatables for our tomorrow\u0026rsquo;s picnic. Both of us were damn bored in weekends as we usually spend watching some old movies from our collection. Hence, She was also desperate for some out of town trip. We haven\u0026rsquo;t planned anything before. As soon as I returned home, we discussed about the places in and around Bangalore and outskirts for the picnic. Finally we landed in Lepakshi, the abandoned artistic treasure of Vijayanagar period. It is 130 kilometres away from our location in Bangalore and situated in Anantapur district of Andhra Pradesh. We were unsure about the hotels or restaurants near the place. So, as per our tamil tradition, we prepared kattu soru, and curd rice which are taken by travelers from ancient timesüòú. Along with that we squeezed some lemons to fill the bottles with lemonade.\nWe set the wheels on road by 8.30 and reached the dusty village by 12. We traveled past Hebbal lake, traced NH 44 that passes through Yelahanka, Devanahalli, Chikballapur. Then we turned into the Lepakshi road, that doesn\u0026rsquo;t seem to be way to such historical significant place, but an ordinary village road.\nThe first spot we reached is the massive granite Nandhi of about 20 feet height and 30 feet width surrounded by grassy area and a pond. This is said to be the largest Nandhi in India and is a monolithic statue i.e, made out of a single boulder. There were so many tourists in that day posing in front of the bull. We were roasted by the sun during the journey, but it turned cloudy then which was best for photography. And one such pic is this\u0026hellip;\nThe Nandhi was finely chiseled and the jewelry like bells, chains etc carved in stone added to the grandeur and made it majestic. Even the finest notes like wrinkles in the face of the bull are visible. It is one of the great evidences of Vijayanagar architecture.\nThere is a parking space where you can lodge only a maximum of 5 to 10 Sedan vehicles, in front of the statue and a restaurant of Andhra Pradesh tourism board nearby this spot which is well maintained.\nNext place is the main attraction of Lepakshi village, the Veerabhadra temple of sixteenth century ( 1585 AD ) said to be built by Veeranna and Virupanna who were under the reign of king Atchutha Raya. The temple rests on the hillock called Kurmasaila ( tortoise hill in Telugu ). This is one of the Divyakshetras or important pilgrimage site in Shaivism.\nWe have to move 200 metres upwards in the same road near Nandhi and turn left to reach the temple. The spot is filled with some shops that sell Pooja articles and toys, also many small vendors to sides of parking lot before we reach the steps to Northern entrance. One can see many monkeys around the temple premises that may threaten the visitors with eatables. But what threatens you more than that is the stories and folklores coming along with the air.\nFrom the legends, it is said that Virupanna was accused of building the temple from the state treasure when Atchutha Raya was out of town, so the king ordered soldiers to dig out the eyes of Virupanna. But actually Virupanna spent his own treasure. To prove that, he threw the eyeballs in the wall which made the dark red lines that are even visible today. Some people say that the ghost of the noble chief still roam around the temple.\nThe history for name Lepakshi date back to Ramayana period that when Ravana kidnapped Sita he slayed the wings of Jadayu, which fell in this hill. Tracing the path lord Ram came to this place where he met the wounded bird and said le-pakshi !!! (rise oh bird) that turned out to Lepakshi.\nAs per Aahama Shastra, a Ganesha carving in the stone wall welcomes you in the East entrance. Then comes the artha Mandapam where we come across so many sculptures like avatars of Shiva, divine musicians, dancers, gods and goddesses.\nIn the ceiling u can see the gorgeous mural painting which are made with natural colours depicting stories from Mahabharata, Ramayana and other puranas.\nOut of about 70 pillars, one pillar stands in air whose base doesn\u0026rsquo;t rest on ground. It is left intentionally to prove the brilliance of the architects and the phenomenon is due to the eccentricity maintained in the construction. A British engineer tried to dislodge the pillar to unearth the mystery but he failed in the mission. This is the reason for slightest disposition of the pillar.\nThe main deities are Shiva, Vishnu and Veerabhadra. The Garba Griha consists of nearly life size Veerabhadra statue of fierce nature embossed in stone wall.\nNext is the kalyana Mandapam which is believed to be where Shiva married Parvati. This hall is incomplete without ceiling, that adds to the curiosity of the visitors. There\u0026rsquo;s also a biggest footprint on rock which is said to be Sita\u0026rsquo;s. A coiled multi hooded serpent which shades the Shiva linga (Naga linga) is a masterpiece.\nAlmost like every tourist spot in India this place too had lots of people who ask for money. But the sad note is that, if you ignore them they curse you badly and follow us like they ll snatch your purse. This will be more incase of foreigners. The governments should take necessary steps to ensure the safety of the visitors.\nThe splendid art and architecture of Vijayanagar period paused us and boosted our the Wanderlust to explore more of such historical significant places. We returned home wondering the contrast in the same spot, luxurious in medival age and abandoned nowadays. On the way, we marked the Devanahalli fort for our next trip that didn\u0026rsquo;t happen till now.\nLet\u0026rsquo;s meet in the next spot where our wheels stop rolling.\n","permalink":"http://localhost:1313/posts/2018-03-03-lepakshi-the-land-of-history-mystery/","summary":"\u003cp\u003eNot I, not any one else can travel that road for you,You must travel it for yourself.It is not far, it is within reach,Perhaps you have been on it since you were born and did not know,Perhaps it is everywhere on water and on land - Walt Whitman.\u003c/p\u003e\n\u003cp\u003eIt was a Friday evening on the end of summer, I called my wife from office and puzzled her by saying we have to prepare some eatables for our tomorrow\u0026rsquo;s picnic. Both of us were damn bored in weekends as we usually spend watching some old movies from our collection. Hence, She was also desperate for some out of town trip. We haven\u0026rsquo;t planned anything before. As soon as I returned home, we discussed about the places in and around Bangalore and outskirts for the picnic. Finally we landed in Lepakshi, the abandoned artistic treasure of Vijayanagar period. It is 130 kilometres away from our location in Bangalore and situated in Anantapur district of Andhra Pradesh. We were unsure about the hotels or restaurants near the place. So, as per our tamil tradition, we prepared kattu soru, and curd rice which are taken by travelers from ancient timesüòú. Along with that we squeezed some lemons to fill the bottles with lemonade.\u003c/p\u003e","title":"Lepakshi - The land of history \u0026 mystery"},{"content":"A little rain for few days filled the rivers to overflow. It was a day the water turns golden brown. Plants and debris everywhere, up to the knees rippling the flowing water. The time has come for us to rejoin the infinite stream of waters.\nIt was around 9 am on August 15, 2017, Me and my wife geared up, to begin the expedition to Hogenakkal. My Tiago is ready, petrol tank was jammed up the day before. My car\u0026rsquo;s final free service was pending and I had the idea of doing it after the trip. I wanted to start the journey at 7 am finishing breakfast at home, but as usual, it was delayed and we decided to take breakfast on the way to Hogenakkal.\nThe trip was already delayed and I don\u0026rsquo;t want to spend more time on the city traffic, so I took the Electronic city toll(70 rupees) and passed near Hosur flyover around 11 am. There are 2 routes to Hogenakkal\nBangalore \u0026ndash;\u0026gt; Hosur \u0026ndash;\u0026gt; Anchetty \u0026ndash;\u0026gt; Hogenakkal Bangalore \u0026ndash;\u0026gt; Hosur \u0026ndash;\u0026gt; Krishnagiri \u0026ndash;\u0026gt; Dharmapuri \u0026ndash;\u0026gt; Hogenakkal I wanted to attempt both the routes, sso I took Hosur-Anchetty for forward and chose the Krishnagiri-Dharmapuri for return. If you use Krishnagiri-Dharmapuri route then we have to cross 2 more tolls, one at Attebelle and the other at Krishnagiri. Which cost you 65 and 40 rupees respectively for the car.\nForward journey map Return journey map The Hosur-Anchetty is a scenic hilly road that passes through the forest. The distance is 20KM less than the expressway but it was more time consuming for us this time due to an unexpected reason that I explained later. The travel time was 30 minutes more on this road than the Krishnagiri-Dharmapuri road. There were not much hotels once you pass Hosur, so it is better to pack the food before.\nThe roads are very narrow for some distance and you can catch the sight of the villagers and vehicles jump right on the road which is usual in rural India. I have to keep honking repeatedly and reduced the speed to 40-60km/hr.\nSince it was the Independence day for us and Vinayagar Chaturthi impending next few days, there were many people, especially the school going kids tried to stop us and collect money.\nI didn\u0026rsquo;t stop the car at any place. It would be a bit scary when you aren\u0026rsquo;t a local or don\u0026rsquo;t speak their language.\nAt one place even the road was blocked with thorn bushes and spines.And the kids were standing nearby with papers and a pen ready to collect money.\nAt one place even the road was blocked with thorn bushes and spines.And the kids were standing nearer with papers and a pen ready to collect money from us.\nNear Anchetty, the usual road was blocked and many other people traveling to Hogenakkal in car and bike were returning to swing in a different route.\nThen we came to know that the day before our visit, one of the dams across The River Cauvery(Anchetty/KRP Dam) was broken due to heavy rainfall in Karnataka. It was raining continuously in Karnataka for the past week before our trip. So we took another road which passes through the forest. The road was in worst condition and in a remote location, Even my google navigation stopped working due to lack of signal and also the route was not fed to Google maps. But luckily there were many other travelers riding the same way. So I started to follow them. This is not even a tar road for some kilometer. The terrain was hilly, the road is dusty and also muddy due to the non-stop rain. Two cars cannot pass in the lane, hence we stopped at some wider place and waited for the car in other direction to pass. It was very thrilling and exciting.\nWe noticed some nice viewpoints on the road and stopped to take some pictures. There were many other people in cars and bike stopping in this spot and taking pictures.\nAt around 12:30 PM we reached the banks of River Cauvery.You may suddenly start to notice people all over the area, Some of them having food and some taking bath on the riverside. You can also notice so many small food stall with fishes ready to be cooked.\nThere are 2 check posts at the Karnataka-Tamilnadu border and each collected a fee of 20 and 50 rupees for the 4 wheeler\nI parked the car near the roadside and didn\u0026rsquo;t pay anything extra for parking other than the ‚Çπ.20 entry fees for the forest guards. There were so many locals approaching us for the Coracle ride. We were very hungry and took the lunch in the street side hut near the river.\nThey offered rice with few curries and fish fry. The taste was ok. While having food, few locals approached us and we fixed the ride ‚Çπ.850 for just 2 us with the boatman. The price is a bit costlier, but we don\u0026rsquo;t want to waste much time negotiating it was already late and we want to reach home Bangalore by 6 PM.\nAfter having the lunch we followed the boatman to the river in the car. There is a place near the river where a lot of space available for parking. It\u0026rsquo;s like a small ground. We parked the car and approached the river by walking. It was just a few 100 meters away from the parking space.\nWe were nervous about getting into the coracle in the beginning. There was no life jacket provided since we took the private and not the one offered by the government. But on seeing other people already on the coracle, we chose to go.\nOn seeing the river I remember the Tamil movie Muthal Mariyadhai.This is one of the famous shooting spots and many other movies were shot here. This place was shown in some songs in Manirathnam\u0026rsquo;s Ravan movie also.\nUsually, this side of the river will have very low water level and coracle ride is not possible. Due to heavy rain, we are lucky to see water in the river which is usually dry.\nThe coracle rider who took us spoke many languages. His name is Murthy.\nMurthy told us that it has been more than 5 years to date, he saw water like that and the people were very happy that day to catch the glimpses of such scenery. He started narrating us about his childhood memories of seeing waters 30 feet higher than the present water level, the Cauvery river crocodiles, fishing in the river etc,. He said, their main occupation is fishing in the river and boat riding is part-time during the season times.\nBirds singing the tunes without words, quenching their thirst and lazing the air with golden feathers.\nThe river was not so deep and you will see rocks uphill the river in many places.\nYou can spot many birds sitting on those rocks and it was a great opportunity for me to capture some nice shots in my DSLR.\nWe slowly crossed the river in Coracle hearing the stories about this place. After an hour we reached another side of the river. The river actually acts as the border between Karnataka and Tamilnadu states.\nMurthy told us, the forest was also the hiding place of Veerappan and there is still currency buried underground which remains a mystery till date. But the sad part is that it could not be used because of demonetization.\nAfter crossing the river we got down and started to walk for few meters to reach a small bridge. On the way, we can see many people having oil massages, cooking fish fries and having food.\nThere was a person who collected ‚Çπ.20 for falls entry fee and I am not sure whether it is legal.\nThere is a viewpoint in the bridge. It was a mesmerizing view seeing the falls from there. The water was coffee brown in color.\nThe water fell down ferociously and It\u0026rsquo;s incredible to look at, but be wary of getting too close. There have been plenty of death reported in the falls.\nAfter enjoying the view for an hour, we froze some scenes through the camera and returned back to the place where he dropped us. Murthy was patiently waiting there and picked us.\nWe started to cross the river again and the climate was very hot and sultry. Murthy told us it was probably going to rain in the evening. We suddenly noticed a centipede in the Coracle(Parisil) and we were much frightened. Murthy suddenly crushed it with his pedal. He told it is very normal to see centipede in Coracle.\nAfter reaching the other side we tipped him ‚Çπ.100 and started back to Bangalore at around 5 PM. This time we took the Dharmapuri-Krishnagiri highway. After crossing Dharmapuri it started heavy raining and we stopped at Krishnagiri Saravana Bhavan for a short break and had the dinner.\nAfter 30 minutes break, we started. The traffic at Krishnagiri toll was very high and it started raining heavily.\nHere is the small compilation of our Coracle(Parisil) ride.\nWe finally reached our home at Bangalore around 9 PM. It was a memorable trip for both of us. Hogenakkal is simply Flawless and definitely a photographer\u0026rsquo;s destination.\nFull resolution images at Flickr\n","permalink":"http://localhost:1313/posts/2018-02-23-car_trip_to_hogenakkal/","summary":"\u003cp\u003eA little rain for few days filled the rivers to overflow. It was a day the water turns golden brown. Plants and debris everywhere, up to the knees rippling the flowing water. The time has come for us to rejoin the infinite stream of waters.\u003c/p\u003e\n\u003cp\u003eIt was around 9 am on August 15, 2017, Me and my wife geared up, to begin the expedition to Hogenakkal. My Tiago is ready, petrol tank was jammed up the day before. My car\u0026rsquo;s final free service was pending and I had the idea of doing it after the trip. I wanted to start the journey at 7 am finishing breakfast at home, but as usual, it was delayed and we decided to take breakfast on the way to Hogenakkal.\u003c/p\u003e","title":"Coracle ride in river Cauvery - an adventurous car trip to Hogenakkal"},{"content":"I worked on a freelance project a year ago which gives me experience in Free IPA server. Here I am sharing some portion of the setup used.It would be useful for beginners trying to setup Free IPA server.\nI have 2 sections here. One is about FreeIPA on the normal server and the other is FreeIPA in Docker container.\nIf you have any questions \u0026amp; comments, feel free to add in the Disqus comment section below\nFreeIPA stands for Free Identity, Policy, Audit. It is an open source alternative to AD that combines LDAP, Kerberos, CA services and management tools, and ships with its own schemas. It is a solution for managing users, groups, hosts, services, DNS, NTP etc.\nHostname used: IPA server name: freeipa.vikki.in\nIPA client name: client01.vikki.in (should be under the same domain e.g.: xxx.vikki.in)\nDocker host machine: fedora25.vikki.in\nFreeIPA server Installation and Configuration Step 1: Set fully qualified hostname\n{% highlight console %}\n#vim /etc/hostname freeipa.vikki.in\n{% endhighlight %}\nStep 2: Set local resolver for the hostname\n{% highlight console %}\n#vim /etc/hosts 10.0.0.1 freeipa.vikki.in freeipa\n{% endhighlight %}\nStep 3: Disable selinux(optional)\n{% highlight console %}\n#vim /etc/sysconfig/selinux SELINUX=disabled\n{% endhighlight %}\nStep 4: Reboot server\n{% highlight console %}\nreboot {% endhighlight %}\nStep 5: Verify hostname and selinux\n{% highlight console %}\n#hostname freeipa.vikki.in #getenforce Disabled\n{% endhighlight %}\nStep 6: Install ipa server\n{% highlight console %}\nyum install -y ipa-server bind bind-dyndb-ldap ipa-server-dns {% endhighlight %}\nStep 7: Configure FreeIPA\nIf you have a separate DNS server for managing the domain remove the option(\u0026ndash;setup-dns)\n{% highlight console %}\nipa-server-install \u0026ndash;setup-dns Existing BIND configuration detected, overwrite? [no]: yes Enter the fully qualified domain name of the computer on which you\u0026rsquo;re setting up server software. Using the form . Example: master.example.com. Server host name [freeipa.vikki.in]: freeipa.vikki.in Please confirm the domain name [vikki.in]: vikki.in Please provide a realm name [VIKKI.IN]: VIKKI.IN Directory Manager password: Password (confirm): IPA admin password: Password (confirm): Do you want to configure DNS forwarders? [yes]: yes Enter the IP address of DNS forwarder to use, or press Enter to finish. Enter IP address for a DNS forwarder: 8.8.8.8 DNS forwarder 8.8.8.8 added Enter IP address for a DNS forwarder: 8.8.4.4 DNS forwarder 8.8.4.4 added Enter IP address for a DNS forwarder: Do you want to configure the reverse zone? [yes]: yes Please specify the reverse zone name [0.0.10.in-addr.arpa.]: 0.0.10.in-addr.arpa Using reverse zone 0.0.10.in-addr.arpa. The IPA Master Server will be configured with: Hostname: freeipa.vikki.in IP address: 10.0.0.1 Domain name: vikki.in Realm name: VIKKI.IN BIND DNS server will be configured to serve IPA domain with: Forwarders: 8.8.8.8, 8.8.4.4 Reverse zone: 0.0.10.in-addr.arpa. Continue to configure the system with these values? [no]: yes The following operations may take some minutes to complete. Please wait until the prompt is returned. Configuring NTP daemon (ntpd) [1/4]: stopping ntpd [2/4]: writing configuration [3/4]: configuring ntpd to start on boot [4/4]: starting ntpd Done configuring NTP daemon (ntpd). Configuring directory server (dirsrv). Estimated time: 1 minute [1/42]: creating directory server user [2/42]: creating directory server instance [3/42]: adding default schema [4/42]: enabling memberof plugin [5/42]: enabling winsync plugin [6/42]: configuring replication version plugin [7/42]: enabling IPA enrollment plugin [8/42]: enabling ldapi [9/42]: configuring uniqueness plugin [10/42]: configuring uuid plugin [11/42]: configuring modrdn plugin [12/42]: configuring DNS plugin [13/42]: enabling entryUSN plugin [14/42]: configuring lockout plugin [15/42]: creating indices [16/42]: enabling referential integrity plugin [17/42]: configuring certmap.conf [18/42]: configure autobind for root [19/42]: configure new location for managed entries [20/42]: configure dirsrv ccache [21/42]: enable SASL mapping fallback [22/42]: restarting directory server [23/42]: adding default layout [24/42]: adding delegation layout [25/42]: creating container for managed entries [26/42]: configuring user private groups [27/42]: configuring netgroups from hostgroups [28/42]: creating default Sudo bind user [29/42]: creating default Auto Member layout [30/42]: adding range check plugin [31/42]: creating default HBAC rule allow_all [32/42]: adding entries for topology management [33/42]: initializing group membership [34/42]: adding master entry [35/42]: initializing domain level [36/42]: configuring Posix uid/gid generation [37/42]: adding replication acis [38/42]: enabling compatibility plugin [39/42]: activating sidgen plugin [40/42]: activating extdom plugin [41/42]: tuning directory server [42/42]: configuring directory to start on boot Done configuring directory server (dirsrv). Configuring certificate server (pki-tomcatd). Estimated time: 3 minutes 30 seconds [1/27]: creating certificate server user [2/27]: configuring certificate server instance [3/27]: stopping certificate server instance to update CS.cfg [4/27]: backing up CS.cfg [5/27]: disabling nonces [6/27]: set up CRL publishing [7/27]: enable PKIX certificate path discovery and validation [8/27]: starting certificate server instance [9/27]: creating RA agent certificate database [10/27]: importing CA chain to RA certificate database [11/27]: fixing RA database permissions [12/27]: setting up signing cert profile [13/27]: setting audit signing renewal to 2 years [14/27]: restarting certificate server [15/27]: requesting RA certificate from CA [16/27]: issuing RA agent certificate [17/27]: adding RA agent as a trusted user [18/27]: authorizing RA to modify profiles [19/27]: configure certmonger for renewals [20/27]: configure certificate renewals [21/27]: configure RA certificate renewal [22/27]: configure Server-Cert certificate renewal [23/27]: Configure HTTP to proxy connections [24/27]: restarting certificate server [25/27]: migrating certificate profiles to LDAP [26/27]: importing IPA certificate profiles [27/27]: adding default CA ACL Done configuring certificate server (pki-tomcatd). Configuring directory server (dirsrv). Estimated time: 10 seconds [1/3]: configuring ssl for ds instance [2/3]: restarting directory server [3/3]: adding CA certificate entry Done configuring directory server (dirsrv). Configuring Kerberos KDC (krb5kdc). Estimated time: 30 seconds [1/10]: adding sasl mappings to the directory [2/10]: adding kerberos container to the directory [3/10]: configuring KDC [4/10]: initialize kerberos container [5/10]: adding default ACIs [6/10]: creating a keytab for the directory [7/10]: creating a keytab for the machine [8/10]: adding the password extension to the directory [9/10]: starting the KDC [10/10]: configuring KDC to start on boot Done configuring Kerberos KDC (krb5kdc). Configuring kadmin [1/2]: starting kadmin [2/2]: configuring kadmin to start on boot Done configuring kadmin. Configuring ipa_memcached [1/2]: starting ipa_memcached [2/2]: configuring ipa_memcached to start on boot Done configuring ipa_memcached. Configuring ipa-otpd [1/2]: starting ipa-otpd [2/2]: configuring ipa-otpd to start on boot Done configuring ipa-otpd. Configuring the web interface (httpd). Estimated time: 1 minute [1/19]: setting mod_nss port to 443 [2/19]: setting mod_nss protocol list to TLSv1.0 - TLSv1.2 [3/19]: setting mod_nss password file [4/19]: enabling mod_nss renegotiate [5/19]: adding URL rewriting rules [6/19]: configuring httpd [7/19]: configure certmonger for renewals [8/19]: setting up ssl [9/19]: importing CA certificates from LDAP [10/19]: setting up browser autoconfig [11/19]: publish CA cert [12/19]: creating a keytab for httpd [13/19]: clean up any existing httpd ccache [14/19]: configuring SELinux for httpd [15/19]: create KDC proxy user [16/19]: create KDC proxy config [17/19]: enable KDC proxy [18/19]: restarting httpd [19/19]: configuring httpd to start on boot Done configuring the web interface (httpd). Applying LDAP updates Upgrading IPA: [1/9]: stopping directory server [2/9]: saving configuration [3/9]: disabling listeners [4/9]: enabling DS global lock [5/9]: starting directory server [6/9]: upgrading server [7/9]: stopping directory server [8/9]: restoring configuration [9/9]: starting directory server Done. Restarting the directory server Restarting the KDC Configuring DNS (named) [1/12]: generating rndc key file WARNING: Your system is running out of entropy, you may experience long delays [2/12]: adding DNS container [3/12]: setting up our zone [4/12]: setting up reverse zone [5/12]: setting up our own record [6/12]: setting up records for other masters [7/12]: adding NS record to the zones [8/12]: setting up CA record [9/12]: setting up kerberos principal [10/12]: setting up named.conf [11/12]: configuring named to start on boot [12/12]: changing resolv.conf to point to ourselves Done configuring DNS (named). Configuring DNS key synchronization service (ipa-dnskeysyncd) [1/7]: checking status [2/7]: setting up bind-dyndb-ldap working directory [3/7]: setting up kerberos principal [4/7]: setting up SoftHSM [5/7]: adding DNSSEC containers [6/7]: creating replica keys [7/7]: configuring ipa-dnskeysyncd to start on boot Done configuring DNS key synchronization service (ipa-dnskeysyncd). Restarting ipa-dnskeysyncd Restarting named Restarting the web server Setup complete Next steps:\nYou must make sure these network ports are open: TCP Ports: 80, 443: HTTP/HTTPS 389, 636: LDAP/LDAPS 88, 464: kerberos 53: bind UDP Ports: 88, 464: kerberos 53: bind 123: ntp You can now obtain a kerberos ticket using the command: \u0026lsquo;kinit admin\u0026rsquo; This ticket will allow you to use the IPA tools (e.g., ipa user-add) and the web user interface. Be sure to back up the CA certificate stored in /root/cacert.p12 This file is required to create replicas. The password for this file is the Directory Manager password {% endhighlight %}\nStep 8: Configure firewall\n{% highlight console %}\n#firewall-cmd \u0026ndash;permanent \u0026ndash;add-service=ntp #firewall-cmd \u0026ndash;permanent \u0026ndash;add-service=http #firewall-cmd \u0026ndash;permanent \u0026ndash;add-service=https #firewall-cmd \u0026ndash;permanent \u0026ndash;add-service=ldap #firewall-cmd \u0026ndash;permanent \u0026ndash;add-service=ldaps #firewall-cmd \u0026ndash;permanent \u0026ndash;add-service=kerberos #firewall-cmd \u0026ndash;permanent \u0026ndash;add-service=kpasswd\n{% endhighlight %}\nEither add all the above rules in firewall or simply flush all rules from iptables(not recommended in production)\n{% highlight console %}\niptables -F {% endhighlight %}\nStep 9: Get Kerberos tickets\nNow the FreeIPA server is ready , we should generate the kerberos ticket with admin privilege. This kerberos ticket is needed to run any IPA commands in server with full privileges and to connect web UI.\nKerberos provides authentication services for entire FreeIPA components.\n{% highlight console %}\n#kinit admin Password for admin@VIKKI.IN:# IPA admin password\n{% endhighlight %}\nWe can also view the current Kerberos ticket with expiry details using the klist command. The default expiry period is 24hours.\n{% highlight console %}\nklist Ticket cache: KEYRING:persistent:0:0 Default principal: admin@VIKKI.IN Valid starting Expires Service principal 08/09/2016 14:45:53 11/09/2016 14:45:50 krbtgt/ADMIN@VIKKI.IN\n{% endhighlight %}\nstep 10: Enabled home directory and default shell\nWe have to enable the home directory creation and default login shell for the new user login. Run the below command to set it.\n{% highlight console %}\nipa config-mod \u0026ndash;defaultshell=/bin/bash authconfig \u0026ndash;enablemkhomedir \u0026ndash;update {% endhighlight %}\nStep 11: Login to freeipa web\nLogin to the freeIPA server from the browser and create some users.\nStep 12: Add dns entry for the IPA client\nNow the user has been created , we can configure the dns record for the new clients to be added under domain vikki.in\n{% highlight console %}\nipa dnsrecord-add vikki.in client01 \u0026ndash;a-rec 10.0.0.2 IPA Client\n{% endhighlight %}\nFreeIPA client Installation and Configuration Step 1: Freeipa client preparation\nSet hostname ‚Äúclient01.vikki.in‚Äù\nDisable ‚Äúselinux‚Äù\nDisable ‚ÄúFirewall‚Äù\nFor all the above follow the same procedure followed in FreeIPA server preparation\nStep 2: Set DNS server IP to FreeIPA server\n{% highlight console %}\nvim /etc/resolv.conf nameserver 10.0.0.1 #dig client01.vikki.in\n{% endhighlight %}\nStep 3: Install ipa client\n{% highlight console %}\nyum -y install ipa-client Discovery was successful! Hostname: client01.vikki.in Realm: VIKKI.IN DNS Domain: vikki.in IPA Server: freeipa.vikki.in BaseDN: dc=vikki,dc=in\nconfirm settings and proceed with \u0026ldquo;yes\u0026rdquo; Continue to configure the system with these values? [no]: yes\nanswer with admin User authorized to enroll computers: admin Synchronizing time with KDC\u0026hellip; Unable to sync time with IPA NTP server, assuming the time is in sync. Please check that 123 UDP port is opened. Password for admin@vikki.in: Successfully retrieved CA cert Subject: CN=Certificate Authority,O=vikki.in Issuer: CN=Certificate Authority,O=vikki.in Valid From: Fri Mar 20 01:42:15 2015 UTC Valid Until: Tue Mar 20 01:42:15 2035 UTC Enrolled in IPA realm VIKKI.IN \u0026hellip;.. \u0026hellip;.. Configured /etc/ssh/ssh_config Configured /etc/ssh/sshd_config Client configuration complete.\n{% endhighlight %}\nStep 4: Try to login using the user created from the FreeIPA server GUI\n{% highlight console %}\nssh vivek@client01.vikki.in Password: Password expired. Change your password now. Current Password: New password: Retype new password: Creating home directory for vivek. Last login: Fri Jan 19 18:44:31 2018 from localhost\n{% endhighlight %}{% highlight console %}\npwd /home/vivek\n{% endhighlight %}\nIf you have not enabled the home directory creation and login shell , you will get the below output.\n{% highlight console %}\nssh vivek@client01.vikki.in Password: Password expired. Change your password now. Current Password: New password: Retype new password: Could not chdir to home directory /home/vivek: No such file or directory -sh-4.2$\n{% endhighlight %}\nFreeIPA in Docker container FreeIPA is also available as a docker container. We can download the FreeIPA repository from github master.zip.\nHow to build FreeIPA docker container Download the zip file, extract it , go inside the directory and run the docker build.\nThis repository has \u0026ldquo;Dockerfile\u0026rdquo; which supports many operation system, use the one you required. Here i am installing the container in Fedora27 , so i passing the respective dockerfile using \u0026ldquo;-f\u0026rdquo; option.\n{% highlight console %}\n[root@fedora25 freeipa-container-master]# docker build -t freeipa-server -f Dockerfile.fedora-27 . Sending build context to Docker daemon 161.3 kB Step 1/45 : FROM registry.fedoraproject.org/fedora:27 Trying to pull repository registry.fedoraproject.org/fedora \u0026hellip; sha256:44be9569d32fee1846afd1d17dc681c2b8f1f2b367764194ade32d9bfdf1984f: Pulling from registry.fedoraproject.org/fedora 178a618c1fcb: Pull complete Digest: sha256:44be9569d32fee1846afd1d17dc681c2b8f1f2b367764194ade32d9bfdf1984f Status: Downloaded newer image for registry.fedoraproject.org/fedora:27 \u0026mdash;\u0026gt; 9881e4229c95 Step 2/45 : MAINTAINER Jan Pazdziora \u0026mdash;\u0026gt; Running in 4ccc52e61959 \u0026mdash;\u0026gt; e6203b4f193f Removing intermediate container 4ccc52e61959 Step 3/45 : RUN groupadd -g 288 kdcproxy ; useradd -u 288 -g 288 -c \u0026lsquo;IPA KDC Proxy User\u0026rsquo; -d \u0026lsquo;/var/lib/kdcproxy\u0026rsquo; -s \u0026lsquo;/sbin/nologin\u0026rsquo; kdcproxy \u0026mdash;\u0026gt; Running in 281bede2e1ca \u0026mdash;\u0026gt; 11f29258686a Removing intermediate container 281bede2e1ca Step 4/45 : RUN groupadd -g 289 ipaapi; useradd -u 289 -g 289 -c \u0026lsquo;IPA Framework User\u0026rsquo; -d / -s \u0026lsquo;/sbin/nologin\u0026rsquo; ipaapi \u0026mdash;\u0026gt; Running in acff04c732af useradd: warning: the home directory already exists. Not copying any file from skel directory into it. \u0026mdash;\u0026gt; 9100a9565039 Removing intermediate container acff04c732af Step 5/45 : RUN mkdir -p /run/lock \u0026amp;\u0026amp; dnf upgrade -y \u0026amp;\u0026amp; dnf install -y freeipa-server freeipa-server-dns freeipa-server-trust-ad initscripts \u0026amp;\u0026amp; dnf clean all \u0026mdash;\u0026gt; Running in b33a6dd071aa Fedora 27 - x86_64 - Updates 3.2 MB/s | 16 MB 00:05 Fedora 27 - x86_64 3.0 MB/s | 58 MB 00:19 Last metadata expiration check: 0:00:06 ago on Sat Jan 20 06:40:05 2018. Dependencies resolved. Package Arch Version Repository Size Upgrading: gdbm x86_64 1:1.13-6.fc27 updates 158 k\nTransaction Summary Upgrade 1 Package\nTotal download size: 158 k Downloading Packages: gdbm-1.13-6.fc27.x86_64.rpm 198 kB/s | 158 kB 00:00 Total 72 kB/s | 158 kB 00:02 Running transaction check Transaction check succeeded. Running transaction test Transaction test succeeded. Running transaction Preparing : 1/1 Upgrading : gdbm-1:1.13-6.fc27.x86_64 1/2 Running scriptlet: gdbm-1:1.13-6.fc27.x86_64 1/2 Cleanup : gdbm-1.14-1.fc27.x86_64 2/2 Running scriptlet: gdbm-1.14-1.fc27.x86_64 2/2 Verifying : gdbm-1:1.13-6.fc27.x86_64 1/2 Verifying : gdbm-1.14-1.fc27.x86_64 2/2\nUpgraded: gdbm.x86_64 1:1.13-6.fc27\nComplete! Last metadata expiration check: 0:00:38 ago on Sat Jan 20 06:40:05 2018. Dependencies resolved. Package Arch Version Repository Size Installing: freeipa-server x86_64 4.6.1-3.fc27 fedora 398 k freeipa-server-dns noarch 4.6.1-3.fc27 fedora 60 k freeipa-server-trust-ad x86_64 4.6.1-3.fc27 fedora 152 k initscripts x86_64 9.79-1.fc27 updates 394 k Installing dependencies: 389-ds-base x86_64 1.3.7.8-1.fc27 updates 1.8 M 389-ds-base-libs x86_64 1.3.7.8-1.fc27 updates 744 k GeoIP x86_64 1.6.11-3.fc27 fedora 124 k GeoIP-GeoLite-data noarch 2018.01-1.fc27 updates 465 k timedatex x86_64 0.4-5.fc27 fedora 30 k\nTransaction Summary Install 416 Packages\nTotal download size: 184 M Installed size: 568 M Downloading Packages: (1/416): freeipa-server-dns-4.6.1-3.fc27.noarch 103 kB/s | 60 kB 00:00 (2/416): freeipa-server-trust-ad-4.6.1-3.fc27.x 208 kB/s | 152 kB 00:00 (3/416): cyrus-sasl-gssapi-2.1.26-34.fc27.x86_6 286 kB/s | 46 kB 00:00 (4/416): freeipa-server-4.6.1-3.fc27.x86_64.rpm 365 kB/s | 398 kB 00:01 (5/416): freeipa-client-4.6.1-3.fc27.x86_64.rpm 441 kB/s | 159 kB 00:00 (415/416): linux-atm-libs-2.5.1-19.fc27.x86_64. 154 kB/s | 40 kB 00:00 (416/416): libicu-57.1-9.fc27.x86_64.rpm 862 kB/s | 8.4 MB 00:09 Total 2.0 MB/s | 184 MB 01:30 Running transaction check Transaction check succeeded. Running transaction test Transaction test succeeded. Running transaction Running scriptlet: copy-jdk-configs-3.3-2.fc27.noarch 1/1 Running scriptlet: pki-base-10.5.1-1.fc27.noarch 1/1 Running scriptlet: java-1.8.0-openjdk-headless-1:1.8.0.151-1.b12.fc27.x 1/1 Preparing : 1/1 Installing : perl-Exporter-5.72-395.fc27.noarch 1/416 Installing : perl-libs-4:5.26.1-402.fc27.x86_64 2/416 Running scriptlet: perl-libs-4:5.26.1-402.fc27.x86_64 Installing : freetype-2.8-7.fc27.x86_64 215/416 Running scriptlet: opendnssec-1.4.14-1.fc27.x86_64 216/416 Installing : opendnssec-1.4.14-1.fc27.x86_64 216/416 Running scriptlet: opendnssec-1.4.14-1.fc27.x86_64 216/416 The token has been initialized and is reassigned to slot 1739892472 INFO: The XML in /etc/opendnssec/conf.xml is valid INFO: The XML in /etc/opendnssec/zonelist.xml is valid INFO: The XML in /etc/opendnssec/kasp.xml is valid WARNING: In policy default, Y used in duration field for Keys/KSK Lifetime (P1Y) in /etc/opendnssec/kasp.xml - this will be interpreted as 365 days WARNING: In policy lab, Y used in duration field for Keys/KSK Lifetime (P1Y) in /etc/opendnssec/kasp.xml - this will be interpreted as 365 days WARNING This will erase all data in the database; are you sure? [y/N] fixing permissions on file /var/opendnssec/kasp.db zonelist filename set to /etc/opendnssec/zonelist.xml. kasp filename set to /etc/opendnssec/kasp.xml. Repository SoftHSM found No Maximum Capacity set. RequireBackup NOT set; please make sure that you know the potential problems of using keys which are not recoverable Policy default found Info: converting P1Y to seconds; M interpreted as 31 days, Y interpreted as 365 days Policy lab found Info: converting P1Y to seconds; M interpreted as 31 days, Y interpreted as 365 days Installing : net-tools-2.0-0.45.20160912git.fc27.x86_64 416/416 Running scriptlet: libsss_sudo-1.16.0-5.fc27.x86_64 416/416Failed to connect to bus: No such file or directory\nVerifying : freeipa-server-4.6.1-3.fc27.x86_64 1/416 Verifying : freeipa-server-dns-4.6.1-3.fc27.noarch 2/416 Verifying : freeipa-server-trust-ad-4.6.1-3.fc27.x86_64 3/416 Verifying : cyrus-sasl-gssapi-2.1.26-34.fc27.x86_64 Verifying : lua-posix-33.3.1-7.fc27.x86_64 415/416 Verifying : perl-libnet-3.11-1.fc27.noarch 416/416\nInstalled: freeipa-server.x86_64 4.6.1-3.fc27 freeipa-server-dns.noarch 4.6.1-3.fc27 freeipa-server-trust-ad.x86_64 4.6.1-3.fc27\nxsom.noarch 0-17.20110809svn.fc27\nComplete! Non-fatal scriptlet failure in rpm package nfs-utils Non-fatal scriptlet failure in rpm package nfs-utils Non-fatal scriptlet failure in rpm package nfs-utils Non-fatal scriptlet failure in rpm package nfs-utils Non-fatal POSTIN scriptlet failure in rpm package freeipa-server-trust-ad Non-fatal POSTIN scriptlet failure in rpm package freeipa-server-trust-ad 18 files removed \u0026mdash;\u0026gt; 8e5982e4aba2 Removing intermediate container b33a6dd071aa Step 6/45 : RUN sed -i \u0026lsquo;/installutils.verify_fqdn(config.master_host_name, options.no_host_dns)/s/)/, local_hostname=False)/\u0026rsquo; /usr/lib/python2.7/site-packages/ipaserver/install/server/replicainstall.py \u0026amp;\u0026amp; python -m compileall /usr/lib/python2.7/site-packages/ipaserver/install/server/replicainstall.py \u0026mdash;\u0026gt; Running in 3f16baa3e82f Compiling /usr/lib/python2.7/site-packages/ipaserver/install/server/replicainstall.py \u0026hellip; \u0026mdash;\u0026gt; 907bee58561a Removing intermediate container 3f16baa3e82f Step 7/45 : RUN sed -i \u0026lsquo;/installutils.verify_fqdn(config.master_host_name, options.no_host_dns)/s/)/, local_hostname=False)/\u0026rsquo; /usr/lib/python3.6/site-packages/ipaserver/install/server/replicainstall.py \u0026amp;\u0026amp; python3 -m compileall /usr/lib/python3.6/site-packages/ipaserver/install/server/replicainstall.py \u0026mdash;\u0026gt; Running in c33cb158b26c Compiling \u0026lsquo;/usr/lib/python3.6/site-packages/ipaserver/install/server/replicainstall.py\u0026rsquo;\u0026hellip; \u0026mdash;\u0026gt; 0a6e9da4fd27 Removing intermediate container c33cb158b26c Step 8/45 : RUN [-L /etc/systemd/system/syslog.service] \u0026amp;\u0026amp; ! [-f /etc/systemd/system/syslog.service] \u0026amp;\u0026amp; rm -f /etc/systemd/system/syslog.service \u0026mdash;\u0026gt; Running in d7ef9f4d9b2a \u0026mdash;\u0026gt; 9980910fa2dc Removing intermediate container d7ef9f4d9b2a Step 9/45 : RUN find /etc/systemd/system/* \u0026lsquo;!\u0026rsquo; -name \u0026lsquo;.wants\u0026rsquo; | xargs rm -rvf \u0026mdash;\u0026gt; Running in 594660eda1bc removed \u0026lsquo;/etc/systemd/system/basic.target.wants/dnf-makecache.timer\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/console-getty.service\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/dbus-org.freedesktop.timedate1.service\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/default.target\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/dev-hugepages.mount\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/getty.target\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/getty.target.wants/getty@tty1.service\u0026rsquo; removed directory \u0026lsquo;/etc/systemd/system/httpd.d\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/local-fs.target.wants/fedora-readonly.service\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/multi-user.target.wants/remote-fs.target\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/multi-user.target.wants/auditd.service\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/multi-user.target.wants/nfs-client.target\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/multi-user.target.wants/sssd.service\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/remote-fs.target.wants/nfs-client.target\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/sockets.target.wants/sssd-secrets.socket\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/sys-fs-fuse-connections.mount\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/sysinit.target.wants/fedora-import-state.service\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/systemd-logind.service\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/systemd-remount-fs.service\u0026rsquo; removed \u0026lsquo;/etc/systemd/system/systemd-timedated.service\u0026rsquo; \u0026mdash;\u0026gt; 6273c3ce8454 Removing intermediate container 594660eda1bc Step 10/45 : RUN for i in basic.target network.service netconsole.service ; do rm -f /usr/lib/systemd/system/$i \u0026amp;\u0026amp; ln -s /dev/null /usr/lib/systemd/system/$i ; done \u0026mdash;\u0026gt; Running in 3805f0460b62 \u0026mdash;\u0026gt; a82df26e2086 Removing intermediate container 3805f0460b62 Step 11/45 : RUN rm -fv /usr/lib/systemd/system/sysinit.target.wants/ \u0026mdash;\u0026gt; Running in 2340c6e904de removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/cryptsetup.target\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/dev-hugepages.mount\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/dev-mqueue.mount\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/ldconfig.service\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/proc-sys-fs-binfmt_misc.automount\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/sys-fs-fuse-connections.mount\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/sys-kernel-config.mount\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/sys-kernel-debug.mount\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/systemd-ask-password-console.path\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/systemd-binfmt.service\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/systemd-firstboot.service\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/systemd-journal-catalog-update.service\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/systemd-journal-flush.service\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/systemd-journald.service\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/systemd-machine-id-commit.service\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/systemd-sysctl.service\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/systemd-sysusers.service\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/systemd-tmpfiles-setup.service\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/systemd-update-done.service\u0026rsquo; removed \u0026lsquo;/usr/lib/systemd/system/sysinit.target.wants/systemd-update-utmp.service\u0026rsquo; \u0026mdash;\u0026gt; e575744cbec4 Removing intermediate container 2340c6e904de Step 12/45 : RUN echo \u0026lsquo;disable \u0026rsquo; \u0026gt; /usr/lib/systemd/system-preset/10-container-disable.preset \u0026mdash;\u0026gt; Running in c2c372a783c6 \u0026mdash;\u0026gt; afbb3fa9de01 Removing intermediate container c2c372a783c6 Step 13/45 : RUN /sbin/ldconfig -X \u0026mdash;\u0026gt; Running in d4e981ba0dae \u0026mdash;\u0026gt; 18cde0edd362 Removing intermediate container d4e981ba0dae Step 14/45 : COPY init-data /usr/local/sbin/init \u0026mdash;\u0026gt; e3cf2eac153f Removing intermediate container b5a7b4f89ee8 Step 15/45 : COPY ipa-server-configure-first ipa-server-status-check exit-with-status ipa-volume-upgrade- /usr/sbin/ \u0026mdash;\u0026gt; c60a71108eda Removing intermediate container b38588e0eecc Step 16/45 : COPY install.sh uninstall.sh /bin/ \u0026mdash;\u0026gt; d208c2ad0dea Removing intermediate container 633df7d21612 Step 17/45 : RUN mv /bin/hostnamectl /bin/hostnamectl.orig \u0026mdash;\u0026gt; Running in d84b0e146325 \u0026mdash;\u0026gt; 41a46d7a76d0 Removing intermediate container d84b0e146325 Step 18/45 : RUN mv /usr/bin/domainname /usr/bin/domainname.orig \u0026mdash;\u0026gt; Running in 808c9796831e \u0026mdash;\u0026gt; bdd853c871b6 Removing intermediate container 808c9796831e Step 19/45 : ADD hostnamectl-wrapper /bin/hostnamectl \u0026mdash;\u0026gt; f34600ec54fd Removing intermediate container f6f3c216ffd4 Step 20/45 : ADD hostnamectl-wrapper /usr/bin/domainname \u0026mdash;\u0026gt; 49fb7d34f8af Removing intermediate container 3c3c1695eb37 Step 21/45 : RUN chmod -v +x /usr/local/sbin/init /usr/sbin/ipa-server-configure-first /usr/sbin/ipa-server-status-check /usr/sbin/exit-with-status /usr/sbin/ipa-volume-upgrade-* /bin/install.sh /bin/uninstall.sh /bin/hostnamectl /usr/bin/domainname \u0026mdash;\u0026gt; Running in c322618dc805 mode of \u0026lsquo;/usr/local/sbin/init\u0026rsquo; retained as 0755 (rwxr-xr-x) mode of \u0026lsquo;/usr/sbin/ipa-server-configure-first\u0026rsquo; retained as 0755 (rwxr-xr-x) mode of \u0026lsquo;/usr/sbin/ipa-server-status-check\u0026rsquo; retained as 0755 (rwxr-xr-x) mode of \u0026lsquo;/usr/sbin/exit-with-status\u0026rsquo; retained as 0755 (rwxr-xr-x) mode of \u0026lsquo;/usr/sbin/ipa-volume-upgrade-0.5-0.6\u0026rsquo; retained as 0755 (rwxr-xr-x) mode of \u0026lsquo;/usr/sbin/ipa-volume-upgrade-0.5-1.0\u0026rsquo; retained as 0755 (rwxr-xr-x) mode of \u0026lsquo;/usr/sbin/ipa-volume-upgrade-0.5-1.1\u0026rsquo; retained as 0755 (rwxr-xr-x) mode of \u0026lsquo;/usr/sbin/ipa-volume-upgrade-0.6-1.0\u0026rsquo; retained as 0755 (rwxr-xr-x) mode of \u0026lsquo;/usr/sbin/ipa-volume-upgrade-0.6-1.1\u0026rsquo; retained as 0755 (rwxr-xr-x) mode of \u0026lsquo;/usr/sbin/ipa-volume-upgrade-1.0-1.1\u0026rsquo; retained as 0755 (rwxr-xr-x) mode of \u0026lsquo;/bin/install.sh\u0026rsquo; retained as 0755 (rwxr-xr-x) mode of \u0026lsquo;/bin/uninstall.sh\u0026rsquo; retained as 0755 (rwxr-xr-x) mode of \u0026lsquo;/bin/hostnamectl\u0026rsquo; retained as 0755 (rwxr-xr-x) mode of \u0026lsquo;/usr/bin/domainname\u0026rsquo; retained as 0755 (rwxr-xr-x) \u0026mdash;\u0026gt; 0395279a17e5 Removing intermediate container c322618dc805 Step 22/45 : COPY container-ipa.target ipa-server-configure-first.service ipa-server-upgrade.service ipa-server-update-self-ip-address.service /usr/lib/systemd/system/ \u0026mdash;\u0026gt; b9a4d37d8af7 Removing intermediate container 8218605ec53b Step 23/45 : RUN rmdir -v /etc/systemd/system/multi-user.target.wants \u0026amp;\u0026amp; mkdir /etc/systemd/system/container-ipa.target.wants \u0026amp;\u0026amp; ln -s /etc/systemd/system/container-ipa.target.wants /etc/systemd/system/multi-user.target.wants \u0026mdash;\u0026gt; Running in d5554f6b4498 rmdir: removing directory, \u0026lsquo;/etc/systemd/system/multi-user.target.wants\u0026rsquo; \u0026mdash;\u0026gt; ca9aaf468af8 Removing intermediate container d5554f6b4498 Step 24/45 : RUN systemctl set-default container-ipa.target \u0026mdash;\u0026gt; Running in 2b40be183177 Created symlink /etc/systemd/system/default.target ‚Üí /usr/lib/systemd/system/container-ipa.target. \u0026mdash;\u0026gt; 3d9f7251ef3d Removing intermediate container 2b40be183177 Step 25/45 : RUN systemctl enable ipa-server-configure-first.service \u0026mdash;\u0026gt; Running in 6a7badce6e3e Created symlink /etc/systemd/system/container-ipa.target.wants/ipa-server-configure-first.service ‚Üí /usr/lib/systemd/system/ipa-server-configure-first.service. \u0026mdash;\u0026gt; 74059555138d Removing intermediate container 6a7badce6e3e Step 26/45 : COPY exit-via-chroot.conf /usr/lib/systemd/system/systemd-poweroff.service.d/ \u0026mdash;\u0026gt; 6f481582dd11 Removing intermediate container d704db80e37d Step 27/45 : COPY atomic-install-help /usr/share/ipa/ \u0026mdash;\u0026gt; 7f31c6a39572 Removing intermediate container 19f606e9e0bc Step 28/45 : COPY volume-data-list volume-data-mv-list volume-data-autoupdate /etc/ \u0026mdash;\u0026gt; b9c4aa387aa4 Removing intermediate container ca6ce5b07075 Step 29/45 : RUN set -e ; cd / ; mkdir /data-template ; cat /etc/volume-data-list | while read i ; do echo $i ; if [-e $i] ; then tar cf - .$i | ( cd /data-template \u0026amp;\u0026amp; tar xf - ) ; else mkdir -p /data-template$( dirname $i ) ; fi ; mkdir -p $( dirname $i ) ; if [\u0026quot;$i\u0026quot; == /var/log/] ; then mv /var/log /var/log-removed ; else rm -rf $i ; fi ; ln -sf /data${i%/} ${i%/} ; done \u0026mdash;\u0026gt; Running in 1e849a37ee72 /etc/certmonger/ /etc/dirsrv/ \u0026hellip;.. /var/named/data/ /var/named/dynamic/ /var/named/dyndb-ldap/ \u0026mdash;\u0026gt; 49bf3153d005 Removing intermediate container 1e849a37ee72 Step 30/45 : RUN rm -rf /var/log-removed \u0026mdash;\u0026gt; Running in 35d0bb1e34ac \u0026mdash;\u0026gt; 6d13c89ca7b8 Removing intermediate container 35d0bb1e34ac Step 31/45 : RUN sed -i \u0026rsquo;s!^d /var/log.*!L /var/log - - - - /data/var/log!\u0026rsquo; /usr/lib/tmpfiles.d/var.conf \u0026mdash;\u0026gt; Running in 3287ce189368 \u0026mdash;\u0026gt; 0fc4ec2c345c Removing intermediate container 3287ce189368 Step 32/45 : RUN mv /usr/lib/tmpfiles.d/journal-nocow.conf /usr/lib/tmpfiles.d/journal-nocow.conf.disabled \u0026mdash;\u0026gt; Running in 07849f109455 \u0026mdash;\u0026gt; d80610223617 Removing intermediate container 07849f109455 Step 33/45 : RUN rm -f /data-template/var/lib/systemd/random-seed \u0026mdash;\u0026gt; Running in b009e068aa00 \u0026mdash;\u0026gt; 3aa621b575ae Removing intermediate container b009e068aa00 Step 34/45 : RUN echo 1.1 \u0026gt; /etc/volume-version \u0026mdash;\u0026gt; Running in a9e66d69544f \u0026mdash;\u0026gt; 05d2ab591923 Removing intermediate container a9e66d69544f Step 35/45 : ENV container docker \u0026mdash;\u0026gt; Running in 27b114bb9901 \u0026mdash;\u0026gt; 3b927b73f0db Removing intermediate container 27b114bb9901 Step 36/45 : EXPOSE 53/udp 53 80 443 389 636 88 464 88/udp 464/udp 123/udp 7389 9443 9444 9445 \u0026mdash;\u0026gt; Running in da76877790e9 \u0026mdash;\u0026gt; 40e09ab03c05 Removing intermediate container da76877790e9 Step 37/45 : VOLUME /tmp /run /data /var/log/journal \u0026mdash;\u0026gt; Running in 0ea0157b1149 \u0026mdash;\u0026gt; 81126c6e8057 Removing intermediate container 0ea0157b1149 Step 38/45 : STOPSIGNAL RTMIN+3 \u0026mdash;\u0026gt; Running in 0829533a8567 \u0026mdash;\u0026gt; 0d9947067ebe Removing intermediate container 0829533a8567 Step 39/45 : ENTRYPOINT /usr/local/sbin/init \u0026mdash;\u0026gt; Running in 3f6c0821043f \u0026mdash;\u0026gt; 31def441bb06 Removing intermediate container 3f6c0821043f Step 40/45 : RUN uuidgen \u0026gt; /data-template/build-id \u0026mdash;\u0026gt; Running in 70183a84ee14 \u0026mdash;\u0026gt; dbb914132b5f Removing intermediate container 70183a84ee14 Step 41/45 : LABEL install \u0026lsquo;docker run -ti \u0026ndash;rm \u0026ndash;privileged -v /:/host -e HOST=/host -e DATADIR=/var/lib/${NAME} -e NAME=${NAME} -e IMAGE=${IMAGE} ${IMAGE} /bin/install.sh\u0026rsquo; \u0026mdash;\u0026gt; Running in 6deca08f490d \u0026mdash;\u0026gt; 5fed892b4df7 Removing intermediate container 6deca08f490d Step 42/45 : LABEL run \u0026lsquo;docker run ${RUN_OPTS} \u0026ndash;name ${NAME} -v /var/lib/${NAME}:/data:Z -v /sys/fs/cgroup:/sys/fs/cgroup:ro \u0026ndash;tmpfs /run \u0026ndash;tmpfs /tmp -v /dev/urandom:/dev/random:ro ${IMAGE}\u0026rsquo; \u0026mdash;\u0026gt; Running in 7c26df7faabc \u0026mdash;\u0026gt; 686f0db1737b Removing intermediate container 7c26df7faabc Step 43/45 : LABEL RUN_OPTS_FILE \u0026lsquo;/var/lib/${NAME}/docker-run-opts\u0026rsquo; \u0026mdash;\u0026gt; Running in 7ecd298291f9 \u0026mdash;\u0026gt; 6070645145b8 Removing intermediate container 7ecd298291f9 Step 44/45 : LABEL stop \u0026lsquo;docker stop ${NAME}\u0026rsquo; \u0026mdash;\u0026gt; Running in 7303fefc3b5c \u0026mdash;\u0026gt; b43a46e901d8 Removing intermediate container 7303fefc3b5c Step 45/45 : LABEL uninstall \u0026lsquo;docker run \u0026ndash;rm \u0026ndash;privileged -v /:/host -e HOST=/host -e DATADIR=/var/lib/${NAME} ${IMAGE} /bin/uninstall.sh\u0026rsquo; \u0026mdash;\u0026gt; Running in bf4fb0f57293 \u0026mdash;\u0026gt; e17fec1f8cf2 Removing intermediate container bf4fb0f57293 Successfully built e17fec1f8cf2\n{% endhighlight %}\nOnce the repository is build, we can verify the images in docker.The docker image we created is freeipa-server showing below\n{% highlight console %}\n[root@fedora25 freeipa-container-master]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE freeipa-server latest e17fec1f8cf2 10 minutes ago 855 MB registry.fedoraproject.org/fedora 27 9881e4229c95 47 hours ago 252 MB docker.io/freeipa/freeipa-server latest c1ab4ccd4cbc 7 days ago 795 MB\n{% endhighlight %}\nHow to run the FreeIPA docker container Now before running the container. Create a directory(used as a volume in docker instance) and set the necessary permission\n{% highlight console %}\n[root@fedora25 freeipa-container-master]# mkdir /var/lib/ipa-data\n{% endhighlight %}{% highlight console %}\n[root@fedora25 freeipa-container-master]# setsebool -P container_manage_cgroup 1 [root@fedora25 freeipa-container-master]# chmod 777 /var/lib/ipa-data\n{% endhighlight %}\nIf you need to access the docker container outside the host, you should map the container ports with host ports using option -p 53:53/udp -p 53:53 -p 80:80 -p 443:443 -p 389:389 -p 636:636 -p 88:88 -p 464:464 -p 88:88/udp -p 464:464/udp -p 123:123/udp -p 7389:7389 -p 9443:9443 -p 9444:9444 -p 9445:9445. But here of for testing i am doing the default install\nNow start run the docker\n{% highlight console %}\n[root@fedora25 freeipa-container-master]# docker run \u0026ndash;name freeipa-server-container -ti -h freeipa.vikki.in -v /sys/fs/cgroup:/sys/fs/cgroup:ro \u0026ndash;tmpfs /run \u0026ndash;tmpfs /tmp -v /var/lib/ipa-data:/data:Z freeipa-server systemd 234 running in system mode. (+PAM +AUDIT +SELINUX +IMA -APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ +LZ4 +SECCOMP +BLKID +ELFUTILS +KMOD -IDN2 +IDN default-hierarchy=hybrid) Detected virtualization docker. Detected architecture x86-64. Set hostname to \u0026lt;freeipa.vikki.in\u0026gt;. Initializing machine ID from random generator. system.slice: Failed to set invocation ID on control group /system.slice/docker-30667326bf93d1776a13acc7f395167f87aeb44326f4785e51c851bf35a1f46f.scope/system.slice, ignoring: Operation not permitted systemd-journald.service: Failed to set invocation ID on control group /system.slice/docker-30667326bf93d1776a13acc7f395167f87aeb44326f4785e51c851bf35a1f46f.scope/system.slice/systemd-journald.service, ignoring: Operation not permitted systemd-tmpfiles-setup.service: Failed to set invocation ID on control group /system.slice/docker-30667326bf93d1776a13acc7f395167f87aeb44326f4785e51c851bf35a1f46f.scope/system.slice/systemd-tmpfiles-setup.service, ignoring: Operation not permitted Sat Jan 20 07:09:58 UTC 2018 /usr/sbin/ipa-server-configure-first\nThe log file for this installation can be found in /var/log/ipaserver-install.log This program will set up the FreeIPA Server.\nThis includes:\nConfigure a stand-alone CA (dogtag) for certificate management Configure the Network Time Daemon (ntpd) Create and configure an instance of Directory Server Create and configure a Kerberos Key Distribution Center (KDC) Configure Apache (httpd) Configure the KDC to enable PKINIT To accept the default shown in brackets, press the Enter key.\nDo you want to configure integrated DNS (BIND)? [no]:\nEnter the fully qualified domain name of the computer on which you\u0026rsquo;re setting up server software. Using the form . Example: master.example.com.\nServer host name [freeipa.vikki.in]:\nThe domain name has been determined based on the host name.\nPlease confirm the domain name [vikki.in]:\nThe kerberos protocol requires a Realm name to be defined. This is typically the domain name converted to uppercase.\nPlease provide a realm name [VIKKI.IN]: Certain directory server operations require an administrative user. This user is referred to as the Directory Manager and has full access to the Directory for system management tasks and will be added to the instance of directory server created for IPA. The password must be at least 8 characters long.\nDirectory Manager password: Password (confirm):\nThe IPA server requires an administrative user, named \u0026lsquo;admin\u0026rsquo;. This user is a regular system account used for IPA server administration.\nIPA admin password: Password (confirm):\nThe IPA Master Server will be configured with: Hostname: freeipa.vikki.in IP address(es): 172.17.0.2 Domain name: vikki.in Realm name: VIKKI.IN\nContinue to configure the system with these values? [no]: yes\nThe following operations may take some minutes to complete. Please wait until the prompt is returned.\nConfiguring NTP daemon (ntpd) [1/4]: stopping ntpd [2/4]: writing configuration [3/4]: configuring ntpd to start on boot [4/4]: starting ntpd Done configuring NTP daemon (ntpd). Configuring directory server (dirsrv). Estimated time: 30 seconds [1/45]: creating directory server instance [2/45]: enabling ldapi [3/45]: configure autobind for root \u0026hellip;.\n[45/45]: configuring directory to start on boot Done configuring directory server (dirsrv). Configuring Kerberos KDC (krb5kdc) [1/10]: adding kerberos container to the directory [2/10]: configuring KDC [3/10]: initialize kerberos container \u0026hellip;. [28/29]: adding \u0026lsquo;ipa\u0026rsquo; CA entry [29/29]: configuring certmonger renewal for lightweight CAs Done configuring certificate server (pki-tomcatd). Configuring directory server (dirsrv) [1/3]: configuring TLS for DS instance [2/3]: adding CA certificate entry [3/3]: restarting directory server Done configuring directory server (dirsrv). Configuring ipa-otpd [1/2]: starting ipa-otpd [2/2]: configuring ipa-otpd to start on boot Done configuring ipa-otpd. Configuring ipa-custodia [1/5]: Generating ipa-custodia config file [2/5]: Making sure custodia container exists [3/5]: Generating ipa-custodia keys [4/5]: starting ipa-custodia [5/5]: configuring ipa-custodia to start on boot Done configuring ipa-custodia. Configuring the web interface (httpd) [1/22]: stopping httpd [2/22]: setting mod_nss port to 443 \u0026hellip;. [22/22]: enabling oddjobd Done configuring the web interface (httpd). Configuring Kerberos KDC (krb5kdc) [1/1]: installing X509 Certificate for PKINIT Done configuring Kerberos KDC (krb5kdc). Applying LDAP updates Upgrading IPA:. Estimated time: 1 minute 30 seconds [1/9]: stopping directory server \u0026hellip;.. [9/9]: starting directory server Done. Restarting the KDC ipaserver.dns_data_management: ERROR unable to resolve host name freeipa.vikki.in. to IP address, ipa-ca DNS record will be incomplete Please add records in this file to your DNS system: /tmp/ipa.system.records.dc9aakkj.db Configuring client side components Using existing certificate \u0026lsquo;/etc/ipa/ca.crt\u0026rsquo;. Client hostname: freeipa.vikki.in Realm: VIKKI.IN DNS Domain: vikki.in IPA Server: freeipa.vikki.in BaseDN: dc=vikki,dc=in\nSkipping synchronizing time with NTP server. New SSSD config will be created Configured sudoers in /etc/nsswitch.conf Configured /etc/sssd/sssd.conf trying https://freeipa.vikki.in/ipa/json [try 1]: Forwarding \u0026lsquo;schema\u0026rsquo; to json server \u0026lsquo;https://freeipa.vikki.in/ipa/json' trying https://freeipa.vikki.in/ipa/session/json [try 1]: Forwarding \u0026lsquo;ping\u0026rsquo; to json server \u0026lsquo;https://freeipa.vikki.in/ipa/session/json' [try 1]: Forwarding \u0026lsquo;ca_is_enabled\u0026rsquo; to json server \u0026lsquo;https://freeipa.vikki.in/ipa/session/json' Systemwide CA database updated. SSSD enabled Configured /etc/openldap/ldap.conf /etc/ssh/ssh_config not found, skipping configuration /etc/ssh/sshd_config not found, skipping configuration Configuring vikki.in as NIS domain. Client configuration complete. The ipa-client-install command was successful\n============================================================================== Setup complete\nNext steps:\nYou must make sure these network ports are open: TCP Ports:\n80, 443: HTTP/HTTPS 389, 636: LDAP/LDAPS 88, 464: kerberos UDP Ports: 88, 464: kerberos 123: ntp You can now obtain a kerberos ticket using the command: \u0026lsquo;kinit admin\u0026rsquo; This ticket will allow you to use the IPA tools (e.g., ipa user-add) and the web user interface.\nKerberos requires time synchronization between clients and servers for correct operation. You should consider enabling ntpd.\nBe sure to back up the CA certificates stored in /root/cacert.p12 These files are required to create replicas. The password for these files is the Directory Manager password FreeIPA server does not run DNS server, skipping update-self-ip-address. Created symlink /etc/systemd/system/container-ipa.target.wants/ipa-server-update-self-ip-address.service ‚Üí /usr/lib/systemd/system/ipa-server-update-self-ip-address.service. Created symlink /etc/systemd/system/container-ipa.target.wants/ipa-server-upgrade.service ‚Üí /usr/lib/systemd/system/ipa-server-upgrade.service. Removed /etc/systemd/system/container-ipa.target.wants/ipa-server-configure-first.service. FreeIPA server configured.\n{% endhighlight %}\nNow the FreeIPA docker container is ready. Verify the status of running container.\n{% highlight console %}\n[root@fedora25 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 30667326bf93 freeipa-server \u0026ldquo;/usr/local/sbin/init\u0026rdquo; 45 minutes ago Up 44 minutes 53/tcp, 80/tcp, 53/udp, 88/udp, 88/tcp, 389/tcp, 443/tcp, 123/udp, 464/tcp, 636/tcp, 7389/tcp, 9443-9445/tcp, 464/udp freeipa-server-container\n{% endhighlight %}\nHow to enroll the host to FreeIPA running in container We can also enroll the host to FreeIPA server running in container.Since i don\u0026rsquo;t have DNS configured, i manually added the FreeIPA docker container IP to the local resolver file in host machine.\n{% highlight console %}\n[root@fedora25 ~]# docker inspect \u0026ndash;format \u0026lsquo;{{.NetworkSettings.IPAddress}}\u0026rsquo; 1freeipa-server-container 172.17.0.2 [root@fedora25 ~]# cat /etc/hosts |grep freeipa 172.17.0.2 freeipa.vikki.in\n{% endhighlight %}\nNow install the ipa client in fedora host machine.\n{% highlight console %}\n[root@fedora25 ~]# ipa-client-install WARNING: ntpd time\u0026amp;date synchronization service will not be configured as conflicting service (chronyd) is enabled Use \u0026ndash;force-ntpd option to disable it and force configuration of ntpd\nDNS discovery failed to determine your DNS domain Provide the domain name of your IPA server (ex: example.com): vikki.in Provide your IPA server name (ex: ipa.example.com): freeipa.vikki.in The failure to use DNS to find your IPA server indicates that your resolv.conf file is not properly configured. Autodiscovery of servers for failover cannot work with this configuration. If you proceed with the installation, services will be configured to always access the discovered server for all operations and will not fail over to other servers in case of failure. Proceed with fixed values and no DNS discovery? [no]: yes Client hostname: fedora25.vikki.in Realm: VIKKI.IN DNS Domain: vikki.in IPA Server: freeipa.vikki.in BaseDN: dc=vikki,dc=in\nContinue to configure the system with these values? [no]: yes Skipping synchronizing time with NTP server. User authorized to enroll computers: admin Password for admin@VIKKI.IN: Successfully retrieved CA cert Subject: CN=Certificate Authority,O=VIKKI.IN Issuer: CN=Certificate Authority,O=VIKKI.IN Valid From: 2018-01-20 07:13:00 Valid Until: 2038-01-20 07:13:00\nEnrolled in IPA realm VIKKI.IN Created /etc/ipa/default.conf New SSSD config will be created Configured sudoers in /etc/nsswitch.conf Configured /etc/sssd/sssd.conf Configured /etc/krb5.conf for IPA realm VIKKI.IN trying https://freeipa.vikki.in/ipa/json [try 1]: Forwarding \u0026lsquo;schema\u0026rsquo; to json server \u0026lsquo;https://freeipa.vikki.in/ipa/json' trying https://freeipa.vikki.in/ipa/session/json [try 1]: Forwarding \u0026lsquo;ping\u0026rsquo; to json server \u0026lsquo;https://freeipa.vikki.in/ipa/session/json' [try 1]: Forwarding \u0026lsquo;ca_is_enabled\u0026rsquo; to json server \u0026lsquo;https://freeipa.vikki.in/ipa/session/json' Systemwide CA database updated. Hostname (fedora25.vikki.in) does not have A/AAAA record. Failed to update DNS records. Missing A/AAAA record(s) for host fedora25.vikki.in: 172.17.0.1. Missing reverse record(s) for address(es): 172.17.0.1. Adding SSH public key from /etc/ssh/ssh_host_rsa_key.pub Adding SSH public key from /etc/ssh/ssh_host_ecdsa_key.pub Adding SSH public key from /etc/ssh/ssh_host_ed25519_key.pub [try 1]: Forwarding \u0026lsquo;host_mod\u0026rsquo; to json server \u0026lsquo;https://freeipa.vikki.in/ipa/session/json' Could not update DNS SSHFP records. SSSD enabled Configured /etc/openldap/ldap.conf Configured /etc/ssh/ssh_config Configured /etc/ssh/sshd_config Configuring vikki.in as NIS domain. Client configuration complete. The ipa-client-install command was successful\n{% endhighlight %}\nOnce client is installed and authenticated to the FreeIPA server, we can verify the hosts by login to GUI as shown below\nVerify the users are synced to the client.\n{% highlight console %}\n[root@fedora25 ~]# id vivek uid=1661600001(vivek) gid=1661600001(vivek) groups=1661600001(vivek) [root@fedora25 ~]#\n{% endhighlight %}\n","permalink":"http://localhost:1313/posts/2017-10-07-freeipa_installation_configuration/","summary":"\u003cp\u003eI worked on a freelance project a year ago which gives me experience in Free IPA server. Here I am sharing some portion of the setup used.It would be useful for beginners trying to setup Free IPA server.\u003c/p\u003e\n\u003cp\u003eI have 2 sections here. One is about FreeIPA on the normal server and the other is FreeIPA in Docker container.\u003c/p\u003e\n\u003cp\u003eIf you have any questions \u0026amp; comments, feel free to add in the Disqus comment section below\u003c/p\u003e","title":"FreeIPA Installation and configuration"},{"content":"In my previous post i described how i created high available replicated storage with raspberrypi\nIn this post i will guide you how to interface raspberry pi to blink a led in morse code for the user input.The same signal fed into the LED can be send to a radio transmitter and we can transmit it in radio frequency . Recently i starter learning morse code and i am going to apply a license for Amateur radio operator.\nWhat you need: Raspberry PI Model 2 ‚Äì 1GB\nBreadboard\nJumper wires (Male to Female)\nLED\n220 ohm resistor\nAssembling the circuit: To make the LED programmable we need to use one of the GPIO pin from raspberry pi. More details on GPIO here.Python has a library ‚ÄúRPi.GPIO‚Äù to interface with raspberry pi. We are going to use this library to designates the GPIO pins to digital inputs or outputs. When a pin turns from LOW (0) to HIGH (1), it supplies a 3.3v signal.\nBelow is the pin diagram of the raspberry pi B+ model\nWe are going to use the GPIO pin 4 and a ground to programably control the led. Connect resistor and led as per the below diagram.\nOnce the circuit is ready we can power on the raspberry pi and connect to pi using SSH or VNC.\nBelow is my final assembled circuit.\nBlinking LED from raspberry pi Connect to raspberry pi using ssh or vnc, open vim and copy paste the below code and save it. But default vim don‚Äôt support syntax highlight in raspbian os,So modify the ‚Äúvimrc‚Äù to enable syntax highlighting.\n{% highlight console %}\npi@raspberrypi ~/vikki/python $ sudo cat /etc/vim/vimrc \u0026quot; Vim5 and later versions support syntax highlighting. Uncommenting the next \u0026quot; line enables syntax highlighting by default. \u0026ldquo;syntax on syntax on\n{% endhighlight %}\nBelow is the python code to blink the led . You can download the updated code from my github.\nThe code is explained inside in comment section.\n{% highlight python linenos %}\nImport the GPIO and time libraries import RPi.GPIO as GPIO import time\nSet the pin designation type. In this case, we use BCM- the GPIO number- rather than the pin number itself. GPIO.setmode (GPIO.BCM)\nSo that you don\u0026rsquo;t need to manage non-descriptive numbers, set \u0026ldquo;LIGHT\u0026rdquo; to 4 so that our code can easily reference the correct pin. LIGHT = 4\nBecause GPIO pins can act as either digital inputs or outputs, we need to designate which way we want to use a given pin. This allows us to use functions in the GPIO library in order to properly send and receive signals. GPIO.setup(LIGHT,GPIO.OUT)\nCause the light to blink 7 times and print a message each time. To blink the light, we call GPIO.output and pass as parameters the pin number (LIGHT) and the state we want. True sets the pin to HIGH (sending a signal), False sets it to LOW. To achieve a blink, we set the pin to High, wait for a fraction of a second, then set it to Low. Adding keyboard interrupt with try and except so that program terminates when user presses Ctrl+C. try: while True: GPIO.output(LIGHT,True) time.sleep(0.5) GPIO.output(LIGHT,False) time.sleep(0.5) print(\u0026ldquo;blink\u0026rdquo;) except KeyboardInterrupt: GPIO.cleanup()\n{% endhighlight %}\nNow run the python code and verify the LED start blinking.\nDemo of LED blinking from raspberry pi: Blinking LED in morse for user input: Once we are able to blink the led from python ,we need to blink it in morse for the user input. Below is the python code to blink the led for morse. You can download the updated code from my github.I just added few lines to the previous code to blink for morse.\n{% highlight python linenos %}\nImport the GPIO and time libraries import RPi.GPIO as GPIO import time\n#####Morse code ###### CODE = {\u0026rsquo; \u0026lsquo;: \u0026rsquo; \u0026lsquo;, \u0026ldquo;\u0026rsquo;\u0026rdquo;: \u0026lsquo;.\u0026mdash;-.\u0026rsquo;, \u0026lsquo;(\u0026rsquo;: \u0026lsquo;-.\u0026ndash;.-\u0026rsquo;, \u0026lsquo;)\u0026rsquo;: \u0026lsquo;-.\u0026ndash;.-\u0026rsquo;, \u0026lsquo;,\u0026rsquo;: \u0026lsquo;\u0026ndash;..\u0026ndash;\u0026rsquo;, \u0026lsquo;-\u0026rsquo;: \u0026lsquo;-\u0026hellip;.-\u0026rsquo;, \u0026lsquo;.\u0026rsquo;: \u0026lsquo;.-.-.-\u0026rsquo;, \u0026lsquo;/\u0026rsquo;: \u0026lsquo;-..-.\u0026rsquo;, \u0026lsquo;0\u0026rsquo;: \u0026lsquo;\u0026mdash;\u0026ndash;\u0026rsquo;, \u0026lsquo;1\u0026rsquo;: \u0026lsquo;.\u0026mdash;-\u0026rsquo;, \u0026lsquo;2\u0026rsquo;: \u0026lsquo;..\u0026mdash;\u0026rsquo;, \u0026lsquo;3\u0026rsquo;: \u0026lsquo;\u0026hellip;\u0026ndash;\u0026rsquo;, \u0026lsquo;4\u0026rsquo;: \u0026lsquo;\u0026hellip;.-\u0026rsquo;, \u0026lsquo;5\u0026rsquo;: \u0026lsquo;\u0026hellip;..\u0026rsquo;, \u0026lsquo;6\u0026rsquo;: \u0026lsquo;-\u0026hellip;.\u0026rsquo;, \u0026lsquo;7\u0026rsquo;: \u0026lsquo;\u0026ndash;\u0026hellip;\u0026rsquo;, \u0026lsquo;8\u0026rsquo;: \u0026lsquo;\u0026mdash;..\u0026rsquo;, \u0026lsquo;9\u0026rsquo;: \u0026lsquo;\u0026mdash;-.\u0026rsquo;, \u0026lsquo;:\u0026rsquo;: \u0026lsquo;\u0026mdash;\u0026hellip;\u0026rsquo;, \u0026lsquo;;\u0026rsquo;: \u0026lsquo;-.-.-.\u0026rsquo;, \u0026lsquo;?\u0026rsquo;: \u0026lsquo;..\u0026ndash;..\u0026rsquo;, \u0026lsquo;A\u0026rsquo;: \u0026lsquo;.-\u0026rsquo;, \u0026lsquo;B\u0026rsquo;: \u0026lsquo;-\u0026hellip;\u0026rsquo;, \u0026lsquo;C\u0026rsquo;: \u0026lsquo;-.-.\u0026rsquo;, \u0026lsquo;D\u0026rsquo;: \u0026lsquo;-..\u0026rsquo;, \u0026lsquo;E\u0026rsquo;: \u0026lsquo;.\u0026rsquo;, \u0026lsquo;F\u0026rsquo;: \u0026lsquo;..-.\u0026rsquo;, \u0026lsquo;G\u0026rsquo;: \u0026lsquo;\u0026ndash;.\u0026rsquo;, \u0026lsquo;H\u0026rsquo;: \u0026lsquo;\u0026hellip;.\u0026rsquo;, \u0026lsquo;I\u0026rsquo;: \u0026lsquo;..\u0026rsquo;, \u0026lsquo;J\u0026rsquo;: \u0026lsquo;.\u0026mdash;\u0026rsquo;, \u0026lsquo;K\u0026rsquo;: \u0026lsquo;-.-\u0026rsquo;, \u0026lsquo;L\u0026rsquo;: \u0026lsquo;.-..\u0026rsquo;, \u0026lsquo;M\u0026rsquo;: \u0026lsquo;\u0026ndash;\u0026rsquo;, \u0026lsquo;N\u0026rsquo;: \u0026lsquo;-.\u0026rsquo;, \u0026lsquo;O\u0026rsquo;: \u0026lsquo;\u0026mdash;\u0026rsquo;, \u0026lsquo;P\u0026rsquo;: \u0026lsquo;.\u0026ndash;.\u0026rsquo;, \u0026lsquo;Q\u0026rsquo;: \u0026lsquo;\u0026ndash;.-\u0026rsquo;, \u0026lsquo;R\u0026rsquo;: \u0026lsquo;.-.\u0026rsquo;, \u0026lsquo;S\u0026rsquo;: \u0026lsquo;\u0026hellip;\u0026rsquo;, \u0026lsquo;T\u0026rsquo;: \u0026lsquo;-\u0026rsquo;, \u0026lsquo;U\u0026rsquo;: \u0026lsquo;..-\u0026rsquo;, \u0026lsquo;V\u0026rsquo;: \u0026lsquo;\u0026hellip;-\u0026rsquo;, \u0026lsquo;W\u0026rsquo;: \u0026lsquo;.\u0026ndash;\u0026rsquo;, \u0026lsquo;X\u0026rsquo;: \u0026lsquo;-..-\u0026rsquo;, \u0026lsquo;Y\u0026rsquo;: \u0026lsquo;-.\u0026ndash;\u0026rsquo;, \u0026lsquo;Z\u0026rsquo;: \u0026lsquo;\u0026ndash;..\u0026rsquo;, \u0026lsquo;_\u0026rsquo;: \u0026lsquo;..\u0026ndash;.-\u0026rsquo;} ######End of morse code######\nSet the pin designation type. In this case, we use BCM- the GPIO number- rather than the pin number itself. GPIO.setmode (GPIO.BCM)\nSo that you don\u0026rsquo;t need to manage non-descriptive numbers, set \u0026ldquo;LIGHT\u0026rdquo; to 4 so that our code can easily reference the correct pin. LIGHT = 4\nBecause GPIO pins can act as either digital inputs or outputs, we need to designate which way we want to use a given pin. This allows us to use functions in the GPIO library in order to properly send and receive signals. GPIO.setup(LIGHT,GPIO.OUT)\ndef dot(): GPIO.output(LIGHT,True) time.sleep(0.2) GPIO.output(LIGHT,False) time.sleep(0.2)\ndef dash(): GPIO.output(LIGHT,True) time.sleep(0.5) GPIO.output(LIGHT,False) time.sleep(0.2)\ntry: while True: input = raw_input(\u0026lsquo;What would you like to send? \u0026lsquo;) for letter in input: for symbol in CODE[letter.upper()]: if symbol == \u0026lsquo;-\u0026rsquo;: dash() elif symbol == \u0026lsquo;.\u0026rsquo;: dot() else: time.sleep(0.5) time.sleep(0.5) except KeyboardInterrupt: GPIO.cleanup()\n{% endhighlight %}\nDemo of LED blinking in Morse for the user input: ","permalink":"http://localhost:1313/posts/2017-10-07-raspberry-pi-blinking-led-in-morse/","summary":"\u003cp\u003eIn my previous post i described how i created high available replicated storage with raspberrypi\u003cbr\u003e\nIn this post i will guide you how to interface raspberry pi to blink a led in morse code for the user input.The same signal fed into the LED can be send to a radio transmitter and we can transmit it in radio frequency . Recently i starter learning morse code and i am going to apply a license for Amateur radio operator.\u003c/p\u003e","title":"Raspberry pi ‚Äì blinking led in morse"},{"content":"Overview This post is about how to classify network traffic captured from wireshark using weka machine learning algorithm. I tried few other methods like nltk,sckikit,python scripts with naive bayes implementation and finally decided to use weka mainly because of its simplicity,easy to use and also because it is written in java so it is easier to integrate with other java applications(which is i am planning to do).You can check my github machine learning project page for the other methods i tried.\nSoftware‚Äôs used Wireshark\nweka\nFedora\nStep 1 :Installing the softwares Install the wireshark and weka. Both of the software packeges comes with fedora default repositories, So we can just do the dnf install .\nStep 2 :Capturing network packets using wireshark Open wireshark and start capturing the packets in your network interface card . I used specific filters(http,telnet,etc) to capture different type of traffic so that i can use the same as traning set in weka. These captured traffic can be exported into csv file. The files used can be downloaded from my github\nStep 3:Converting csv to arff format We need to export this as csv because the weka application don‚Äôt support pcap file. The exported csv can be converted to arff format later using the weka arff viewer.\nStep 4:Defining the class Now we have converted the wireshark output to arff so that it can be readable be the weka application. The next step is to define the class . Class is something like a category in which the specific instance is belongs to.\nFor this, I opened the arff file and manually defined the class based on the application the traffic is generated\nTraffic type Application used http browser ftp filezilla client bittorrent peer-peer_client snmp monitoring_tools My final arff file is below, with the class name ‚ÄúTraffic\\_category‚Äù added. Now if you see closely, for some of the instances(5,31,35,53) the ‚ÄúTraffic\\_category‚Äù class is left undefined.In the next step i am going to use weka to predict the class in which the instance belongs to using machine learning algorithm. Step 5: Predicting the ‚ÄúTraffic category‚Äù using J48ext machine learning algorithm Now i am going to run the weka J48ext machine learning algorithm to predict the traffic category.There are many algorithm supported by weka like zeroR,naive bayes,we can use any of this which is best suited for our datasets.\nMake sure the class path is properly exported before executing the code. In my setup the weka.jar is located inside ‚Äú/usr/share/java‚Äù\n{% highlight console %}\nexport CLASSPATH=$CLASSPATH:/usr/share/java/weka.jar ~jython UsingJ48Ext.py main.arff\n{% endhighlight %}\nThe code can be downloaded from github\nThe output contains 3 type of informations\n1. Generated model J48 pruned tree\n{% highlight console %}\nProtocol = SNMP: monitoring_tools (10.0/1.0) Protocol = SRVLOC: telnet (0.0) Protocol = NBNS: telnet (0.0) Protocol = BROWSER: telnet (0.0) Protocol = ICMP: telnet (0.0) Protocol = TCP: telnet (0.0) Protocol = TELNET: telnet (18.0) Protocol = BitTorrent: peer-peer_client (12.0) Protocol = TFTP: filezilla (9.0) Protocol = HTTP: browsers (4.0)\nNumber of Leaves : 10\nSize of the tree : 11\n{% endhighlight %}\n2. Evaluation {% highlight console %}\nCorrectly Classified Instances 52 98.1132 % Incorrectly Classified Instances 1 1.8868 % Kappa statistic 0.9753 Mean absolute error 0.0136 Root mean squared error 0.0824 Relative absolute error 4.4312 % Root relative squared error 21.0904 % Total Number of Instances 53 Ignored Class Unknown Instances 4\n{% endhighlight %}\n3. Prediction {% highlight console %}\n1 5:monitori 5:monitori 0.9 2 5:monitori 5:monitori 0.9 5 1:? 5:monitori 0.9 6 5:monitori 5:monitori 0.9 7 5:monitori 5:monitori 0.9 27 4:telnet 4:telnet 1 28 4:telnet 4:telnet 1 29 2:peer-pee 2:peer-pee 1 30 2:peer-pee 2:peer-pee 1 31 1:? 2:peer-pee 1 32 2:peer-pee 2:peer-pee 1 33 2:peer-pee 2:peer-pee 1 34 2:peer-pee 2:peer-pee 1 35 1:? 2:peer-pee 1 36 2:peer-pee 2:peer-pee 1 37 2:peer-pee 2:peer-pee 1 43 3:filezill 3:filezill 1 44 3:filezill 3:filezill 1 45 3:filezill 5:monitori + 0.9 46 3:filezill 3:filezill 1 47 3:filezill 3:filezill 1 52 3:filezill 3:filezill 1 53 1:? 1:browsers 1 54 1:browsers 1:browsers 1\n{% endhighlight %}\nNow the information provided in prediction is our focus.\nIt contains 4 column\n{% highlight console %}\nColumn number\tDetails First column\tInstance number Second column\tClass defined in dataset Third column\tClass predicted by machine learning Fourth column\tProbability that the instance is belong to the class predicted my machine\n{% endhighlight %}\nNow if you notice the instance number 5,31,35 and 53 the class defined in dataset is empty(‚Äú?‚Äù) and the machine has predicted the class as monitoring_tool,peer-peer client and browser respectively. The ‚Äú+‚Äù symbol in the instance number 45 denotes there is difference in the class defined in dataset and the class predicted by machine learning.\n","permalink":"http://localhost:1313/posts/2017-10-07-machine-learning-network-traffic-classification-using-weka/","summary":"\u003ch3 id=\"overview\"\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThis post is about how to classify network traffic captured from wireshark using weka machine learning algorithm. I tried few other methods like nltk,sckikit,python scripts with naive bayes implementation and finally decided to use weka mainly because of its simplicity,easy to use and also because it is written in java so it is easier to integrate with other java applications(which is i am planning to do).You can check my github machine learning project page for the other methods i tried.\u003c/p\u003e","title":"Machine learning ‚Äì Network traffic classification using weka"},{"content":"I have been trying my hand in photography for a few months now. Recently during a trip to Bannerghatta National Park i managed to shoot some of the wildlife using my DSLR camera. Here are few snaps from my recent trip to Bannerghatta National Park.\nTiger‚Äôs tongue is so coarse, it can lick flesh down to the bone. The tiger‚Äôs tongue is covered with numerous small, sharp, rear-facing projections called papillae. These papillae gives the tongue is rough, rasping texture and is designed to help strip the skin, feathers, fur and meat right off its prey\nAfter eating a big meal, a lion can sleep a full 24 hours. Lions enjoy relaxing and lazing around. They spend between 16 and 20 hours each day resting and sleeping. They have few sweat glands so they wisely tend to conserve their energy by resting during the day and become more active at night when it is cooler.\nAll subspecies of tiger have white spots or ‚Äòflashes‚Äô on the backs of their ears. It is more likely that ear spots are a signal of aggression. A tiger under threat will rotate the ears in such a way that the spots can be seen from the front so providing a visual warning\nAll cats walk on their toes. During a walk the tiger lifts both limbs on the same side together. This gait is similar to that of a camel, and quite unlike that of a horse.Some people claim the tiger has almost mathematically precise movement with the hind paw stepping in exactly the spot previously occupied by the forefoot.\nA tiger‚Äôs eyes are the brightest of any animal. Tigers have eyes with round pupils, unlike domestic cats, which have slitted pupils. This is because domestic cats are nocturnal whereas tigers are crepuscular ‚Äì they hunt primarily in the morning and evening.\nTiger facts credit ‚Äì http://www.lairweb.org.nz/tiger ","permalink":"http://localhost:1313/posts/2017-10-07-trip-to-wild-bannerghatta-national-park/","summary":"\u003cp\u003eI have been trying my hand in photography for a few months now. Recently during a trip to Bannerghatta National Park i managed to shoot some of the wildlife using my DSLR camera. Here are few snaps from my recent trip to Bannerghatta National Park.\u003c/p\u003e\n\u003ch3 id=\"tigers-tongue-is-so-coarse-it-can-lick-flesh-down-to-the-bone\"\u003eTiger‚Äôs tongue is so coarse, it can lick flesh down to the bone.\u003c/h3\u003e\n\u003c!--kg-card-begin: image--\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"/content/images/2019/02/30354172535_fdf9c66184_k_opt.jpg\" class=\"kg-image\" alt=\"30354172535_fdf9c66184_k_opt\"\u003e\u003c/figure\u003e\u003c!--kg-card-end: image--\u003e\n\u003cp\u003eThe tiger‚Äôs tongue is covered with numerous small, sharp, rear-facing projections called papillae. These papillae gives the tongue is rough, rasping texture and is designed to help strip the skin, feathers, fur and meat right off its prey\u003c/p\u003e","title":"Trip to wild ‚Äì Bannerghatta National Park"},{"content":"So‚Ä¶. I thought of a bike trip after New Year. This time taking my wife on the trip. So I was thinking something short in distance something less than 150 km, then Ooty comes to my mind. Then we both decided to go on a trip from Coimbatore to Ooty. It is a weekend trip, we just decided 3 days before, so no big plans.\nSince it is the weekend, I know most of the hotels will be booked. So, I searched for decent hotels under 3k. After reading reviews from Tripadvisor and Google I booked Hotel Lakeview. I managed to pay around 2.5k after applying coupons from MakeMyTrip.\nThere are 2 routes to reach Ooty from Coimbatore.\nCoimbatore -\u0026gt; Mettupalayam -\u0026gt; Coonoor -\u0026gt; Ooty Coimbatore -\u0026gt; Mettupalayam -\u0026gt; Kotagiri -\u0026gt; Ooty The second route takes 15 km more, but after some research, we still decided to take the Kotagiri route. This route has less traffic and good scenic view, though the distance is more. I recommend anyone to take the Kotagiri route than Coonoor route for enjoying the ride. Kotagiri route has 7 hairpin bends to reach Ooty. Here is the Google route map.\nFirst royal enfield trip I usually take my Yamaha Fz for all the bike trip but this time, I decided to take Royal Enfield.\nI googled and found some bike rentals from Coimbatore. I called and checked with almost all such rentals. But in most of them, Royal Enfield is already booked.\nFinally, I managed to book a Royal Enfield Thunderbird 500cc. It‚Äôs 1800 rupees per day and I have to pay 3600 rupees for 2 days. The price is little high comparing other bike rental services. But we had no other choice, so I booked it and asked them to deliver it by 8:30am.\nOn Saturday morning after breakfast, we started at 10.00 AM from Coimbatore. Till Mettupalayam there is traffic but after that when we take Kotagiri road it is traffic free and very scenic. Here are some of the pictures on the way to Ooty.\nScenic view of Nilgiri hill Kotagiri village We reached Ooty at around 1 PM and checked into the Hotel. Hotel Lake view is one of the best hotels to stay in Ooty on budget. It is very close to the Ooty Lake, you can almost walk, though you will not get a lake view from hotel. Here are some of the pic from the hotel.\nCottages with parking space After lunch, we decided to visit Ooty Lake and went on a pedal boat ride. It‚Äôs 140 rupees for 30 minutes. Since it is off-season, there is no much crowd and we truly enjoyed the ride.\nSome other pictures Row boat in Ooty lake Ooty lake staring point We visited Ooty botanical garden in evening. Here are some picture from the garden.\nArtificial lake Flowers in the garden Child playing in the garden We also visited the tribal village of Toda people. Toda people are a small pastoral tribal community who live on the isolated Nilgiri plateau religion features the sacred buffalo, rituals are performed for all dairy activities by dairymen-priests\nNote the very small door in front of the temple. The priest used to sleep inside this temple. The priest uses only the buffalo milk for food. Buffalo milk is used in a variety of forms: butter, buttermilk, yogurt, cheese and drunk plain.\nToda temple A small hut used for shifting god sculpture Next day we dropped the plan to visit Doddaabetta. This place had some restriction since the Governor is visiting. It was also very cold in the morning and temperature reached 7 degrees. So after breakfast, we went for shopping. Ooty is famous for homemade chocolates, tea, spices and essential oils. We went to Tibetan market where you can get sweaters, riding jackets in cheap price.\nAfter shopping, we had lunch and started to ride back to Coimbatore.\nHere is me with my wife and the journey continues ‚Ä¶\nView this post on Instagram #throwback #ooty\nA post shared by Vignesh Ragupathy (@vikkiinimg) on Feb 24, 2019 at 2:18am PST\n","permalink":"http://localhost:1313/posts/2017-10-07-bike-trip-to-ooty/","summary":"\u003cp\u003eSo‚Ä¶. I thought of a bike trip after New Year. This time taking my wife on the trip. So I was thinking something short in distance something less than 150 km, then Ooty comes to my mind. Then we both decided to go on a trip from Coimbatore to Ooty. It is a weekend trip, we just decided 3 days before, so no big plans.\u003c/p\u003e\n\u003cp\u003eSince it is the weekend, I know most of the hotels will be booked. So, I searched for decent hotels under 3k. After reading reviews from Tripadvisor and Google I booked Hotel Lakeview. I managed to pay around 2.5k after applying coupons from MakeMyTrip.\u003c/p\u003e","title":"bike trip to ooty"},{"content":"Someone once said ‚ÄúHappiness is a path, not a destination‚Äù and I know its very true when i started to travel in bike.I have plans to visit many places in bike for this year and then one of my friend called and we both decided to go Goa for a bike trip. We are 3 person in two bike, a Royal enfield and a Yamaha Fz. We don‚Äôt have much plans to do in Goa, only thing we wanted is to go there in a bike.\nIt was really a very long trip, we never expected it will take this long(almost 19 hours). We started at 8:30AM in Bangalore, that was off course a biggest mistake.It was very high traffic and it took more than 3 hours to cross Bangalore. Second after reaching Hubali-Dharwad we tried a shortcut which goes via the forest area and not the NH road. We took too many breaks,almost every 100KM. The climate is very hot and too cold after entering the forest at night .It also happened that one of our bike headlight got fuse, so we should travel inside forest for around 60 Km with only one light . It was a horrible experience, there are no street lights and no signs of human anywhere.We started going inside the forest at 11:30PM. In many places there was no signal and we are not able to use GPS for navigation . We used the downloaded offline maps to navigate for some distance. We also lost and traveled some 20Km in wrong direction because of this. Do not use the forest road after 11PM, It is very risky and you won‚Äôt find anyone to help you there. There is no shops and you can‚Äôt halt anywhere between,Its pitch black everywhere.\nOne of the bike fuel also got empty but at that time we almost reached Goa. Some of them are ready to help us for getting fuel but we transferred the fuel from my bike and used it . Finally we reached the room at 5AM in morning. We don‚Äôt know the flat that we booked won‚Äôt allow check in before 8am in morning. It‚Äôs one of the stupidest thing i heard. There is no key available for the flat, as the owner is staying away from the apartment. After sometime, finally he agreed to arrange a temporary room in a nearby hotel. We stayed there until 10am and then shifted to the flat. We all were very tired and slept until 3PM. That day we don‚Äôt have much plans. We did little purchasing and slept again.\nNext day it was Sunday. We woke up early morning and went to the Calangute Beach. Many water sports event happening there. We tried Parasailing and Speed boat . It was 900 rupees for the Parasailing and 700 rupees for speed boat,They will allow you to jump into the water and swim there for another 5 min by paying extra 300 rupees. We did temporary tattoos. Then we went Fort Aguada, its lighthouse and took some photos.\nRound trip distance : 1350 km\nTime taken from Bangalore to Goa : 19 hours\nTime taken from Goa to Bangalore : 15 hours\nBelow are some of the pictures and videos from my bike trip to Goa. We are not a professional photographer üôÇ\nOur team(Bangalore) : View this post on Instagram #throwback #goa #www.vikki.in\nA post shared by Vignesh Ragupathy (@vikkiinimg) on Feb 24, 2019 at 1:29am PST\n### On the way to Goa : View this post on Instagram #throwback #goa #www.vikki.in\nA post shared by Vignesh Ragupathy (@vikkiinimg) on Feb 24, 2019 at 1:28am PST\nView this post on Instagram These stunts are performed by experts, don't try this at home!!! #throwback #goa #www.vikki.in\nA post shared by Vignesh Ragupathy (@vikkiinimg) on Feb 24, 2019 at 1:44am PST\nView this post on Instagram These stunts area performed by expert, don't try this at home #throwback #goa #vikki\nA post shared by Vignesh Ragupathy (@vikkiinimg) on Feb 24, 2019 at 1:49am PST\nView this post on Instagram #throwback #goa #vikki\nA post shared by Vignesh Ragupathy (@vikkiinimg) on Feb 24, 2019 at 2:05am PST\n### Speed boating in beach View this post on Instagram #throwback #goa #www.vikki.in #blog\nA post shared by Vignesh Ragupathy (@vikkiinimg) on Feb 24, 2019 at 1:38am PST\n### Superman Tshirt View this post on Instagram #throwback #superman #goa #vikki\nA post shared by Vignesh Ragupathy (@vikkiinimg) on Feb 24, 2019 at 1:55am PST\n### Our trip map : ### Parasailing in Goa beach : ","permalink":"http://localhost:1313/posts/2017-10-07-bike-trip-to-goa/","summary":"\u003cp\u003eSomeone once said ‚ÄúHappiness is a path, not a destination‚Äù and I know its very true when i started to travel in bike.I have plans to visit many places in bike for this year and then one of my friend called and we both decided to go Goa for a bike trip. We are 3 person in two bike, a Royal enfield and a Yamaha Fz. We don‚Äôt have much plans to do in Goa, only thing we wanted is to go there in a bike.\u003c/p\u003e","title":"Bike trip to Goa"},{"content":"Introduction Linux Bonding driver The Linux bonding driver provides a method for aggregating multiple network interface controllers (NICs) into a single logical bonded interface of two or more so-called (NIC) slaves. The majority of modern Linux distributions (distros) come with a Linux kernel which has the Linux bonding driver (bonding)integrated as a loadable kernel module.\nWe can check the bonding module by\n{% highlight console %}\n[root@example.com ~]# lsmod |grep bonding bonding 142537 0\n{% endhighlight %}\nLinux Teaming driver The Linux Team driver provides an alternative to bonding driver. it has actually been designed to solve the same problem(s) using a wholly different design and different approach; an approach where special attention was paid to flexibility and efficiency. The best part is that the configuration, management, and monitoring of team driver is significantly improved with no compromise on performance, features, or throughput.The main difference is that Team driver kernel part contains only essential code and the rest of the code (link validation, LACP implementation, decision making, etc.) is run in userspace as a part of teamd daemon.\nFeatures comparison between Bonding and Teaming\nsource ‚Äì redhat.com\nFeature Bonding Team {% highlight console %}\nbroadcast TX policy\tYes\tYes round-robin TX policy\tYes\tYes active-backup TX policy\tYes\tYes LACP (802.3ad) support\tYes\tYes hash-based TX policy\tYes\tYes TX load-balancing support (TLB)\tYes\tYes VLAN support\tYes\tYes LACP hash port select\tYes\tYes Ethtool link monitoring\tYes\tYes ARP link monitoring\tYes\tYes ports up/down delays\tYes\tYes configurable via Network Manager (gui, tui, and cli)\tYes\tYes multiple device stacking\tYes\tYes highly customizable hash function setup\tNo\tYes D-Bus interface\tNo\tYes √òMQ interface\tNo\tYes port priorities and stickiness (‚Äúprimary‚Äù option enhancement)\tNo\tYes separate per-port link monitoring setup\tNo\tYes logic in user-space\tNo\tYes modular design\tNo\tYes NS/NA (IPV6) link monitoring\tNo\tYes load-balancing for LACP support\tNo\tYes lockless TX/RX path\tNo\tYes user-space runtime control\tLimited\tFull multiple link monitoring setup\tLimited\tYes extensibility\tHard\tEasy performance overhead\tLow\tVery Low RX load-balancing support (ALB)\tYes\tPlanned RX load-balancing support (ALB) in bridge or OVS\tNo\tPlanned Performance Analysis between Bonding and Teaming\n{% endhighlight %}\nsource ‚Äì redhat.com\n{% highlight console %}\nMachine type: 3.3Ghz CPU (Intel), 4GB RAM Link Type: 10GFO Interface\tPerformance with64byte packets\tPerformance with 1KB packets\tPerformance with 64KB packets\tAverage Latency eth0\t1664.00Mb/s (27.48%CPU)\t8053.53Mb/s (30.71%CPU)\t9414.99Mb/s (17.08%CPU)\t54.7usec eth1\t1577.44Mb/s (26.91%CPU)\t7728.04Mb/s (32.23%CPU)\t9329.05Mb/s (19.38%CPU)\t49.3usec bonded (eth0+eth1)\t1510.13Mb/s (27.65%CPU)\t7277.48Mb/s (30.07%CPU)\t9414.97Mb/s (15.62%CPU)\t55.5usec teamed (eth0+eth1)\t1550.15Mb/s (26.81%CPU)\t7435.76Mb/s (29.56%CPU)\t9413.8Mb/s (17.63%CPU)\t55.5usec\n{% endhighlight %}\nMigration We can migrate the existing bond interface to a team interface.\nBond2team is a cli tool used to convert an existing bond interface to team.\nTo convert a current bond0 configuration to team ifcfg, issue a command as root:\n{% highlight console %}\n[root@HOST1 ~]# /usr/bin/bond2team ‚Äìmaster bond0 ‚Äìrename team0\n{% endhighlight %}\nTo convert a current bond0 configuration to team ifcfg, and to manually specify the path to the ifcfg file, issue a command as root:\n{% highlight console %}\n[root@HOST1 ~]# /usr/bin/bond2team ‚Äìmaster bond0 ‚Äìconfigdir /path/to/ifcfg-file\n{% endhighlight %}\nIt is also possible to create a team configuration by supplying the bond2team tool with a list of bonding parameters. For example:\n{% highlight console %}\n[root@HOST1 ~]# /usr/bin/bond2team ‚Äìbonding_opts ‚Äúmode=1 miimon=500‚Äù\n{% endhighlight %}\nConfiguration ‚Äì Mode:active-backup Step 1:\nCheck the available network device status\n{% highlight console %}\n[root@HOST1 ~]# nmcli device status DEVICE TYPE STATE CONNECTION eth0 ethernet connected System eth0 eno1 ethernet disconnected ‚Äî eno2 ethernet disconnected ‚Äî lo loopback unmanaged ‚Äî\n{% endhighlight %}\nCheck the network connection status\n{% highlight console %}\n[root@HOST1 ~]# nmcli connection show NAME UUID TYPE DEVICE System eth0 5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03 802-3-ethernet eth0\n{% endhighlight %}\nStep 2:\nCreating a team interface named team0 in active-backup mode\n{% highlight console %}\n[root@HOST1 ~]# nmcli connection add con-name team0 type team ifname team0 config ‚Äò{‚Äúrunner‚Äù: {‚Äúname‚Äù: ‚Äúactivebackup‚Äù}}‚Äô Connection ‚Äòteam0‚Äô (c30dca42-cc35-4578-95c6-ae79f33b5db3) successfully added.\n{% endhighlight %}\nVerify the new team connection is created\n{% highlight console %}\n[root@HOST1 ~]# nmcli connection show NAME UUID TYPE DEVICE team0 c30dca42-cc35-4578-95c6-ae79f33b5db3 team team0 System eth0 5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03 802-3-ethernet eth0\n{% endhighlight %}\nStep 3:\nAdd the slave interface (eno1 and en02) and point the master to the newly created team interface team0\n{% highlight console %}\n[root@HOST1 ~]# nmcli connection add con-name team0-sl1 type team-slave ifname eno1 master team0 Connection ‚Äòteam0-sl1‚Äô (8d5be820-2e88-46a2-8c08-b34ec0caf8a0) successfully added.\n{% endhighlight %}{% highlight console %}\n[root@HOST1 ~]# nmcli connection add con-name team0-sl2 type team-slave ifname eno2 master team0 Connection ‚Äòteam0-sl2‚Äô (3372f52c-0164-49d5-9783-938df99f18bc) successfully added.\n{% endhighlight %}\nVerify the connection status now . we can see team masters and slaves here\n{% highlight console %}\n[root@HOST1 ~]# nmcli connection show NAME UUID TYPE DEVICE team0 c30dca42-cc35-4578-95c6-ae79f33b5db3 team team0 System eth0 5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03 802-3-ethernet eth0 team0-sl2 3372f52c-0164-49d5-9783-938df99f18bc 802-3-ethernet eno2 team0-sl1 8d5be820-2e88-46a2-8c08-b34ec0caf8a0 802-3-ethernet eno1\n{% endhighlight %}\nStep 4:\nNow assign the IP address to the team0 interface\n{% highlight console %}\n[root@HOST1 ~]# nmcli connection modify team0 ipv4.addresses 192.168.20.100/24 ipv4.method manual\n{% endhighlight %}\nCheck the teaming status. Here we can see team0 interface status and active slave\n{% highlight console %}\n[root@HOST1 ~]# teamdctl team0 state setup: runner: activebackup ports: eno1 link watches: link summary: up instance[link_watch_0]: name: ethtool link: up eno2 link watches: link summary: up instance[link_watch_0]: name: ethtool link: up runner: active port: eno1\n{% endhighlight %}\nStep 5:\nChecking the redundancy\nNow bring down the current active slave interface and check the other interface becomes active\n{% highlight console %}\n[root@HOST1 ~]# nmcli connection down team0-sl team0-sl1 team0-sl2\n{% endhighlight %}{% highlight console %}\n[root@HOST1 ~]# nmcli connection down team0-sl1\n{% endhighlight %}{% highlight console %}\n[root@HOST1 ~]# teamdctl team0 state setup: runner: activebackup ports: eno2 link watches: link summary: up instance[link_watch_0]: name: ethtool link: up runner: active port: eno2\n{% endhighlight %}\nNow we can see the active port is changed to eno2 automatically.\nConfiguration ‚Äì Mode:round-robin Step 1:\nCheck the network interfaces available\n{% highlight console %}\n[root@node1 ~]# ifconfig |grep eno -A 2 eno16777728: flags=4163 mtu 1500 inet 10.0.0.21 netmask 255.255.255.0 broadcast 10.0.0.255\neno33554976: flags=4163 mtu 1500 ether 00:0c:29:9e:6f:a9 txqueuelen 1000 (Ethernet)\neno50332200: flags=4163 mtu 1500 ether 00:0c:29:9e:6f:b3 txqueuelen 1000 (Ethernet)\n{% endhighlight %}\nStep 2:\nCreate a teaming interface with config roundrobin\n{% highlight console %}\n[root@node1 ~]# nmcli connection add con-name team1 type team ifname team1 config ‚Äò{‚Äúrunner‚Äù: {‚Äúname‚Äù: ‚Äúroundrobin‚Äù}}‚Äô Connection ‚Äòteam1‚Äô (ef3bc4dc-c060-4d39-865a-6c6df65d6c3d) successfully added.\n{% endhighlight %}\nStep 3:\nCreate a slave interface and point the master to team1\n{% highlight console %}\n[root@node1 ~]# nmcli connection add con-name slave1 type team-slave ifname eno33554976 master team1 Connection ‚Äòslave1‚Äô (f77e9f45-43fe-438d-9b91-9e2397b15272) successfully added.\n{% endhighlight %}{% highlight console %}\n[root@node1 ~]# nmcli connection add con-name slave2 type team-slave ifname eno50332200 master team1 Connection ‚Äòslave2‚Äô (4559604b-ffd4-4513-a3a9-b9c94ceb5108) successfully added.\n{% endhighlight %}\nStep 4:\nAssign IP to team1 interface\n{% highlight console %}\n[root@node1 ~]# nmcli connection modify team1 ipv4.addresses 10.0.0.23/24 ipv4.method manual\n{% endhighlight %}\nStep 5:\nCheck the connecton status\n{% highlight console %}\n[root@node1 ~]# nmcli connection show NAME UUID TYPE DEVICE slave1 f77e9f45-43fe-438d-9b91-9e2397b15272 802-3-ethernet eno33554976 team1 ef3bc4dc-c060-4d39-865a-6c6df65d6c3d team team1 eno16777728 c0cbd7c2-b6cc-473d-919f-4a291fc72cd4 802-3-ethernet eno16777728 slave2 4559604b-ffd4-4513-a3a9-b9c94ceb5108 802-3-ethernet eno50332200\n{% endhighlight %}{% highlight console %}\n[root@node1 ~]# teamdctl team1 state setup: runner: roundrobin ports: eno50332200 link watches: link summary: up instance[link_watch_0]: name: ethtool link: up eno33554976 link watches: link summary: up instance[link_watch_0]: name: ethtool link: up\n{% endhighlight %}\nBandwidth analysis between individual NIC and teaming NIC (mode=round-robin) I Used another VM(node2) with identical configuraton and followed the same steps and assined the IP 10.0.0.24 to team1 interface.\n{% highlight console %}\n[root@node2 ~]# nmcli connection modify team1 ipv4.addresses 10.0.0.24/24 ipv4.method manual\n{% endhighlight %}\nStep 1:\nInstalling qperf packege in both the node\n{% highlight console %}\n[root@nodeX]# rpm -ivh qperf-0.4.9-2.el7.x86_64.rpm warning: qperf-0.4.9-2.el7.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID fd431d51: NOKEY Preparing‚Ä¶ ################################# [100%] Updating / installing‚Ä¶ 1:qperf-0.4.9-2.el7 ################################# [100%]\n{% endhighlight %}\nStep 2:\nFlush the iptables in both the nodes\n{% highlight console %}\n[root@nodeX ~]# iptables -F\n{% endhighlight %}\nStep 3:\nNow bringing down the teaming interface from both the nodes to check the bandwith by keeping only individual NIC\n{% highlight console %}\n[root@node1 ~]# nmcli connection down team1\n{% endhighlight %}{% highlight console %}\n[root@node1 ~]# nmcli connection show NAME UUID TYPE DEVICE slave2 4559604b-ffd4-4513-a3a9-b9c94ceb5108 802-3-ethernet ‚Äî eno16777728 c0cbd7c2-b6cc-473d-919f-4a291fc72cd4 802-3-ethernet eno16777728 team1 ef3bc4dc-c060-4d39-865a-6c6df65d6c3d team ‚Äî slave1 f77e9f45-43fe-438d-9b91-9e2397b15272 802-3-ethernet ‚Äî\n{% endhighlight %}{% highlight console %}\n[root@node2 ~]# nmcli connection down team1\n{% endhighlight %}{% highlight console %}\n[root@node2 ~]# nmcli connection show NAME UUID TYPE DEVICE slave1 3d2ae74e-00b5-42d9-b531-d2950190f9de 802-3-ethernet ‚Äî team1 18b13233-eb4b-4468-8bc9-e8d8263a404b team ‚Äî eno16777728 42910ac3-a701-4f5e-b416-59c61ce1eb11 802-3-ethernet eno16777728 slave2 72061aa5-dac5-4376-b6df-34c0e5620b2e 802-3-ethernet ‚Äî\n{% endhighlight %}\nStep 4:\nNow starting qperf in node2\n{% highlight console %}\n[root@node2 ~]# qperf\n{% endhighlight %}\nStep 5:\nNow running qperf for 5 minutes to check the tcp and udp bandwidth between node 1 and node 2 .\nThe IP 10.0.0.22 is assined to NIC eno16777728 of node2\n{% highlight console %}\n[root@node1 ~]# qperf 10.0.0.22 -t 300 tcp_bw udp_bw tcp_bw: bw = 105 MB/sec udp_bw: send_bw = 55.9 MB/sec recv_bw = 55.2 MB/sec\n{% endhighlight %}\nStep 6:\nNow bandwidth of individual NIC is tested, lets bring up team interface and bring down individual NIC\n{% highlight console %}\n[root@node1 ~]# nmcli connection down eno16777728\n{% endhighlight %}{% highlight console %}\n[root@node1 ~]# nmcli connection show NAME UUID TYPE DEVICE slave2 4559604b-ffd4-4513-a3a9-b9c94ceb5108 802-3-ethernet eno50332200 eno16777728 c0cbd7c2-b6cc-473d-919f-4a291fc72cd4 802-3-ethernet ‚Äî team1 ef3bc4dc-c060-4d39-865a-6c6df65d6c3d team team1 slave1 f77e9f45-43fe-438d-9b91-9e2397b15272 802-3-ethernet eno33554976\n{% endhighlight %}{% highlight console %}\n[root@node2 ~]# nmcli connection down eno16777728\n{% endhighlight %}{% highlight console %}\n[root@node2 ~]# nmcli connection show NAME UUID TYPE DEVICE slave1 3d2ae74e-00b5-42d9-b531-d2950190f9de 802-3-ethernet eno33554976 team1 18b13233-eb4b-4468-8bc9-e8d8263a404b team team1 eno16777728 42910ac3-a701-4f5e-b416-59c61ce1eb11 802-3-ethernet ‚Äî slave2 72061aa5-dac5-4376-b6df-34c0e5620b2e 802-3-ethernet eno50332200\n{% endhighlight %}\nStep 7:\nStarting qperf on node2\n{% highlight console %}\n[root@node2 ~]# qperf\n{% endhighlight %}\nStep 8:\nNow running qperf for 5 minutes to check the tcp and udp bandwidth between node 1 and node 2 .\nThe IP 10.0.0.24 is assined to team1 interface of node2\n{% highlight console %}\n[root@node1 ~]# qperf 10.0.0.24 -t 300 tcp_bw udp_bw tcp_bw: bw = 108 MB/sec udp_bw: send_bw = 51.5 MB/sec recv_bw = 51 MB/sec\n{% endhighlight %}\nNow we can see the bandwith between individual NIC and team interface is almost same.\n","permalink":"http://localhost:1313/posts/2017-10-07-network-teaming-rhel7/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003ch4 id=\"linux-bonding-driver\"\u003eLinux Bonding driver\u003c/h4\u003e\n\u003cp\u003eThe Linux bonding driver provides a method for aggregating multiple network interface controllers (NICs) into a single logical bonded interface of two or more so-called (NIC) slaves. The majority of modern Linux distributions (distros) come with a Linux kernel which has the Linux bonding driver (bonding)integrated as a loadable kernel module.\u003c/p\u003e\n\u003cp\u003eWe can check the bonding module by\u003c/p\u003e\n\u003cp\u003e{% highlight console %}\u003c/p\u003e\n\u003cp\u003e[root@example.com ~]# lsmod |grep bonding\nbonding 142537 0\u003c/p\u003e","title":"Network teaming ‚Äì RHEL7"},{"content":"Budapest, otherwise called the Little Paris of Central Europe is one of the celebrated vacationer spots in Europe.\nThis city is comprised of two particular sides, the Buda and Pest, both isolated by the wonderful Danube River.\nThe Buda side is notable with a few historical centers and Roman vestiges, while the Pest is the urban focus of the city and is for business, loaded with clubs, bistros, and markets.\nI remained in Budapest for 2 weeks and had 3 entire days to visit the city. On the first day, I leased a bicycle. Bicycle rental is cheap and is accessible all around, you simply need to pay some advance (i paid 50 euro) and I leased for a day which cost 3500 HUF.\nFor the following two day, I took the tours from city sightseeing, which offers 48 hours trip in hop on -hop off buses and 2 boat ride, free walking tour to some places. Free bicycle rental likewise included for a few spots which I don‚Äôt know prior. There are likewise many free coupons which incorporate free brew, goulash soup and so on. And all included for only 24 Euro(1800 INR) which is super cheap.\nThe full route maps offered by the tour is below. If you are new to the city and needs to visit many places in short time, then I suggest taking this tour.\nNow let‚Äôs show you some of my favorite pictures taken during the trip.\nHungarian Parliament Building The Hungarian Parliament Building in Budapest is one of the popular tourist destination in the city.\nI took the picture from the boat in Danube river. It‚Äôs truly delightful to see from the Danube, particularly around night time.\nBuda Castle The castle is part of what is known as the Castle District, which has several medieval and Baroque houses, churches and other buildings.\nThe castle is the great spot to get a panoramic view over the Danube. Climbing up might be tiring, so I recommend to take tram for arrival and walk for the return, which take 10-15 minutes.\nGell√©rt Hill Named after Saint Gerard who was thrown to death from here, Gell√©rt Hill rises 235 meters over the Danube.\nThe Gellert hill is also a great place to get the panoramic view of the Pest side of the city. Here is the view from hill\nHeroes‚Äô Square The square itself is a UNESCO World Heritage Site.The square was built in 1896 to commemorate the 1000th anniversary of the Magyar conquest of Hungary.\nThe square is also the site of two important museums ‚Äì the Museum of Fine Arts and Palace of Art.\nChain Bridge The Chain Bridge was the first permanent stone bridge in the Danube river which connects Buda and Pest. Here is the view taken from Buda castle.\nSt. Stephen‚Äôs Basilica It is named in honor of Stephen, the first King of Hungary. The dome is absolutely beautiful and you will get a beautiful view of the city from the top.\nRedbull Airshow I also had an opportunity to see the red bull flying show. It was really stunning.\nView this post on Instagram #danuberiver #budapest #flight #airshow\nA post shared by Vignesh Ragupathy (@vikkiinimg) on Feb 12, 2019 at 12:55am PST\nHere is one of the flight model placed during the airshow.\nContrasted with Indian cities, Budapest is much smaller, much less congested, less polluted and has friendly people, however the exception is food.\nHere is me at a Hungarian restaurant, fighting with food üôÇ More pictures can be found on my Flickr\n#Throwback #www.vikki.in #blog #Budapest #2017 pic.twitter.com/KD5lWDvdvL\n‚Äî vignesh ragupathy (@r_vignesh88) February 24, 2019 ","permalink":"http://localhost:1313/posts/2017-10-07-beautiful-budapest/","summary":"\u003cp\u003eBudapest, otherwise called the Little Paris of Central Europe is one of the celebrated vacationer spots in Europe.\u003cbr\u003e\nThis city is comprised of two particular sides, the Buda and Pest, both isolated by the wonderful Danube River.\u003c/p\u003e\n\u003cp\u003eThe Buda side is notable with a few historical centers and Roman vestiges, while the Pest is the urban focus of the city and is for business, loaded with clubs, bistros, and markets.\u003c/p\u003e","title":"Beautiful Budapest"},{"content":"This article is my own and based on various other sources from the internet. If you have something to say please provide your feedback in the comment section below.\nwhat is systemd: systemd is a service manager/init process used to bootstrap the userspace and manage all process.\nAlready systemd is the default init system for many operating systems including redhat, ubuntu, fedora and many others. Systemd is useful for a system administrator, as it provides many features to manage the OS efficiently.\nWhy systemd: Here are the few reasons why systemd is needed.\nNote: Some of the reasons below are disagreed by the developers and traditional SysV users\nSpeedup the boot by increasing the concurrency of process startup\nBetter process management ‚Äì by babysitting process\nBetter dependencies management of process during startup\nReduce computational overhead of shell script used during boot\nSystemd components: How systemd increases concurrency of process start\nIn Linux, services can be provided by INET socket or D-bus.\nFor INET socket: Each process has dependencies. Example NFS mounts daemon waits for RPC bind.sock and portmapper IP port.\nSimilarly, each daemon has its own dependencies and it may have to wait for the socket opening. Further, each daemon may have other daemon dependencies, this will further increase the wait time.\nSo the idea of the systemd is to start the sockets before the daemon start. Although socket has no much dependencies, we can create all the sockets for all daemon at one step. If all sockets are created then we can start all the daemons at the same time currently.\nThere may be wait time if a daemon needs another daemon dependency the daemon is not started, But that‚Äôs ok according to systemd.\nWe still reduce the wait time of socket creation. By doing this we achieve following things\nBoot time reduced\nDependencies between service no longer exist\nStartup is parallelized\nFor D bus\nD-bus can also be activated using the same logic of traditional INET socket.\nInstead of starting the service at startup, we can start the first time it is accessed using bus activation.\nIt also provides options for starting up provider and consumer at the same time.\nFilesystem During system boot, there are lots of time spent waiting for filesystem jobs example: mounting, fsck, quota, quotoACL etc.\nOnly after this, the kernel will start the actual services. To reduce this time spent, systemd will setup an autofs mount and start the service.\nwhen the filesystem finishes the quota etc it will replace by the real mount. This cannot be done for the filesystem like /,/proc where the service binaries are stored.\nHow daemon escapes init by double forking Double forking is meant to make a process orphan and making it as a child for init process(PID 1). By this way, we can daemonize a process.\nBy double forking a process, it‚Äôs difficult to identify how the process is originally spawned in SysV. But systemd uses the Cgroups to keep track of the process.\nFor full application servers like apache which usually spawn many child processes, it is difficult to kill the entire service. Killing the apache process sometimes will not kill all its child process.Here systemd comes to rescue which uses systemctl to easily kill all process of a service.\n{% highlight console %}\nvikki@trinity:~$ systemctl kill httpd.service\n{% endhighlight %}\nCgroups also keeps track of cpu/mem utilzation of each process. To see Cgroups cpu/memory details\n{% highlight console %}\nvikki@trinity:~$ systemd-cgtop Control Group Tasks %CPU Memory Input/s Output/s / ‚Äì 12.6 3.4G ‚Äì ‚Äì /init.scope 1 ‚Äì ‚Äì ‚Äì ‚Äì /system.slice 43 ‚Äì ‚Äì ‚Äì ‚Äì /user.slice 546 ‚Äì ‚Äì ‚Äì ‚Äì /user.slice/user-1000.slice 546 ‚Äì ‚Äì ‚Äì ‚Äì\n{% endhighlight %}\nTo recursively see Cgroups contents\n{% highlight console %}\nvikki@trinity:~$ systemd-cgls Control group /: -.slice ‚îú‚îÄinit.scope ‚îÇ ‚îî‚îÄ1 /sbin/init splash ‚îú‚îÄsystem.slice ‚îÇ ‚îú‚îÄavahi-daemon.service ‚îÇ ‚îÇ ‚îú‚îÄ1015 avahi-daemon: running [trinity.local ‚îÇ ‚îÇ ‚îî‚îÄ1040 avahi-daemon: chroot helpe ‚îÇ ‚îú‚îÄthermald.service ‚îÇ ‚îÇ ‚îî‚îÄ953 /usr/sbin/thermald ‚Äìno-daemon ‚Äìdbus-enable ‚îÇ ‚îú‚îÄdbus.service ‚îÇ ‚îÇ ‚îú‚îÄ 970 /usr/bin/dbus-daemon ‚Äìsystem ‚Äìaddress=systemd: ‚Äìnofork ‚Äìnopidfile ‚Äìsystemd-activation ‚îÇ ‚îÇ ‚îî‚îÄ2530 /usr/lib/x86_64-linux-gnu/fwupd/fwupd ‚îÇ ‚îú‚îÄuuidd.service ‚îÇ ‚îÇ ‚îî‚îÄ2651 /usr/sbin/uuidd ‚Äìsocket-activation ‚îÇ ‚îú‚îÄModemManager.service ‚îÇ ‚îÇ ‚îî‚îÄ967 /usr/sbin/ModemManager ‚îÇ ‚îú‚îÄcron.service ‚îÇ ‚îÇ ‚îî‚îÄ1004 /usr/sbin/cron -f ‚îÇ ‚îú‚îÄwpa_supplicant.service ‚îÇ ‚îÇ ‚îî‚îÄ1218 /sbin/wpa_supplicant -u -s -O /run/wpa_supplicant ‚îÇ ‚îú‚îÄlightdm.service ‚îÇ ‚îÇ ‚îú‚îÄ1190 /usr/sbin/lightdm ‚îÇ ‚îÇ ‚îî‚îÄ1268 /usr/lib/xorg/Xorg -core :0 -seat seat0 -auth /var/run/lightdm/root/:0 -nolisten tcp vt7 -novtswitch ‚îÇ ‚îú‚îÄaccounts-daemon.service ‚îÇ ‚îÇ ‚îî‚îÄ1014 /usr/lib/accountsservice/accounts-daemon\n{% endhighlight %}\nTo see the time spent in system startup\n{% highlight console %}\nvikki@trinity:~$/usr/lib/systemd/network$ systemd-analyze time Startup finished in 6.503s (kernel) + 11.296s (userspace) = 17.799s\n{% endhighlight %}\nTo see the time spent by each process during startup\n{% highlight console %}\nvikki@trinity:~$/usr/lib/systemd/network$ systemd-analyze blame 7.985s postgresql@9.5-main.service 2.450s dev-sda1.device 2.149s mysql.service 1.858s vmware-tools-thinprint.service 1.630s vmware-tools.service\n{% endhighlight %}\nComparison of different init system:\n{% highlight console %}\nsysvinit\tUpstart\tsystemd Interfacing via D-Bus\tno\tyes\tyes Shell-free bootup\tno\tno\tyes Modular C coded early boot services included\tno\tno\tyes Read-Ahead\tno\tno[1]\tyes Socket-based Activation\tno\tno[2]\tyes Socket-based Activation: inetd compatibility\tno\tno[2]\tyes Bus-based Activation\tno\tno[3]\tyes Device-based Activation\tno\tno[4]\tyes Configuration of device dependencies with Udev rules\tno\tno\tyes Path-based Activation (inotify)\tno\tno\tyes Timer-based Activation\tno\tno\tyes Mount handling\tno\tno[5]\tyes fsck handling\tno\tno[5]\tyes Quota handling\tno\tno\tyes Automount handling\tno\tno\tyes Swap handling\tno\tno\tyes Snapshotting of system state\tno\tno\tyes XDG_RUNTIME_DIR Support\tno\tno\tyes Optionally kills remaining processes of users logging out\tno\tno\tyes Linux Control Groups Integration\tno\tno\tyes Audit record generation for started services\tno\tno\tyes SELinux integration\tno\tno\tyes PAM integration\tno\tno\tyes Encrypted hard disk handling (LUKS)\tno\tno\tyes SSL Certificate/LUKS Password handling, including Plymouth, Console, wall(1), TTY and GNOME agents\tno\tno\tyes Network Loopback device handling\tno\tno\tyes binfmt_misc handling\tno\tno\tyes System-wide locale handling\tno\tno\tyes Console and keyboard setup\tno\tno\tyes Infrastructure for creating, removing, cleaning up of temporary and volatile files\tno\tno\tyes Handling for /proc/sys sysctl\tno\tno\tyes Plymouth integration\tno\tyes\tyes Save/restore random seed\tno\tno\tyes Static loading of kernel modules\tno\tno\tyes Automatic serial console handling\tno\tno\tyes Unique Machine ID handling\tno\tno\tyes Dynamic host name and machine meta data handling\tno\tno\tyes Reliable termination of services\tno\tno\tyes Early boot /dev/log logging\tno\tno\tyes Minimal kmsg-based Syslog daemon for embedded use\tno\tno\tyes Respawning on service crash without losing connectivity\tno\tno\tyes Gapless service upgrades\tno\tno\tyes Graphical UI\tno\tno\tyes Built-In Profiling and Tools\tno\tno\tyes Instantiated services\tno\tyes\tyes PolicyKit integration\tno\tno\tyes Remote access/Cluster support built into client tools\tno\tno\tyes Can list all processes of a service\tno\tno\tyes Can identify service of a process\tno\tno\tyes Automatic per-service CPU cgroups to even out CPU usage between them\tno\tno\tyes Automatic per-user cgroups\tno\tno\tyes SysV compatibility\tyes\tyes\tyes SysV services controllable like native services\tyes\tno\tyes SysV-compatible /dev/initctl\tyes\tno\tyes Reexecution with full serialization of state\tyes\tno\tyes Interactive boot-up\tno[6]\tno[6]\tyes Container support (as advanced chroot() replacement)\tno\tno\tyes Dependency-based bootup\tno[7]\tno\tyes Disabling of services without editing files\tyes\tno\tyes Masking of services without editing files\tno\tno\tyes Robust system shutdown within PID 1\tno\tno\tyes Built-in kexec support\tno\tno\tyes Dynamic service generation\tno\tno\tyes Upstream support in various other OS components\tyes\tno\tyes Service files compatible between distributions\tno\tno\tyes Signal delivery to services\tno\tno\tyes Reliable termination of user sessions before shutdown\tno\tno\tyes utmp/wtmp support\tyes\tyes\tyes Easily writable, extensible and parseable service files, suitable for manipulation with enterprise management tools\tno\tno\tyes\n{% endhighlight %}\n","permalink":"http://localhost:1313/posts/2017-10-02-systemd-introduction-to-system-admin/","summary":"\u003cp\u003eThis article is my own and based on various other sources from the internet. If you have something to say please provide your feedback in the comment section below.\u003c/p\u003e\n\u003ch3 id=\"what-is-systemd\"\u003ewhat is systemd:\u003c/h3\u003e\n\u003cp\u003esystemd is a service manager/init process used to bootstrap the userspace and manage all process.\u003cbr\u003e\nAlready systemd is the default init system for many operating systems including redhat, ubuntu, fedora and many others. Systemd is useful for a system administrator, as it provides many features to manage the OS efficiently.\u003c/p\u003e","title":"Systemd ‚Äì Introduction to system admin"},{"content":"Overview This post is about how to create a high available redundant storage (Glusterfs replicated volume) from Raspberrypi and a centos server. This is just for fun project, which i am experimenting with my new raspberrypi 2 device. This is not a perfect setup, like i am using a 2 nodes replicated volume without quorom,created brick from ext3 filesystem and on root directory etc.Do not use this setup in production üôÇ\nArchitecture In this i am using a wifi hotspot router, to act as a DHCP server and connected both the raspberrypi device (server1.vikki.in) using wifi adapter(Edimax 150 Mbps Wireless Nano USB Adaptor) and a laptop running 2 VM‚Äôs ( server2.vikki.in and client.vikki.in)in same wireless network (192.168.1.0)\nFor details about how to enable wifi interface in raspberrypi please go through the Link\nSoftwares used Ubuntu 14.04\nCentos 6.3\nRaspbian wheezy\nGlusterfs 3.7\nHardware used Raspberry PI Model 2 ‚Äì 1GB\nEdimax 150 Mbps Wireless Nano USB Adaptor (EW-7811Un)\nCenda T-50 Power Bank 10000 mAh\nsandisk ultra 16gb memory card\nWifi router\nLaptop\nInstallation Step 1 :\nLogin to Raspberry pi and run the below command to install glusterfs\n{% highlight console %}\nroot@raspberrypi:~# sudo apt-get install glusterfs-server\n{% endhighlight %}\nStep 2 :\nLogin to the centos VM and do the following.\nDownload the glusterfs version 3.2.7. Since raspbian OS comes wit 3.2.7 , i am downloading the same version to avoid any conflicts.\nNote : I tried a combination of version 3.2.7 and the latest version that comes with redhat EPL , but this doesn‚Äôt work out (peer probe connection failed)\n{% highlight console %}\nroot@server2]# wget -c http://bits.gluster.com/pub/gluster/glusterfs/src/glusterfs-3.2.7.tar.gz\n{% endhighlight %}\nNow download and install all the build dependencies\n{% highlight console %}\nroot@server2]# yum install flex automake autoconf libtool flex bison openssl-devel libxml2-devel python-devel libaio-devel libibverbs-devel librdmacm-devel readline-devel lvm2-devel glib2-devel userspace-rcu-devel libcmocka-devel libacl-devel\n{% endhighlight %}\nExtract the source file, navigate and do configure\n{% highlight console %}\n[root@server2 glusterfs-3.2.7]# ./configureGlusterFS configure summary FUSE client : yes Infiniband verbs : yes epoll IO multiplex : yes argp-standalone : no fusermount : no readline : yes georeplication : yes\n{% endhighlight %}\nAnd the finally make and install the gluster\n{% highlight console %}\n[root@server2 glusterfs-3.2.7]# make [root@server2 glusterfs-3.2.7]# make install\n{% endhighlight %}\nConfiguration Login to raspberrypi and probe the centos server (server2.vikki.in)\nserver1 :\n{% highlight console %}\nroot@raspberrypi:~# gluster peer probe server2.vikki.in Probe successful\n{% endhighlight %}\nCheck the peer status\nserver1 :\n{% highlight console %}\nroot@raspberrypi:~# gluster peer status Number of Peers: 1 Hostname: server2.vikki.in Uuid: 0531327f-1b4a-4694-b525-58b7277472fd State: Peer in Cluster (Connected)\n{% endhighlight %}\nSimilarly login to the centos server and probe the raspberry pi server(server1.vikki.in)\nserver2 :\n{% highlight console %}\nroot@server2 glusterfs]# gluster peer probe server2.vikki.in Probe successful\n{% endhighlight %}\nserver2 :\n{% highlight console %}\nroot@server2 glusterfs]# gluster peer status Number of Peers: 1 Hostname: server1.vikki.in Uuid: 0531327f-1b4a-4694-b525-58b7277472fe State: Peer in Cluster (Connected)\n{% endhighlight %}\nI have a brick /share1 of 1GB size mounted in both raspberrypi and centos server\nNow create a replicated volume from any of the server . Here i am creating it from the raspberry pi server(server1.vikki.in)\nserver1 :\n{% highlight console %}\nroot@raspberrypi:/var/log/glusterfs# gluster volume create replicated_volume replica 2 server1.vikki.in:/share1 server2.vikki.in:/share1\n{% endhighlight %}\nCreation of volume replicated_volume has been successful. Please start the volume to access data.\nNow start the replicated volume\nserver1 :\n{% highlight console %}\nroot@raspberrypi:/var/log/glusterfs# gluster volume start replicated_volume Starting volume replicated_volume has been successful\n{% endhighlight %}\nCheck the replicated volume info\n{% highlight console %}\n[root@server2 glusterfs]# gluster volume info replicated_volumeVolume Name: replicated_volume Type: Replicate Status: Started Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: server1.vikki.in:/share1 Brick2: server2.vikki.in:/share1\n{% endhighlight %}\nNow the replicated volume is created . Login to the client (client.vikki.in) and mount the glusterfs volume\nClient :\n{% highlight console %}\n[root@client ~]# mount.glusterfs server1.vikki.in:/replicated_volume /mnt/gluster/\n{% endhighlight %}\nThe replicated volume is successfully mounted in the client\n{% highlight console %}\n[root@client gluster]# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda2 18G 3.5G 14G 21% / tmpfs 495M 88K 495M 1% /dev/shm /dev/sda1 291M 33M 244M 12% /boot server1.vikki.in:/replica_volume 985M 18M 917M 2% /mnt/gluster\n{% endhighlight %}\nTesting create some files in gluster volume\n{% highlight console %}\n[root@client gluster]# echo ‚Äúwhen both nodes are running‚Äù \u0026gt; file1.txt [root@client gluster]#ls file1.txt\n{% endhighlight %}\nNow go to both the server (raspberrypi,centos) and check the individual brick.\n{% highlight console %}\n[root@server1 glusterfs]# ls /share1 file1.txt [root@server2 glusterfs]# ls /share1 file1.txt\n{% endhighlight %}\nNow we can see the files are replicated in both the nodes .\nNow bring down one of the server and check if the gluster volume is still accessible in client.\nMy setup: ","permalink":"http://localhost:1313/posts/2017-10-01-glusterfs-ha-storage-with-raspberrypi/","summary":"\u003ch3 id=\"overview\"\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThis post is about how to create a high available redundant storage (Glusterfs replicated volume) from Raspberrypi and a centos server. This is just for fun project, which i am experimenting with my new raspberrypi 2 device. This is not a perfect setup, like i am using a 2 nodes replicated volume without quorom,created brick from ext3 filesystem and on root directory etc.Do not use this setup in production üôÇ\u003c/p\u003e","title":"Glusterfs -high available redundant storage with Raspberry pi/Centos server"}]